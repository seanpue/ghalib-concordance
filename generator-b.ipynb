{
 "metadata": {
  "name": "",
  "signature": "sha256:21f74db6208797dc1b06b903c11ac1b4339bcb54aabc4de962a19196bac8802e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ghalib Concordance Generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Description\n",
      "\n",
      "This notebook contains code to generate a concordance for the murav\u0101j d\u012bv\u0101n of Ghalib.\n",
      "\n",
      "Verses are taken from \"input/verses.csv\"\n",
      "\n",
      "The current task is to identify the proper lemma of the tokens, e.g. singular instead of plural,\n",
      "verb infinitive instead of verb root, etc. This can partially be done computationally.\n",
      "\n",
      "Lemma that remain to be checked are in \"output/tocheck.csv\" The first column, if marked as 'x',\n",
      "means that entry is okay. Checked lemma can then be entered into \"input/okay.csv\" using the\n",
      "functions \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import re\n",
      "from collections import *\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = {}                      # dictionary of verses, e.g. 001.01.0='naqsh faryaadii..'\n",
      "tokens = {}                      # dictionary of tokens where key is verses+.xx, e.g. 001.01.0.01 = 'naqsh'\n",
      "unique_tokens = Counter()        # Counter of tokens where value is their count\n",
      "lemmas = defaultdict(list)       # dictionary of tokens where value is a list of their lemmas\n",
      "unique_lemmas = []               # list of unique lemmas\n",
      "okay_lemmas = defaultdict(list)  # dictionary of unique tokens with lists of lemma, e."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# moved load_verses, moved to util.py\n",
      "\n",
      "def load_verses(inputfile='input/verses.csv'):\n",
      "    '''\n",
      "    Loads verses from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: verses where verses['ggg.vv.l']=token; where ggg=ghazal #; vv=verse number;l=line number\n",
      "    '''\n",
      "\n",
      "\n",
      "    verses = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            if len(row)<3: print row\n",
      "            (verse_id, input_string, real_scan) = row # \n",
      "            if not 'x' in verse_id: # only muravvaj divan for now\n",
      "                verses[verse_id] = input_string.strip() \n",
      "    return verses\n",
      "\n",
      "def get_okay_lemmas(inputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Loads checked lemmas from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: checked_lemmas where checked_lemmas['token'] = [lemmas]\n",
      "    '''\n",
      "\n",
      "    import csv\n",
      "    okay_lemmas = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            if len(row)!=3:\n",
      "                print row\n",
      "            (status, unique_token, lemmas) = row\n",
      "#            if not status in ['','x']:\n",
      "#                print 'error in row,row'\n",
      "        \n",
      "            assert status in ['','x']\n",
      "            if status=='x':\n",
      "                okay_lemmas[unique_token]=lemmas.split('|')\n",
      "    return okay_lemmas\n",
      "\n",
      "\n",
      "def get_tokens(verses):\n",
      "    '''\n",
      "    Identifies tokens in verses\n",
      "    verses: verses\n",
      "    returns: tokens, where tokens['ggg.vv.l.tt']=token {tt = token # on line starting  at zero}\n",
      "    '''\n",
      "    tokens = {}\n",
      "    token_instances=defaultdict(list)\n",
      "    token_instance_count = Counter()\n",
      "    for k in verses.keys():\n",
      "        v_tokens = verses[k].split(' ')\n",
      "        for id,t in enumerate(v_tokens):\n",
      "\n",
      "            token_id = k+'.'+str(id).zfill(2)\n",
      "            tokens[token_id] = t\n",
      "            token_instances[t].append(token_id)\n",
      "            token_instance_count[t]+=1\n",
      "    return tokens,token_instances,token_instance_count\n",
      "\n",
      "def locate_token(token):\n",
      "    '''\n",
      "    Finds locations of token\n",
      "    token: string \n",
      "    Input: token (string)\n",
      "    returns: a list of locations, e.g. [001.01.0.01]\n",
      "    '''\n",
      "    assert tokens\n",
      "    return [k  for k,v in tokens.iteritems() if v==token]\n",
      "\n",
      "def match_tokens(match_string):\n",
      "    '''\n",
      "    Finds tokens matching a pattern (from start)\n",
      "    match_string: regular expression string (assumes ^,e.g. 'naq')\n",
      "    returns: a list of tokens,e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.match(match_string,k)]\n",
      "\n",
      "def search_tokens(match_string):\n",
      "    '''\n",
      "    Searches for tokens matching a pattern (anywhere in it)\n",
      "    match_string: regular expression of string\n",
      "    Input: regular expression string (e.g. 'naqsh'\n",
      "    returns: a list of tokens, e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.search(match_string,k)]\n",
      "\n",
      "def get_unique_tokens(tokens):\n",
      "    '''\n",
      "    Finds unique tokens\n",
      "    tokens: a dictionary of tokens at locations, e.g. tokens['001.01.0.00']='naqsh'\n",
      "    returns: a dictionary of unique tokens and their count, unique_tokens['token']=1\n",
      "    '''\n",
      "    unique = Counter()\n",
      "#    print type(tokens)\n",
      "    for k,t in tokens.iteritems():\n",
      "        unique[t]+=1\n",
      "    return unique\n",
      "\n",
      "\n",
      "def get_lemmas(unique_tokens):\n",
      "    '''\n",
      "    Generate lemmas of tokens\n",
      "    unique_tokens: dictionary of unique tokens\n",
      "    returns: lemmas[original_token]=['lemma1','lemma2']\n",
      "    '''\n",
      "    lemmas = {}\n",
      "\n",
      "    \n",
      "    for t in unique_tokens.keys():\n",
      "        lemma = t\n",
      "        if re.search(\"-e$\",t):\n",
      "            lemma = t[:-2] # remove izaafat ending '-e'\n",
      "        if re.search(\"[-']haa$\",t): \n",
      "            lemma = t[:-4] # remove Persian plural ['-]haa ending\n",
      "#            print lemma\n",
      "        t_lemmas = [lemma]\n",
      "        if re.search('-o-',lemma):\n",
      "            nouns = lemma.split('-o-')\n",
      "            t_lemmas = t_lemmas + nouns\n",
      "            \n",
      "        lemmas[t]=t_lemmas\n",
      "    return lemmas\n",
      "\n",
      "def get_unique_lemmas(lemmas):\n",
      "    '''\n",
      "    Generates unique lemma forms\n",
      "    lemmas: dictionary keyed by tokens containing lists of lemma, e.g. lemmas['rang-o-buu']=['rang','buu','rang-o-buu']\n",
      "    returns: unique_lemmas as unique_lemmas['lemma']=count\n",
      "    '''\n",
      "    unique_lemmas = set()\n",
      "    for t,t_lemmas in lemmas.iteritems():\n",
      "        for lemma in t_lemmas:\n",
      "            unique_lemmas.add(lemma)\n",
      "    return unique_lemmas\n",
      "\n",
      "\n",
      "def to_check():\n",
      "    '''\n",
      "    Generates list of unique tokens that still need to be checked.\n",
      "    '''\n",
      "    out = []\n",
      "    return [t for t in sorted(unique_tokens.keys()) if not t in okay_lemmas]\n",
      "\n",
      "def print_stats():\n",
      "    print \"Currently there are \",len(okay_lemmas),\" out of \",len(lemmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "defaultdict(<type 'list'>, {})"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = load_verses()\n",
      "tokens,token_instances,token_instance_count = get_tokens(verses)\n",
      "unique_tokens = get_unique_tokens(tokens)\n",
      "\n",
      "lemmas = get_lemmas(unique_tokens)\n",
      "unique_lemmas = get_unique_lemmas(lemmas)\n",
      "okay_lemmas = get_okay_lemmas()\n",
      "\n",
      "okay_tokens_not_in_lemmas = [ok for ok in okay_lemmas if not ok in lemmas]\n",
      "\n",
      "if len(okay_tokens_not_in_lemmas) > 0:\n",
      "    print 'the following tokens are marked as okay but are not any longer'\n",
      "    print okay_tokens_not_in_lemmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Update Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_to_check():\n",
      "    '''\n",
      "    Writes unique tokens not contained in okay_lemmas to output/tocheck.csv\n",
      "    '''\n",
      "    with open('output/tocheck.csv','w') as f:\n",
      "        for t in sorted(unique_tokens.keys()):\n",
      "            if not t in okay_lemmas: # only add unchecked ones\n",
      "                line  = \",\" # good or bad\n",
      "                line += t+\",\" #token\n",
      "                line += '|'.join(lemmas[t]) # possible lemma of token\n",
      "                line += \"\\n\" \n",
      "                f.write(line)\n",
      "\n",
      "def update_okay(inputfile='output/tocheck.csv'):\n",
      "    '''\n",
      "    Loads lemmas noted as correct from inputfile into okay_lemmas\n",
      "    '''\n",
      "    lemmas_to_add = get_okay_lemmas(inputfile=inputfile)\n",
      "    for k,v in lemmas_to_add.iteritems():\n",
      "        if k in okay_lemmas:\n",
      "            print \"WARNING: \",k,\" found in okay_lemmas. Will override.\"\n",
      "        okay_lemmas[k] = v\n",
      "    \n",
      "def write_okay(outputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Writes okay_lemmas to outputfile, as status,token,lemma1|lemma2|lemma3\n",
      "    '''\n",
      "    with open(outputfile,'w') as f:\n",
      "        for t in sorted(okay_lemmas.keys()):\n",
      "            line  = \"x,\" # good or bad\n",
      "            line += t+\",\" #token\n",
      "            line += '|'.join(okay_lemmas[t])\n",
      "            line += \"\\n\" \n",
      "            f.write(line)\n",
      "\n",
      "def update_files():\n",
      "    '''\n",
      "    Loads lemmas noted as correct from tocheck.csv, \n",
      "    Writes okay_lemmas as input/okay.csv\n",
      "    Regenerates output/tocheck.csv\n",
      "    '''\n",
      "    update_okay() \n",
      "    write_okay()\n",
      "    update_to_check()\n",
      "    print_stats()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "update_files()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Currently there are  4486  out of  4486\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Concordance Details\n",
      "\n",
      "Generates \"output/conc_details.csv\" which lists lemmas and their sources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lemmas_out = defaultdict(set)\n",
      "\n",
      "\n",
      "for k,v in okay_lemmas.iteritems(): # k = word; v = lemmas\n",
      "    for l in v:\n",
      "        lemmas_out[l].add(k)\n",
      "\n",
      "with open('output/conc_details.csv','w') as f:\n",
      "    for k,v in sorted(lemmas_out.iteritems()):\n",
      "        f.write(k+','+'|'.join(v)+'\\n')\n",
      "        \n",
      "#okay_lemmas.keys()[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Lemma Instances\n",
      "Sorted list of lemma instances."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def instances_of_lemma(lemma):\n",
      "    i=0\n",
      "    for x in lemmas_out[lemma]:\n",
      "        i+= token_instance_count[x]\n",
      "    return i\n",
      "\n",
      "lemma_instance_count = {lemma: instances_of_lemma(lemma) for lemma in lemmas_out.keys()}\n",
      "#instances_of_lemma for\n",
      "#zz=sorted(lemmas_out.keys(),key=instances_of_lemma)#sort_by_instances)#size_of_lemma_by_instances)\n",
      "#for z in zz: print z, instances_of_lemma[zz])\n",
      "with open(\"output/statistics/lemma-counts.csv\",\"w\") as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        f.write(x+','+str(lemma_instance_count[x])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Izafats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am not sure yet how we will wind up using these. Probably based on a token location range, similarly to compound verbs, etc. There may be some combos I am not grabbing properly. These will need to lemma-ed later (e.g. nasalization)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "izafat_verse_ids = [v_id for v_id in sorted(verses.keys()) if re.search('-e ',verses[v_id])]\n",
      "izafat_verses = [verses[v_id] for v_id in izafat_verse_ids]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "izafat_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+')\n",
      "izafats=Counter()\n",
      "for s in izafat_verses:\n",
      "    x = izafat_re.findall(s)#re.findall(m,s)\n",
      "    for y in x:\n",
      "        izafats[y]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "with open('output/izafats.csv','w') as f:\n",
      "    f.write('\\n'.join(sorted(izafats.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here also is a version of the tokens where izafat phrases are treated as individual tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "iast=Counter() # izafats as tokens, along with tokens\n",
      "iast_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+|[^ ]+')\n",
      "for i,s in verses.iteritems():\n",
      "    words = iast_re.findall(s)\n",
      "    for t in words:\n",
      "        iast[t]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Statistics\n",
      "Word frequencies, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def make_csv_of_token_freq(d, filename):\n",
      "    '''\n",
      "    Generates a CSV file of a dictionary based on numeric value of key, reverse sorted\n",
      "    d: dictionary of tokens and values(token: #)\n",
      "    filename = output file name\n",
      "    '''\n",
      "    with open(filename,'w') as f:\n",
      "        for k,v in d.most_common():\n",
      "            f.write(k+','+str(v)+'\\n')\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "make_csv_of_token_freq(izafats, 'output/statistics/izafat-freq.csv')\n",
      "make_csv_of_token_freq(unique_tokens, 'output/statistics/uniquetokens-freq.csv')\n",
      "make_csv_of_token_freq(iast, 'output/statistics/izafatastokens-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "type(izafats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "collections.Counter"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lemma_counts_beta=Counter()\n",
      "\n",
      "for token, count in unique_tokens.iteritems():\n",
      "    if token in okay_lemmas:\n",
      "        lemma = okay_lemmas[token][0]\n",
      "    else:\n",
      "        lemma = token\n",
      "    lemma_counts_beta[lemma]+=count\n",
      "lemma_counts_beta\n",
      "make_csv_of_token_freq(lemma_counts_beta,'output/statistics/lemmas-beta-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# the following will generate the urdu versions of the statistics (a little slow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import generate_urdu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#redo here\n",
      "reload(generate_urdu)#generate_urdu.write_all_urdu_statistics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "<module 'generate_urdu' from 'generate_urdu.pyc'>"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quick and Dirty Output\n",
      "\n",
      "This generates some quick output for proofing as .md; this a bit sloppy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "with open('output/lemmas-by-size.txt','w') as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        words=lemmas_out[x]\n",
      "        words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "        f.write(x+' '+str(lemma_instance_count[x])+'\\n')\n",
      "        for w in words:\n",
      "            f.write(\"  - \"+w+' '+str(token_instance_count[w])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import sys\n",
      "sys.path.append('./graphparser/')\n",
      "import graphparser\n",
      "urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "urdudiacriticsp = graphparser.GraphParser('./graphparser/settings/urdu-diacritics.yaml')\n",
      "for x in [urdup, urdudiacriticsp,nagarip]: print x.parse(' jur))at ma))aal').output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \u062c\u0631\u0623\u062a \u0645\u0622\u0644\n",
        " \u062c\u064f\u0631\u0652\u0623\u062a \u0645\u064e\u0622\u0644\n",
        " \u091c\u0941\u0930\u0905\u0924 \u092e\u0906\u0932\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\n",
      "def gen_hiur_lemmas_by_size():\n",
      "    import codecs\n",
      "    import sys\n",
      "    sys.path.append('./graphparser/')\n",
      "    import graphparser\n",
      "    urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "    nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "    def out_hiur(w):\n",
      "        return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "    with codecs.open('output/lemmas-by-size-hiur.md','w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "\n",
      "            f.write(out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "            for w in words:\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                \n",
      "def out_hiur(w):\n",
      "    return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "\n",
      "def out_hiur_csv(w):\n",
      "    return urdup.parse(w).output+','+nagarip.parse(w).output+','+w\n",
      "def html_out(w):\n",
      "    return td(urdup.parse(w).output)+td(nagarip.parse(w).output)+td(w)\n",
      "\n",
      "def td(x):\n",
      "    return '<td>'+x+'</td>'\n",
      "\n",
      "def li(x):\n",
      "    return ('<li>'+x+'</li>')\n",
      "def md_link(s,urdu=True):\n",
      "    out =  \" [\"+s+\"]\"\n",
      "    out += \"(\"+'http://www.columbia.edu/itc/mealac/pritchett/00ghalib/'\n",
      "    out += s[0:3]+'/'+s[0:3]+\"_\"+s[4:6]+\".html\"\n",
      "    if urdu==True:\n",
      "        out+=\"?urdu\"\n",
      "    out += \") \"#\n",
      "    return out\n",
      "\n",
      "def get_url(s,urdu=False):\n",
      "    url='http://www.columbia.edu/itc/mealac/pritchett/00ghalib/'+s[0:3]+'/'+str(int(s[0:3]))+\"_\"+s[4:6]+\".html\"\n",
      "    if urdu:\n",
      "        url+='?urdu'\n",
      "    return url\n",
      "\n",
      "def a_link(s,urdu=False):\n",
      "    url=get_url(s,urdu)\n",
      "    out = '<a href=\"'+url+'\">'+s+'</a>'\n",
      "    return out\n",
      "\n",
      "def gen_hiur_lemmas_by_size_hiur(file_name, with_verses=False, truncate=True,truncate_limit=50):\n",
      "\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_o+ut[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "            \n",
      "            f.write(' '+out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "\n",
      "            for w in words:\n",
      "                f.write(\"  - \")\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                vi = set(x[:-5] for x in token_instances[w]) # eg001.01 from 001.01.01.0\n",
      "\n",
      "                if with_verses==True:\n",
      "                    if (truncate==False) or (truncate==True and len (vi)< truncate_limit):\n",
      "    #                print list(vi)[0]\n",
      "                        f.write(\"    - \")# nested indent\n",
      "                        f.write(', '.join([md_link(v) for v in vi]))\n",
      "                        f.write('\\n')\n",
      "\n",
      "def gen_hiur_lemmas_by_size_ul(file_name='output/hiur-lemmas-by-size-ul.html'):\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        f.write('<!DOCTYPE html>\\n')\n",
      "        f.write('<html lang=\"en-US\">\\n')\n",
      "        f.write('<head><meta charset=\"utf-8\"></head>\\n')\n",
      "        f.write('<body>\\n')\n",
      "\n",
      "\n",
      "        f.write('<table>\\n')\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "            f.write('<p><b>'+out_hiur(x)+' '+str(lemma_instance_count[x])+'</b></p>\\n')\n",
      "            f.write('<ul>\\n')\n",
      "            for w in words:\n",
      "                f.write('<li>'+out_hiur(w)+' '+str(token_instance_count[w])+'</li>\\n')\n",
      "            f.write(\"</ul>\")\n",
      "\n",
      "        f.write(\"</body></html>\")\n",
      "\n",
      "def gen_hiur_lemmas(filename='output/hiur-lemmas.html'):\n",
      "    with codecs.open(filename,'w','utf-8') as f:\n",
      "        f.write('<!DOCTYPE html>\\n')\n",
      "        f.write('<html lang=\"en-US\">\\n')\n",
      "        f.write('<head><meta charset=\"utf-8\"></head>\\n')\n",
      "        f.write(\"<body><table>\")\n",
      "        for l,tkns in sorted(lemmas_out.iteritems()):\n",
      "            locs=[]\n",
      "            for t in tkns:\n",
      "                locs += [v[0:6] for v,t_x in tokens.iteritems() if t_x ==t]\n",
      "            locs=sorted(list(set(sorted(locs))))\n",
      "            hyperlocs = [a_link(loc,urdu=False) for loc in locs]\n",
      "            f.write('<tr>'+td(l)+td(urdup.parse(l).output)+td(nagarip.parse(l).output)+td(', '.join(hyperlocs))+'</tr>\\n')\n",
      "#                    print l,urdup.parse(l).output,locs\n",
      "        f.write(\"</table></body></html>\")\n",
      "a_link('101.01')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "'<a href=\"http://www.columbia.edu/itc/mealac/pritchett/00ghalib/101/101_01.html\">101.01</a>'"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "gen_hiur_lemmas()    \n",
      "#gen_hiur_lemmas_by_size()\n",
      "#gen_hiur_lemmas_by_size_with_verses()\n",
      "#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-w-verses-all-hiur.md', with_verses=True, truncate=False)#True,truncate_limit=50):\n",
      "#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-countsonly.md', with_verses=False)#True,truncate_limit=50):\n",
      "gen_hiur_lemmas_by_size_ul()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def gen_hiur_lemmas_by_size_md(file_name='output/hiur-lemmas-by-size.md'):\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        f.write('# Lemmas and Tokens (Sorted by Number of Occurences)\\n\\n')\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "            f.write('\\n'+out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "            for w in words:\n",
      "                f.write('* '+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                \n",
      "gen_hiur_lemmas_by_size_md()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Experimenting with Lemma version of text (for topic modeling)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def gen_documents(file_name = 'output/lemma_documents.txt'):\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        it = iter(sorted(verses))\n",
      "        for x in it:\n",
      "            v_id0,v_id1 = x,next(it)\n",
      "            #print v_id0,v_id1\n",
      "            lemmastring = ''\n",
      "            for v in [v_id0,v_id1]:\n",
      "            #print v\n",
      "                vtkns = [t for t in tokens if t.startswith(v)]\n",
      "                for t in vtkns:\n",
      "                    l = okay_lemmas[tokens[t]]\n",
      "                    if len(l)>1:\n",
      "                        while '-o-' in l[0]:\n",
      "                            l=l[1:]\n",
      "                    lemmas_out = ' '.join(l)\n",
      "                    lemmastring+=' '+lemmas_out\n",
      "\n",
      "            f.write(lemmastring+'\\n')\n",
      "gen_documents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import PyICU\n",
      "import PyICU\n",
      "locale=PyICU.Locale('ur')\n",
      "urducol = PyICU.Collator.createInstance(locale)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import graphparser\n",
      "reload(graphparser)\n",
      "\n",
      "urdup = graphparser.GraphParser('./graphparser/settings/urdu-diacritics.yaml')\n",
      "urdudiacriticsp = graphparser.GraphParser('./graphparser/settings/urdu-diacritics.yaml')\n",
      "#nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "lemmas_diacritics = {urdudiacriticsp.parse(x).output:x for x in lemmas_out }\n",
      "\n",
      "urdu_lemmas = [urdudiacriticsp.parse(x).output for x in lemmas_out]\n",
      "urdu_lemmas_sorted = sorted(urdu_lemmas,urducol.compare)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "urducol.compare('\u0628','\u0627')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#for x in sorted(set([urdup.parse(t).output for t in tokens.values() if t.endswith('-e')] ), col.compare): print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import codecs\n",
      "def gen_concordance(filename='output/concordance-urdu.html'):\n",
      "\n",
      "    with codecs.open(filename,'w','utf-8') as f:\n",
      "        f.write('<!DOCTYPE html>\\n')\n",
      "        f.write('<html lang=\"ur-PK\">\\n')\n",
      "        f.write('<head><meta charset=\"utf-8\"></head>\\n')\n",
      "        f.write(\"<body><table>\")\n",
      "        for l_d in urdu_lemmas_sorted:\n",
      "            tkns = lemmas_out[lemmas_diacritics[l_d]]\n",
      "            \n",
      "#        for l,tkns in sorted(lemmas_out.iteritems(),urducol.compare):\n",
      "            locs=[]\n",
      "            for t in tkns:\n",
      "                locs += [v[0:6] for v,t_x in tokens.iteritems() if t_x ==t]\n",
      "            locs=sorted(list(set(sorted(locs))))\n",
      "            hyperlocs = [a_link(loc,urdu=True) for loc in locs]\n",
      "            f.write('<tr>'+td(l_d)+td(', '.join(hyperlocs))+'</tr>\\n')\n",
      "#                    print l,urdup.parse(l).output,locs\n",
      "        f.write(\"</table></body></html>\")\n",
      "gen_concordance()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lemmas_diacritics = {urdudiacriticsp.parse(x).output:x for x in lemmas_out }\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#sorted(lemmas_diacritics.keys(),urducol.compare)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def tex_link(s,urdu=False):\n",
      "    url=get_url(s,urdu)\n",
      "    out = '\\href{'+url+'}{'+s+'}'\n",
      "    return out\n",
      "\n",
      "\n",
      "out=''\n",
      "xetex_header=r'''\n",
      "%!TEX TS-program = xelatex\n",
      "%!TEX encoding = UTF-8 Unicode\n",
      "\n",
      "\\documentclass[12pt]{article}\n",
      "\\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.\n",
      "\n",
      "\\geometry{letterpaper}                   % ... or a4paper or a5paper or ... \n",
      "\n",
      "%\\geometry{landscape}                % Activate for for rotated page geometry\n",
      "%\\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent\n",
      "%\\usepackage{graphicx}\n",
      "%\\usepackage{amssymb}\n",
      "\\usepackage{hyperref}\n",
      "\\usepackage{longtable}\n",
      "\\usepackage{fontspec}\n",
      "\\newfontfamily\\ur[Script=Arabic,Scale=1.4]{Jameel Noori Nastaleeq}\n",
      "\\setromanfont[Mapping=tex-text]{Hoefler Text}\n",
      "\\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}\n",
      "\\setmonofont[Scale=MatchLowercase]{Andale Mono}\n",
      "\\usepackage{tabularx}\n",
      "%\\title{Brief Article}\n",
      "%\\author{The Author}\n",
      "%\\date{}                                           % Activate to display a given date or no date\n",
      "\\usepackage{bidi}\n",
      "%\\usepackage{bidipoem}\n",
      "\n",
      "\\usepackage{ltablex}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{tabularx}{\\linewidth}{@{}cX@{}}   \n",
      "'''\n",
      "out=xetex_header\n",
      "for l_d in urdu_lemmas_sorted:\n",
      "    tkns = lemmas_out[lemmas_diacritics[l_d]]\n",
      "    locs=[]\n",
      "    for t in tkns:\n",
      "        locs += [v[0:6] for v,t_x in tokens.iteritems() if t_x ==t]\n",
      "    locs=sorted(list(set(sorted(locs))))\n",
      "    hyperlocs_s = '\\n'.join([tex_link(loc,urdu=True) for loc in locs])\n",
      "    \n",
      "    out+='{\\\\ur %s} & %s \\\\\\\\\\n'%(l_d,hyperlocs_s)\n",
      "    \n",
      "out+=r'''\n",
      "\n",
      "\\end{tabularx}   \n",
      "\n",
      "\n",
      "\\end{document}\n",
      "\n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('output/tex/conc-urdu.tex','w','utf8') as f:\n",
      "    f.write(out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tex_link(s,urdu=False):\n",
      "    url=get_url(s,urdu)\n",
      "    out = '\\href{'+url+'}{'+s+'}'\n",
      "    return out\n",
      "\n",
      "\n",
      "out=''\n",
      "xetex_header=r'''\n",
      "%!TEX TS-program = xelatex\n",
      "%!TEX encoding = UTF-8 Unicode\n",
      "\n",
      "\\documentclass[12pt]{article}\n",
      "\\usepackage[margin=.5in]{geometry}                % See geometry.pdf to learn the layout options. There are lots.\n",
      "\n",
      "\n",
      "\n",
      "\\geometry{letterpaper}                   % ... or a4paper or a5paper or ... \n",
      "\n",
      "\\geometry{landscape}                % Activate for for rotated page geometry\n",
      "%\\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent\n",
      "%\\usepackage{graphicx}\n",
      "%\\usepackage{amssymb}\n",
      "\\usepackage{hyperref}\n",
      "\\usepackage{longtable}\n",
      "\\usepackage{fontspec}\n",
      "\\newfontfamily\\ur[Script=Arabic,Scale=1.4]{Jameel Noori Nastaleeq}\n",
      "\\setromanfont[Mapping=tex-text]{Hoefler Text}\n",
      "\\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}\n",
      "\\setmonofont[Scale=MatchLowercase]{Andale Mono}\n",
      "\\usepackage{tabularx}\n",
      "%\\title{Brief Article}\n",
      "%\\author{The Author}\n",
      "%\\date{}                                           % Activate to display a given date or no date\n",
      "\\usepackage{multicol}\n",
      "\\usepackage{bidi}\n",
      "\n",
      "%\\usepackage{bidipoem}\n",
      "\n",
      "\\usepackage{ltablex}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{RTL}\n",
      "\n",
      "\\raggedleft\n",
      " \n",
      "\\begin{multicols}{5}\n",
      "'''\n",
      "\n",
      "out=xetex_header\n",
      "for l_d in urdu_lemmas_sorted:\n",
      "    tkns = lemmas_out[lemmas_diacritics[l_d]]\n",
      "    locs=[]\n",
      "    for t in tkns:\n",
      "        locs += [v[0:6] for v,t_x in tokens.iteritems() if t_x ==t]\n",
      "    instances = len(locs)\n",
      "    locs=sorted(list(set(sorted(locs))))\n",
      "    hyperlocs_s = '\\n'.join([tex_link(loc,urdu=True) for loc in locs])\n",
      "    \n",
      "    out+='{\\\\ur %s}   '%l_d# %s \\\\\\\\\\n'%(l_d,hyperlocs_s)\n",
      "    out+='\\\\textsuperscript{'+str(instances+1)+'} '\n",
      "    out+=lemmas_diacritics[l_d]+'\\n\\n'\n",
      "out+=r'''\n",
      "\n",
      " \n",
      "\n",
      "\\end{multicols}\n",
      "\\end{RTL}\n",
      "\\end{document}\n",
      "\n",
      "'''\n",
      "\n",
      "with codecs.open('output/tex/lemmas.tex','w','utf8') as f:\n",
      "    f.write(out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parser = urdudiacriticsp.parse\n",
      "t_i = token_instances['parvaanah']\n",
      "verse_indexes = sorted(set([ x[:7] for x in t_i]))\n",
      "def highlight(s):\n",
      "    return '<b>'+s+'</b>'\n",
      "out=''\n",
      "for v_i in verse_indexes:\n",
      "\n",
      "    out +=\"<p>\"+a_link(v_i[:6])+\"</p>\"\n",
      "    first_tkns = sorted([x for x in tokens.keys() if (x.startswith(v_i) and x[-4]=='0')])# and x.endswith('.0')]\n",
      "    second_tkns = sorted([x for x in tokens.keys() if x.startswith(v_i) and x[-4]=='1'])\n",
      "    out += '<p>'\n",
      "    for t in first_tkns:\n",
      "        s = tokens[t]\n",
      "        if parser:\n",
      "            s = parser(s).output\n",
      "        if t in t_i:\n",
      "            s=highlight(s)\n",
      "        out+=s+' '\n",
      "    out+='<br/>'\n",
      "    for t in second_tkns:\n",
      "        s = tokens[t]\n",
      "        if parser:\n",
      "            s = parser(s).output\n",
      "\n",
      "\n",
      "        if t in t_i:\n",
      "            s=highlight(s)\n",
      "        out+=s+' '\n",
      "        \n",
      "    \n",
      "#    print out\n",
      "\n",
      "from IPython.core.display import HTML\n",
      "\n",
      "HTML(out)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p><a href=\"http://www.columbia.edu/itc/mealac/pritchett/00ghalib/045/45_05.html\">045.05</a></p><p>\u062c\u0627\u06ba \u062f\u064e\u0631 \u06c1\u064e\u0648\u0627\u0626\u06d2 \u06cc\u064e\u06a9 \u0646\u0650\u06af\u064e\u06c2 \u06af\u064e\u0631\u0652\u0645 \u06c1\u06d2 \u0627\u064e\u0633\u064e\u062f <br/><b>\u067e\u064e\u0631\u0652\u0648\u0627\u0646\u064e\u06c1</b> \u06c1\u06d2 \u0648\u064e\u06a9\u0650\u06cc\u0644 \u062a\u0650\u0631\u06d2 \u062f\u0627\u062f \u062e\u0652\u0648\u0627\u06c1 \u06a9\u0627 <p><a href=\"http://www.columbia.edu/itc/mealac/pritchett/00ghalib/075/75_04.html\">075.04</a></p><p>\u063a\u064e\u0645 \u0627\u064f\u0633 \u06a9\u0648 \u062d\u064e\u0633\u0652\u0631\u064e\u062a\u0650 <b>\u067e\u064e\u0631\u0652\u0648\u0627\u0646\u064e\u06c1</b> \u06a9\u0627 \u06c1\u06d2 \u0627\u064e\u06cc \u0634\u064f\u0639\u0652\u0644\u06d2 <br/>\u062a\u0650\u0631\u06d2 \u0644\u064e\u0631\u064e\u0632\u0652\u0646\u06d2 \u0633\u06d2 \u0638\u0627\u06c1\u0650\u0631 \u06c1\u06d2 \u0646\u0627 \u062a\u064e\u0648\u0627\u0646\u06cc\u0650 \u0634\u064e\u0645\u0652\u0639\u064e <p><a href=\"http://www.columbia.edu/itc/mealac/pritchett/00ghalib/081/81_03.html\">081.03</a></p><p>\u0628\u0627 \u0648\u064f\u062c\u064f\u0648\u062f\u0650 \u06cc\u064e\u06a9 \u062c\u064e\u06c1\u0627\u06ba \u06c1\u064e\u0646\u0652\u06af\u0627\u0645\u064e\u06c1 \u067e\u0650\u06cc\u062f\u0627\u0626\u06cc \u0646\u064e\u06c1\u0650\u06cc\u06ba <br/>\u06c1\u0650\u06cc\u06ba \u0686\u0650\u0631\u0627\u063a\u0627\u0646\u0650 \u0634\u064e\u0628\u0650\u0633\u0652\u062a\u0627\u0646\u0650 \u062f\u0650\u0644\u0650 <b>\u067e\u064e\u0631\u0652\u0648\u0627\u0646\u064e\u06c1</b> \u06c1\u064e\u0645 <p><a href=\"http://www.columbia.edu/itc/mealac/pritchett/00ghalib/166/166_03.html\">166.03</a></p><p>\u067e\u064e\u0631\u0650 <b>\u067e\u064e\u0631\u0652\u0648\u0627\u0646\u064e\u06c1</b> \u0634\u0627\u06cc\u064e\u062f \u0628\u0627\u062f\u0652\u0628\u0627\u0646\u0650 \u06a9\u0650\u0634\u0652\u062a\u06cc\u0650 \u0645\u06d2 \u062a\u06be\u0627 <br/>\u06c1\u064f\u0648\u0626\u06cc \u0645\u064e\u062c\u0652\u0644\u0650\u0633 \u06a9\u06cc \u06af\u064e\u0631\u0652\u0645\u06cc \u0633\u06d2 \u0631\u064e\u0648\u0627\u0646\u06cc \u062f\u0648\u0631\u0650 \u0633\u0627\u063a\u064e\u0631 \u06a9\u06cc "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "<IPython.core.display.HTML at 0x107b3fe10>"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}