{
 "metadata": {
  "name": "",
  "signature": "sha256:7d9e50b65ef5ab2285a9baf3978f687e4f21e4f1b268fb730e25fd7f34b03f34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('graphparser')\n",
      "import graphparser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nagarip = graphparser.GraphParser('graphparser/settings/devanagari.yaml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<z_consonant> + <z_consonant>\n",
        "<m> + <h_char>\n",
        "<n> + consonant\n",
        "<k> + <kh>\n",
        "<r> + <r>\n",
        "<r> + <z>\n",
        "<l> + <h>\n",
        "<v> + <v>\n",
        "<;t> + <;th>\n",
        "<d> + <d>\n",
        "<consonant> <vowel> <s> + <t> <long_vowel>\n",
        "<consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <vowel> <vowel_nasal> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <short_vowel> <h_char> + <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel> <vowel_nasal>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <short_vowel>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant>\n",
        "<consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <vowel> <consonant> + <consonant> <vowel>\n",
        "<wb> <consonant> <short_vowel> <z> + <z> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<n> + <consonant>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nagarip.parse('aakaa').output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['short_vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['short_vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u093e', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u093e', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "trying to match rule  ParserRule(production=u'\\u0915', prev_classes=None, prev_tokens=None, tokens=['k'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0915', prev_classes=None, prev_tokens=None, tokens=['k'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['short_vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['short_vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0906', prev_classes=['vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0906', prev_classes=['vowel'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u093e', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u093e', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "\u093e\u0915\u093e\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nagarip.old_parser.rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[{'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092f\\u0939\\u0940',\n",
        "  'tokens': ['y', 'i', 'h', 'ii']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u090f\\u0901',\n",
        "  'tokens': ['uu', '))', 'e', ';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u0913\\u0902',\n",
        "  'tokens': ['uu', '))', 'o', ';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0935\\u0939\\u0940',\n",
        "  'tokens': ['v', 'u', 'h', 'ii']},\n",
        " {'prev_classes': ['wb'],\n",
        "  'production': u'\\u090f\\u0924',\n",
        "  'tokens': ['i', '((', 't', 'i']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u0939\\u093e-\\u090f-',\n",
        "  'tokens': ['-', 'h', 'aa', '-e']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0915\\u093f',\n",
        "  'tokens': ['k', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0935\\u094b',\n",
        "  'tokens': ['v', 'u', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092a\\u0947',\n",
        "  'tokens': ['p', 'a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092f\\u0947',\n",
        "  'tokens': ['y', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u091a\\u0947',\n",
        "  'tokens': ['ch', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n', 'a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092c',\n",
        "  'tokens': ['b', 'a', 'h']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', '((', 'aa']},\n",
        " {'prev_classes': ['wb'],\n",
        "  'production': u'\\u0959\\u0941',\n",
        "  'tokens': [';x', 'v', 'u']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u0939\\u093e',\n",
        "  'tokens': ['-', 'h', 'aa']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u0906',\n",
        "  'tokens': ['u', '((', 'aa']},\n",
        " {'production': u'\\u2010\\u0913\\u2010', 'tokens': ['-', 'o', '-']},\n",
        " {'production': u'\\u0928\\u0924\\u0940', 'tokens': ['n', 't', 'ii']},\n",
        " {'production': u'\\u0928\\u0924\\u0947', 'tokens': ['n', 't', 'e']},\n",
        " {'production': u'\\u0928\\u0924\\u093e', 'tokens': ['n', 't', 'aa']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb', 'consonant', 'short_vowel', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['izaafat'],\n",
        "  'prev_classes': ['wb', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['a', '((']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['short_vowel'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['((', 'a']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928\\u094d\\u0928',\n",
        "  'tokens': ['n', 'n']},\n",
        " {'next_classes': ['izaafat'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['a', '((']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0909', 'tokens': ['((', 'u']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0907', 'tokens': ['((', 'i']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0905', 'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0907', 'tokens': ['((', 'i']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', '((']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0914', 'tokens': ['((', 'au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0910', 'tokens': ['((', 'ai']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['((', 'aa']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0905',\n",
        "  'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0908', 'tokens': ['((', 'ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0908', 'tokens': ['((', 'ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0905', 'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u094b',\n",
        "  'tokens': [';o', '((']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['r', 'consonant', 'candrabindu_vowel'],\n",
        "  'production': u'\\u0902',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel', 'e'],\n",
        "  'production': u'\\u0901',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['candrabindu_vowel'],\n",
        "  'production': u'\\u0901',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['<izaafat'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['((']},\n",
        " {'next_classes': ['consonant'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0902',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['((']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0909', 'tokens': ['u']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090f', 'tokens': [';e']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0913', 'tokens': [';o']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090f', 'tokens': ['e']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0907', 'tokens': ['i']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0913', 'tokens': ['o']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0908', 'tokens': ['ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090a', 'tokens': ['uu']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u090f\\u2010',\n",
        "  'tokens': ['-e']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['izaafat'], 'production': '', 'tokens': ' '},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090f', 'tokens': ['e']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0907', 'tokens': ['i']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0913', 'tokens': ['o']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0909', 'tokens': ['u']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090a', 'tokens': ['uu']},\n",
        " {'prev_classes': ['consonant'], 'production': u'\\u094b', 'tokens': [';o']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0913', 'tokens': [';o']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090b', 'tokens': ['.ri']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090b', 'tokens': ['.ri']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0908', 'tokens': ['ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'production': '', 'tokens': ['\\x00']},\n",
        " {'production': u'\\u0943', 'tokens': ['.ri']},\n",
        " {'production': u'\\u0918', 'tokens': ['gh']},\n",
        " {'production': ' ', 'tokens': ' '},\n",
        " {'production': u'\\u095d', 'tokens': [';rh']},\n",
        " {'production': '(', 'tokens': ['(']},\n",
        " {'production': ',', 'tokens': [',']},\n",
        " {'production': u'\\u0925', 'tokens': ['th']},\n",
        " {'production': u'\\u0928', 'tokens': [':n']},\n",
        " {'production': '\\n', 'tokens': ['\\n']},\n",
        " {'production': u'\\u0927', 'tokens': ['dh']},\n",
        " {'production': '', 'tokens': ['((']},\n",
        " {'production': u'\\u095b', 'tokens': [':z']},\n",
        " {'production': u'\\u0924', 'tokens': [':t']},\n",
        " {'production': u'\\u0926', 'tokens': ['d']},\n",
        " {'production': u'\\u0939', 'tokens': ['h']},\n",
        " {'production': u'\\u0932', 'tokens': ['l']},\n",
        " {'production': u'\\u092a', 'tokens': ['p']},\n",
        " {'production': u'\\u0924', 'tokens': ['t']},\n",
        " {'production': '', 'tokens': ['|']},\n",
        " {'production': u'\\u0938', 'tokens': [';s']},\n",
        " {'production': u'\\u095c', 'tokens': [';r']},\n",
        " {'production': u'\\u091d\\u093c', 'tokens': ['zh']},\n",
        " {'production': u'\\u091f', 'tokens': [';t']},\n",
        " {'production': u'\\u095b', 'tokens': [';z']},\n",
        " {'production': u'\\u0959', 'tokens': [';x']},\n",
        " {'production': u'\\u0922', 'tokens': [';dh']},\n",
        " {'production': u'\\u0947', 'tokens': [';e']},\n",
        " {'production': u'\\u0921', 'tokens': [';d']},\n",
        " {'production': u'\\u0939', 'tokens': [';h']},\n",
        " {'production': u'\\u0902', 'tokens': [';n']},\n",
        " {'production': u'\\u0901', 'tokens': [';m']},\n",
        " {'production': u'\\u0920', 'tokens': [';th']},\n",
        " {'production': '', 'tokens': [\"'\"]},\n",
        " {'production': '', 'tokens': ['/']},\n",
        " {'production': u';', 'tokens': [';']},\n",
        " {'production': '?', 'tokens': ['?']},\n",
        " {'production': u'\\u092d', 'tokens': ['bh']},\n",
        " {'production': u'\\u091d', 'tokens': ['jh']},\n",
        " {'production': '', 'tokens': ['_']},\n",
        " {'production': u'\\u0938', 'tokens': ['.s']},\n",
        " {'production': u'\\u0917', 'tokens': ['g']},\n",
        " {'production': u'\\u0915', 'tokens': ['k']},\n",
        " {'production': u'\\u093e', 'tokens': [';aa']},\n",
        " {'production': u'\\u094b', 'tokens': ['o']},\n",
        " {'production': u'\\u0938', 'tokens': ['s']},\n",
        " {'production': '', 'tokens': ['w']},\n",
        " {'production': u'\\u091a', 'tokens': ['ch']},\n",
        " {'production': '', 'tokens': ['))']},\n",
        " {'production': u'\\u091b', 'tokens': ['chh']},\n",
        " {'production': u'\\u092b', 'tokens': ['ph']},\n",
        " {'production': u'\\u092c', 'tokens': ['b']},\n",
        " {'production': u'\\u095e', 'tokens': ['f']},\n",
        " {'production': u'\\u0942', 'tokens': ['uu']},\n",
        " {'production': u'\\u091c', 'tokens': ['j']},\n",
        " {'production': u'\\u0928', 'tokens': ['n']},\n",
        " {'production': u'\\u095b', 'tokens': ['.z']},\n",
        " {'production': u'\\u0930', 'tokens': ['r']},\n",
        " {'production': u'\\u0935', 'tokens': ['v']},\n",
        " {'production': u'\\u095b', 'tokens': ['z']},\n",
        " {'production': u'\\u093e', 'tokens': ['aa']},\n",
        " {'production': u'\\u0916', 'tokens': ['kh']},\n",
        " {'production': u'\\u0948', 'tokens': ['ai']},\n",
        " {'production': u'\\u0940', 'tokens': ['ii']},\n",
        " {'production': u'\\u094c', 'tokens': ['au']},\n",
        " {'production': ')', 'tokens': [')']},\n",
        " {'production': '', 'tokens': ['-']},\n",
        " {'production': u'\\u095a', 'tokens': [';g']},\n",
        " {'production': '', 'tokens': ['a']},\n",
        " {'production': u'\\u0947', 'tokens': ['e']},\n",
        " {'production': u'\\u093f', 'tokens': ['i']},\n",
        " {'production': u'\\u092e', 'tokens': ['m']},\n",
        " {'production': u'\\u0958', 'tokens': ['q']},\n",
        " {'production': u'\\u0936', 'tokens': ['sh']},\n",
        " {'production': u'\\u0941', 'tokens': ['u']},\n",
        " {'production': u'\\u092f', 'tokens': ['y']}]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nagarip.old_parser.parse('dil-e ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u0926\u093f\u0932\u2010\u090f\u2010\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    nagarip."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nagarip.old_parser.rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[{'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092f\\u0939\\u0940',\n",
        "  'tokens': ['y', 'i', 'h', 'ii']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u090f\\u0901',\n",
        "  'tokens': ['uu', '))', 'e', ';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u0913\\u0902',\n",
        "  'tokens': ['uu', '))', 'o', ';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0935\\u0939\\u0940',\n",
        "  'tokens': ['v', 'u', 'h', 'ii']},\n",
        " {'prev_classes': ['wb'],\n",
        "  'production': u'\\u090f\\u0924',\n",
        "  'tokens': ['i', '((', 't', 'i']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u0939\\u093e-\\u090f-',\n",
        "  'tokens': ['-', 'h', 'aa', '-e']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0915\\u093f',\n",
        "  'tokens': ['k', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0935\\u094b',\n",
        "  'tokens': ['v', 'u', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092a\\u0947',\n",
        "  'tokens': ['p', 'a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092f\\u0947',\n",
        "  'tokens': ['y', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u091a\\u0947',\n",
        "  'tokens': ['ch', 'i', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n', 'a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb'],\n",
        "  'production': u'\\u092c',\n",
        "  'tokens': ['b', 'a', 'h']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', '((', 'aa']},\n",
        " {'prev_classes': ['wb'],\n",
        "  'production': u'\\u0959\\u0941',\n",
        "  'tokens': [';x', 'v', 'u']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u0939\\u093e',\n",
        "  'tokens': ['-', 'h', 'aa']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0941\\u0906',\n",
        "  'tokens': ['u', '((', 'aa']},\n",
        " {'production': u'\\u2010\\u0913\\u2010', 'tokens': ['-', 'o', '-']},\n",
        " {'production': u'\\u0928\\u0924\\u0940', 'tokens': ['n', 't', 'ii']},\n",
        " {'production': u'\\u0928\\u0924\\u0947', 'tokens': ['n', 't', 'e']},\n",
        " {'production': u'\\u0928\\u0924\\u093e', 'tokens': ['n', 't', 'aa']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb', 'consonant', 'short_vowel', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['wb', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['izaafat'],\n",
        "  'prev_classes': ['wb', 'consonant'],\n",
        "  'production': u'\\u0939',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['a', '((']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['short_vowel'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0906',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['((', 'a']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928\\u094d\\u0928',\n",
        "  'tokens': ['n', 'n']},\n",
        " {'next_classes': ['izaafat'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', 'h']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['a', '((']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0909', 'tokens': ['((', 'u']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0907', 'tokens': ['((', 'i']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0905', 'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0907', 'tokens': ['((', 'i']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u093e',\n",
        "  'tokens': ['a', '((']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0914', 'tokens': ['((', 'au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0910', 'tokens': ['((', 'ai']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['((', 'aa']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u0905',\n",
        "  'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0908', 'tokens': ['((', 'ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0908', 'tokens': ['((', 'ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0905', 'tokens': ['((', 'a']},\n",
        " {'prev_classes': ['consonant'],\n",
        "  'production': u'\\u094b',\n",
        "  'tokens': [';o', '((']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['r', 'consonant', 'candrabindu_vowel'],\n",
        "  'production': u'\\u0902',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel', 'e'],\n",
        "  'production': u'\\u0901',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['candrabindu_vowel'],\n",
        "  'production': u'\\u0901',\n",
        "  'tokens': [';n']},\n",
        " {'next_classes': ['<izaafat'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['((']},\n",
        " {'next_classes': ['consonant'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0902',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['vowel'],\n",
        "  'prev_classes': ['vowel'],\n",
        "  'production': u'\\u0928',\n",
        "  'tokens': ['n']},\n",
        " {'next_classes': ['wb'],\n",
        "  'prev_classes': ['consonant'],\n",
        "  'production': '',\n",
        "  'tokens': ['((']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0909', 'tokens': ['u']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090f', 'tokens': [';e']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0913', 'tokens': [';o']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090f', 'tokens': ['e']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0907', 'tokens': ['i']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0913', 'tokens': ['o']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u0908', 'tokens': ['ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090a', 'tokens': ['uu']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'next_classes': ['wb'],\n",
        "  'production': u'\\u2010\\u090f\\u2010',\n",
        "  'tokens': ['-e']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0905', 'tokens': ['a']},\n",
        " {'prev_classes': ['izaafat'], 'production': '', 'tokens': ' '},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090f', 'tokens': ['e']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0907', 'tokens': ['i']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0913', 'tokens': ['o']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0909', 'tokens': ['u']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090a', 'tokens': ['uu']},\n",
        " {'prev_classes': ['consonant'], 'production': u'\\u094b', 'tokens': [';o']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0913', 'tokens': [';o']},\n",
        " {'prev_classes': ['short_vowel'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['wb'], 'production': u'\\u090b', 'tokens': ['.ri']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u090b', 'tokens': ['.ri']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0914', 'tokens': ['au']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0908', 'tokens': ['ii']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0906', 'tokens': ['aa']},\n",
        " {'prev_classes': ['vowel'], 'production': u'\\u0910', 'tokens': ['ai']},\n",
        " {'production': '', 'tokens': ['\\x00']},\n",
        " {'production': u'\\u0943', 'tokens': ['.ri']},\n",
        " {'production': u'\\u0918', 'tokens': ['gh']},\n",
        " {'production': ' ', 'tokens': ' '},\n",
        " {'production': u'\\u095d', 'tokens': [';rh']},\n",
        " {'production': '(', 'tokens': ['(']},\n",
        " {'production': ',', 'tokens': [',']},\n",
        " {'production': u'\\u0925', 'tokens': ['th']},\n",
        " {'production': u'\\u0928', 'tokens': [':n']},\n",
        " {'production': '\\n', 'tokens': ['\\n']},\n",
        " {'production': u'\\u0927', 'tokens': ['dh']},\n",
        " {'production': '', 'tokens': ['((']},\n",
        " {'production': u'\\u095b', 'tokens': [':z']},\n",
        " {'production': u'\\u0924', 'tokens': [':t']},\n",
        " {'production': u'\\u0926', 'tokens': ['d']},\n",
        " {'production': u'\\u0939', 'tokens': ['h']},\n",
        " {'production': u'\\u0932', 'tokens': ['l']},\n",
        " {'production': u'\\u092a', 'tokens': ['p']},\n",
        " {'production': u'\\u0924', 'tokens': ['t']},\n",
        " {'production': '', 'tokens': ['|']},\n",
        " {'production': u'\\u0938', 'tokens': [';s']},\n",
        " {'production': u'\\u095c', 'tokens': [';r']},\n",
        " {'production': u'\\u091d\\u093c', 'tokens': ['zh']},\n",
        " {'production': u'\\u091f', 'tokens': [';t']},\n",
        " {'production': u'\\u095b', 'tokens': [';z']},\n",
        " {'production': u'\\u0959', 'tokens': [';x']},\n",
        " {'production': u'\\u0922', 'tokens': [';dh']},\n",
        " {'production': u'\\u0947', 'tokens': [';e']},\n",
        " {'production': u'\\u0921', 'tokens': [';d']},\n",
        " {'production': u'\\u0939', 'tokens': [';h']},\n",
        " {'production': u'\\u0902', 'tokens': [';n']},\n",
        " {'production': u'\\u0901', 'tokens': [';m']},\n",
        " {'production': u'\\u0920', 'tokens': [';th']},\n",
        " {'production': '', 'tokens': [\"'\"]},\n",
        " {'production': '', 'tokens': ['/']},\n",
        " {'production': u';', 'tokens': [';']},\n",
        " {'production': '?', 'tokens': ['?']},\n",
        " {'production': u'\\u092d', 'tokens': ['bh']},\n",
        " {'production': u'\\u091d', 'tokens': ['jh']},\n",
        " {'production': '', 'tokens': ['_']},\n",
        " {'production': u'\\u0938', 'tokens': ['.s']},\n",
        " {'production': u'\\u0917', 'tokens': ['g']},\n",
        " {'production': u'\\u0915', 'tokens': ['k']},\n",
        " {'production': u'\\u093e', 'tokens': [';aa']},\n",
        " {'production': u'\\u094b', 'tokens': ['o']},\n",
        " {'production': u'\\u0938', 'tokens': ['s']},\n",
        " {'production': '', 'tokens': ['w']},\n",
        " {'production': u'\\u091a', 'tokens': ['ch']},\n",
        " {'production': '', 'tokens': ['))']},\n",
        " {'production': u'\\u091b', 'tokens': ['chh']},\n",
        " {'production': u'\\u092b', 'tokens': ['ph']},\n",
        " {'production': u'\\u092c', 'tokens': ['b']},\n",
        " {'production': u'\\u095e', 'tokens': ['f']},\n",
        " {'production': u'\\u0942', 'tokens': ['uu']},\n",
        " {'production': u'\\u091c', 'tokens': ['j']},\n",
        " {'production': u'\\u0928', 'tokens': ['n']},\n",
        " {'production': u'\\u095b', 'tokens': ['.z']},\n",
        " {'production': u'\\u0930', 'tokens': ['r']},\n",
        " {'production': u'\\u0935', 'tokens': ['v']},\n",
        " {'production': u'\\u095b', 'tokens': ['z']},\n",
        " {'production': u'\\u093e', 'tokens': ['aa']},\n",
        " {'production': u'\\u0916', 'tokens': ['kh']},\n",
        " {'production': u'\\u0948', 'tokens': ['ai']},\n",
        " {'production': u'\\u0940', 'tokens': ['ii']},\n",
        " {'production': u'\\u094c', 'tokens': ['au']},\n",
        " {'production': ')', 'tokens': [')']},\n",
        " {'production': '', 'tokens': ['-']},\n",
        " {'production': u'\\u095a', 'tokens': [';g']},\n",
        " {'production': '', 'tokens': ['a']},\n",
        " {'production': u'\\u0947', 'tokens': ['e']},\n",
        " {'production': u'\\u093f', 'tokens': ['i']},\n",
        " {'production': u'\\u092e', 'tokens': ['m']},\n",
        " {'production': u'\\u0958', 'tokens': ['q']},\n",
        " {'production': u'\\u0936', 'tokens': ['sh']},\n",
        " {'production': u'\\u0941', 'tokens': ['u']},\n",
        " {'production': u'\\u092f', 'tokens': ['y']}]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup = graphparser.GraphParser('graphparser/settings/urdu.yaml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.parse(\"she\").output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trying to match rule  ParserRule(production=u'\\u0634', prev_classes=None, prev_tokens=None, tokens=['sh'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0634', prev_classes=None, prev_tokens=None, tokens=['sh'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "trying to match rule  ParserRule(production=u'\\u0627\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb'])\n",
        "trying rule  ParserRule(production=u'\\u0627\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb'])\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0627\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0627\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u06d2', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb'])\n",
        "trying rule  ParserRule(production=u'\\u06d2', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb'])\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "could not find  ['wb'] in  ['wb']  were  {'\\x00': ['wb'], 'ch': ['consonant'], '\\n': ['wb'], '))': ['consonant'], 'gh': ['consonant'], '-e': ['izaafat', 'wb'], ' ': ['wb'], ';rh': ['consonant'], '\"': ['wb'], 'chh': ['consonant'], '(': ['wb'], ',': ['wb'], '.': ['wb'], 'ai': ['vowel'], 'th': ['consonant'], 'ph': ['consonant'], ':': ['wb'], ':n': ['consonant'], 'dh': ['consonant'], '----': ['wb'], '((': ['ain', 'consonant'], ':z': ['consonant'], ':t': ['consonant'], 'b': ['consonant'], 'd': ['consonant'], 'au': ['vowel', 'nc_long_vowel'], 'f': ['consonant'], '--': ['wb'], 'h': ['consonant', 'consonant_h'], 'j': ['consonant'], 'l': ['consonant'], 'n': ['consonant'], 'p': ['consonant'], '.z': ['consonant'], 'r': ['consonant'], 't': ['consonant'], 'v': ['consonant'], ';g': ['consonant'], '.r': ['consonant'], 'z': ['consonant'], '|': ['hindi_break'], ';s': ['consonant'], ';r': ['consonant'], 'aa': ['nc_long_vowel', 'alef', 'vowel'], 'zh': ['consonant'], ';t': ['consonant'], '\\t': ['wb'], ';z': ['consonant'], ';x': [';x_class', 'consonant'], '\\r': ['wb'], 'ii': ['vowel'], ';dh': ['consonant'], ';e': ['short_vowel', 'short_vowel_e'], ';d': ['consonant'], ';h': ['consonant'], ';n': ['consonant'], ';m': ['vowel_nasal'], '!': ['wb'], '(-e)': ['hidden_izaafat'], ';th': ['consonant'], \"'\": ['letter_break'], '^i': ['short_vowel'], ')': ['wb'], '-': ['wb'], '/': ['wb'], 'sh': ['consonant'], ';': ['wb'], '?': ['wb'], '^au': ['vowel', 'nc_long_vowel'], 'bh': ['consonant'], ';aa': ['vowel'], 'jh': ['consonant'], '[': ['wb'], '^ai': ['vowel'], ']': ['wb'], '_': ['hidden_break'], 'a': ['short_vowel', 'short_vowel_a'], '.s': ['consonant'], 'e': ['vowel'], 'g': ['consonant'], 'i': ['short_vowel'], 'k': ['consonant'], 'kh': ['consonant'], 'm': ['consonant'], '^u': ['short_vowel'], 'o': ['vowel', 'nc_long_vowel'], 'q': ['consonant'], 'uu': ['vowel', 'nc_long_vowel'], 's': ['consonant'], '^ii': ['vowel'], 'u': ['short_vowel'], 'w': ['consonant'], 'y': ['consonant'], '^a': ['short_vowel', 'short_vowel_a'], '^uu': ['vowel', 'nc_long_vowel']}\n",
        "trying to match rule  ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "u'\\u0634\\u06cc'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print urdup.parse(\"haa;n\").output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trying to match rule  ParserRule(production=u'\\u06c1', prev_classes=['short_vowel_a'], prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=['wb'])\n",
        "trying rule  ParserRule(production=u'\\u06c1', prev_classes=['short_vowel_a'], prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=['wb'])\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u06c1', prev_classes=None, prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u06c1', prev_classes=None, prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "trying to match rule  ParserRule(production=u'\\u0622', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0622', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "in prev_class\n",
        "trying to match rule  ParserRule(production=u'\\u0627', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0627', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "trying to match rule  ParserRule(production=u'\\u06ba', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=['wb'])\n",
        "trying rule  ParserRule(production=u'\\u06ba', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=['wb'])\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "could not find  ['wb'] in  ['wb']  were  {'\\x00': ['wb'], 'ch': ['consonant'], '\\n': ['wb'], '))': ['consonant'], 'gh': ['consonant'], '-e': ['izaafat', 'wb'], ' ': ['wb'], ';rh': ['consonant'], '\"': ['wb'], 'chh': ['consonant'], '(': ['wb'], ',': ['wb'], '.': ['wb'], 'ai': ['vowel'], 'th': ['consonant'], 'ph': ['consonant'], ':': ['wb'], ':n': ['consonant'], 'dh': ['consonant'], '----': ['wb'], '((': ['ain', 'consonant'], ':z': ['consonant'], ':t': ['consonant'], 'b': ['consonant'], 'd': ['consonant'], 'au': ['vowel', 'nc_long_vowel'], 'f': ['consonant'], '--': ['wb'], 'h': ['consonant', 'consonant_h'], 'j': ['consonant'], 'l': ['consonant'], 'n': ['consonant'], 'p': ['consonant'], '.z': ['consonant'], 'r': ['consonant'], 't': ['consonant'], 'v': ['consonant'], ';g': ['consonant'], '.r': ['consonant'], 'z': ['consonant'], '|': ['hindi_break'], ';s': ['consonant'], ';r': ['consonant'], 'aa': ['nc_long_vowel', 'alef', 'vowel'], 'zh': ['consonant'], ';t': ['consonant'], '\\t': ['wb'], ';z': ['consonant'], ';x': [';x_class', 'consonant'], '\\r': ['wb'], 'ii': ['vowel'], ';dh': ['consonant'], ';e': ['short_vowel', 'short_vowel_e'], ';d': ['consonant'], ';h': ['consonant'], ';n': ['consonant'], ';m': ['vowel_nasal'], '!': ['wb'], '(-e)': ['hidden_izaafat'], ';th': ['consonant'], \"'\": ['letter_break'], '^i': ['short_vowel'], ')': ['wb'], '-': ['wb'], '/': ['wb'], 'sh': ['consonant'], ';': ['wb'], '?': ['wb'], '^au': ['vowel', 'nc_long_vowel'], 'bh': ['consonant'], ';aa': ['vowel'], 'jh': ['consonant'], '[': ['wb'], '^ai': ['vowel'], ']': ['wb'], '_': ['hidden_break'], 'a': ['short_vowel', 'short_vowel_a'], '.s': ['consonant'], 'e': ['vowel'], 'g': ['consonant'], 'i': ['short_vowel'], 'k': ['consonant'], 'kh': ['consonant'], 'm': ['consonant'], '^u': ['short_vowel'], 'o': ['vowel', 'nc_long_vowel'], 'q': ['consonant'], 'uu': ['vowel', 'nc_long_vowel'], 's': ['consonant'], '^ii': ['vowel'], 'u': ['short_vowel'], 'w': ['consonant'], 'y': ['consonant'], '^a': ['short_vowel', 'short_vowel_a'], '^uu': ['vowel', 'nc_long_vowel']}\n",
        "trying to match rule  ParserRule(production=u'\\u0646', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=None)\n",
        "trying rule  ParserRule(production=u'\\u0646', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=None)\n",
        "  passed prev tokens\n",
        "passed prev class\n",
        " pased next tokns\n",
        "\u06c1\u0627\u0646\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[r for r in urdup.rules if '-e' in r.tokens]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[ParserRule(production=u'\\u06d2', prev_classes=['nc_long_vowel'], prev_tokens=None, tokens=['-e'], next_tokens=None, next_classes=['wb']),\n",
        " ParserRule(production=u'\\u0650', prev_classes=None, prev_tokens=None, tokens=['-e'], next_tokens=None, next_classes=['wb'])]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.parse('shaa-e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "error in string shaa-e 6\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-eaeef665bc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shaa-e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/graphparser/graphparser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error in string\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# for now, croak on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monmatch_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nodes =urdup.DG.nodes(data=True)#[n for n in urdup.DG.nodes(data=True) if '-e' in n['tokens']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(nodes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "list"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n in nodes:\n",
      "    if 'token' in n[1]!=None:\n",
      "#        print n[1]['token']\n",
      "        if n[1]['token']=='y':\n",
      "            print n\n",
      "            print urdup.DG.in_edges(162)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(102, {'token': 'y'})\n",
        "[(0, 162)]\n",
        "(128, {'token': 'y'})\n",
        "[(0, 162)]\n",
        "(129, {'token': 'y'})\n",
        "[(0, 162)]\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.DG.node[129]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "{'token': 'y'}"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.DG.in_edges(162)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[(0, 162)]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.parse('-e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "error in string -e 2\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-37-97c65e807255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/graphparser/graphparser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error in string\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# for now, croak on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monmatch_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.tokenize('-e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "['-e']"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.DG.edges(data=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "[(0, 1, {'weight': 0}),\n",
        " (0, 8, {'weight': 0}),\n",
        " (0, 17, {'weight': 0}),\n",
        " (0, 22, {'weight': 0}),\n",
        " (0, 31, {'weight': 0}),\n",
        " (0, 39, {'weight': 0}),\n",
        " (0, 42, {'weight': 0}),\n",
        " (0, 45, {'weight': 0}),\n",
        " (0, 50, {'weight': 0}),\n",
        " (0, 53, {'weight': 0}),\n",
        " (0, 56, {'weight': 0}),\n",
        " (0, 59, {'weight': 0}),\n",
        " (0, 62, {'weight': 0}),\n",
        " (0, 65, {'weight': 0}),\n",
        " (0, 68, {'weight': 0}),\n",
        " (0, 71, {'weight': 0}),\n",
        " (0, 76, {'weight': 0}),\n",
        " (0, 83, {'weight': 0}),\n",
        " (0, 86, {'weight': 0}),\n",
        " (0, 89, {'weight': 0}),\n",
        " (0, 92, {'weight': 0}),\n",
        " (0, 95, {'weight': 0}),\n",
        " (0, 98, {'weight': 0}),\n",
        " (0, 101, {'weight': 0}),\n",
        " (0, 106, {'weight': 0}),\n",
        " (0, 111, {'weight': 0}),\n",
        " (0, 116, {'weight': 0}),\n",
        " (0, 119, {'weight': 0}),\n",
        " (0, 122, {'weight': 0}),\n",
        " (0, 125, {'weight': 0}),\n",
        " (0, 128, {'weight': 0}),\n",
        " (0, 135, {'weight': 0}),\n",
        " (0, 140, {'weight': 0}),\n",
        " (0, 145, {'weight': 0}),\n",
        " (0, 148, {'weight': 0}),\n",
        " (0, 151, {'weight': 0}),\n",
        " (0, 154, {'weight': 0}),\n",
        " (0, 162, {'weight': 0}),\n",
        " (0, 164, {'weight': 0}),\n",
        " (0, 166, {'weight': 0}),\n",
        " (0, 168, {'weight': 0}),\n",
        " (0, 170, {'weight': 0}),\n",
        " (0, 172, {'weight': 0}),\n",
        " (0, 175, {'weight': 0}),\n",
        " (0, 177, {'weight': 0}),\n",
        " (0, 179, {'weight': 0}),\n",
        " (0, 181, {'weight': 0}),\n",
        " (0, 183, {'weight': 0}),\n",
        " (0, 185, {'weight': 0}),\n",
        " (0, 188, {'weight': 0}),\n",
        " (0, 190, {'weight': 0}),\n",
        " (0, 192, {'weight': 0}),\n",
        " (0, 194, {'weight': 0}),\n",
        " (0, 196, {'weight': 0}),\n",
        " (0, 200, {'weight': 0}),\n",
        " (0, 202, {'weight': 0}),\n",
        " (0, 204, {'weight': 0}),\n",
        " (0, 207, {'weight': 0}),\n",
        " (0, 209, {'weight': 0}),\n",
        " (0, 211, {'weight': 0}),\n",
        " (0, 215, {'weight': 0}),\n",
        " (0, 219, {'weight': 0}),\n",
        " (0, 221, {'weight': 0}),\n",
        " (0, 223, {'weight': 0}),\n",
        " (0, 225, {'weight': 0}),\n",
        " (0, 227, {'weight': 0}),\n",
        " (0, 229, {'weight': 0}),\n",
        " (0, 231, {'weight': 0}),\n",
        " (0, 233, {'weight': 0}),\n",
        " (0, 235, {'weight': 0}),\n",
        " (0, 241, {'weight': 0}),\n",
        " (0, 247, {'weight': 0}),\n",
        " (0, 251, {'weight': 0}),\n",
        " (0, 256, {'weight': 0}),\n",
        " (0, 263, {'weight': 0}),\n",
        " (0, 265, {'weight': 0}),\n",
        " (0, 267, {'weight': 0}),\n",
        " (0, 269, {'weight': 0}),\n",
        " (0, 272, {'weight': 0}),\n",
        " (0, 274, {'weight': 0}),\n",
        " (0, 276, {'weight': 0}),\n",
        " (0, 278, {'weight': 0}),\n",
        " (0, 282, {'weight': 0}),\n",
        " (0, 286, {'weight': 0}),\n",
        " (0, 289, {'weight': 0}),\n",
        " (0, 293, {'weight': 0}),\n",
        " (0, 295, {'weight': 0}),\n",
        " (0, 297, {'weight': 0}),\n",
        " (0, 299, {'weight': 0}),\n",
        " (0, 309, {'weight': 0}),\n",
        " (0, 315, {'weight': 0}),\n",
        " (0, 320, {'weight': 0}),\n",
        " (0, 322, {'weight': 0}),\n",
        " (0, 324, {'weight': 0}),\n",
        " (0, 327, {'weight': 0}),\n",
        " (0, 333, {'weight': 0}),\n",
        " (1,\n",
        "  161,\n",
        "  {'rule': ParserRule(production=u'\\u06c1', prev_classes=['short_vowel_a'], prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (1, 2, {'weight': 0}),\n",
        " (1, 131, {'weight': 0}),\n",
        " (1,\n",
        "  243,\n",
        "  {'rule': ParserRule(production=u'\\u06c1', prev_classes=None, prev_tokens=None, tokens=['h'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (2, 3, {'weight': 0}),\n",
        " (3, 4, {'weight': 0}),\n",
        " (3, 6, {'weight': 0}),\n",
        " (3, 15, {'weight': 0}),\n",
        " (4,\n",
        "  5,\n",
        "  {'rule': ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'ii'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (6,\n",
        "  7,\n",
        "  {'rule': ParserRule(production=u'\\u06c1\\u0648\\u0627', prev_classes=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'aa'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (8, 9, {'weight': 0}),\n",
        " (8, 35, {'weight': 0}),\n",
        " (8, 37, {'weight': 0}),\n",
        " (8,\n",
        "  291,\n",
        "  {'rule': ParserRule(production=u'\\u0626', prev_classes=None, prev_tokens=None, tokens=['))'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (9, 10, {'weight': 0}),\n",
        " (10, 11, {'weight': 0}),\n",
        " (10, 13, {'weight': 0}),\n",
        " (11,\n",
        "  12,\n",
        "  {'rule': ParserRule(production=u'\\u06d2\\u0654\\u06af\\u0627', prev_classes=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'aa'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (13,\n",
        "  14,\n",
        "  {'rule': ParserRule(production=u'\\u06d2\\u0654\\u06af\\u06cc', prev_classes=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'ii'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (15,\n",
        "  16,\n",
        "  {'rule': ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (17,\n",
        "  331,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (17,\n",
        "  174,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (17, 18, {'weight': 0}),\n",
        " (17,\n",
        "  213,\n",
        "  {'rule': ParserRule(production=u'\\u06d2', prev_classes=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (17, 26, {'weight': 0}),\n",
        " (17,\n",
        "  159,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (18, 19, {'weight': 0}),\n",
        " (19, 20, {'weight': 0}),\n",
        " (20,\n",
        "  21,\n",
        "  {'rule': ParserRule(production=u'\\u06cc\\u06ba\\u200c\\u06af\\u06d2', prev_classes=None, prev_tokens=None, tokens=['e', ';n', 'g', 'e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (22, 104, {'weight': 0}),\n",
        " (22,\n",
        "  312,\n",
        "  {'rule': ParserRule(production=u'\\u06a9', prev_classes=None, prev_tokens=None, tokens=['k'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (22, 138, {'weight': 0}),\n",
        " (22, 23, {'weight': 0}),\n",
        " (23, 24, {'weight': 0}),\n",
        " (24,\n",
        "  25,\n",
        "  {'rule': ParserRule(production=u'\\u06a9\\u06c1\\u06c1', prev_classes=['wb'], prev_tokens=None, tokens=['k', 'a', 'h'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (26, 27, {'weight': 0}),\n",
        " (26, 29, {'weight': 0}),\n",
        " (27,\n",
        "  28,\n",
        "  {'rule': ParserRule(production=u'\\u06d2\\u06af\\u06cc', prev_classes=None, prev_tokens=None, tokens=['e', 'g', 'ii'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (29,\n",
        "  30,\n",
        "  {'rule': ParserRule(production=u'\\u06d2\\u06af\\u0627', prev_classes=None, prev_tokens=None, tokens=['e', 'g', 'aa'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (31, 32, {'weight': 0}),\n",
        " (31,\n",
        "  218,\n",
        "  {'rule': ParserRule(production=' ', prev_classes=None, prev_tokens=None, tokens=['-'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (32, 33, {'weight': 0}),\n",
        " (33,\n",
        "  34,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['-', 'o', '-'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (35,\n",
        "  36,\n",
        "  {'rule': ParserRule(production=u'\\u0648\\u0654', prev_classes=['vowel'], prev_tokens=None, tokens=['))', 'uu'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (37,\n",
        "  38,\n",
        "  {'rule': ParserRule(production=u'\\u0648\\u0654', prev_classes=['vowel'], prev_tokens=None, tokens=['))', 'o'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (39, 40, {'weight': 0}),\n",
        " (39, 114, {'weight': 0}),\n",
        " (39,\n",
        "  246,\n",
        "  {'rule': ParserRule(production=u'\\u062a', prev_classes=None, prev_tokens=None, tokens=['t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (40,\n",
        "  41,\n",
        "  {'rule': ParserRule(production=u'\\u062a\\u0651', prev_classes=None, prev_tokens=None, tokens=['t', 't'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (42, 48, {'weight': 0}),\n",
        " (42, 43, {'weight': 0}),\n",
        " (42,\n",
        "  245,\n",
        "  {'rule': ParserRule(production=u'\\u067e', prev_classes=None, prev_tokens=None, tokens=['p'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (43,\n",
        "  44,\n",
        "  {'rule': ParserRule(production=u'\\u067e\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['p', 'ph'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (45,\n",
        "  307,\n",
        "  {'rule': ParserRule(production=u'\\u0631', prev_classes=None, prev_tokens=None, tokens=['r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (45, 46, {'weight': 0}),\n",
        " (46,\n",
        "  47,\n",
        "  {'rule': ParserRule(production=u'\\u0631\\u0651', prev_classes=None, prev_tokens=None, tokens=['r', 'r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (48,\n",
        "  49,\n",
        "  {'rule': ParserRule(production=u'\\u067e\\u0651', prev_classes=None, prev_tokens=None, tokens=['p', 'p'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (50, 51, {'weight': 0}),\n",
        " (50,\n",
        "  301,\n",
        "  {'rule': ParserRule(production=u'\\u0628', prev_classes=None, prev_tokens=None, tokens=['b'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (50, 79, {'weight': 0}),\n",
        " (51,\n",
        "  52,\n",
        "  {'rule': ParserRule(production=u'\\u0628\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['b', 'bh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (53,\n",
        "  244,\n",
        "  {'rule': ParserRule(production=u'\\u0644', prev_classes=None, prev_tokens=None, tokens=['l'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (53, 54, {'weight': 0}),\n",
        " (54,\n",
        "  55,\n",
        "  {'rule': ParserRule(production=u'\\u0644', prev_classes=None, prev_tokens=None, tokens=['l', 'l'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (56, 57, {'weight': 0}),\n",
        " (56, 109, {'weight': 0}),\n",
        " (56,\n",
        "  281,\n",
        "  {'rule': ParserRule(production=u'\\u06af', prev_classes=None, prev_tokens=None, tokens=['g'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (57,\n",
        "  58,\n",
        "  {'rule': ParserRule(production=u'\\u06af\\u0651', prev_classes=None, prev_tokens=None, tokens=['g', 'g'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (59, 60, {'weight': 0}),\n",
        " (59,\n",
        "  239,\n",
        "  {'rule': ParserRule(production=u'\\u0637', prev_classes=None, prev_tokens=None, tokens=[':t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (60,\n",
        "  61,\n",
        "  {'rule': ParserRule(production=u'\\u0637\\u0651', prev_classes=None, prev_tokens=None, tokens=[':t', ':t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (62,\n",
        "  255,\n",
        "  {'rule': ParserRule(production=u'\\u062e', prev_classes=None, prev_tokens=None, tokens=[';x'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (62, 63, {'weight': 0}),\n",
        " (63,\n",
        "  64,\n",
        "  {'rule': ParserRule(production=u'\\u062e\\u0651', prev_classes=None, prev_tokens=None, tokens=[';x', ';x'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (65, 81, {'weight': 0}),\n",
        " (65, 66, {'weight': 0}),\n",
        " (65,\n",
        "  259,\n",
        "  {'rule': ParserRule(production=u'\\u0688', prev_classes=None, prev_tokens=None, tokens=[';d'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (66,\n",
        "  67,\n",
        "  {'rule': ParserRule(production=u'\\u0688\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=[';d', ';dh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (68,\n",
        "  288,\n",
        "  {'rule': ParserRule(production=u'\\u0686', prev_classes=None, prev_tokens=None, tokens=['ch'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (68, 74, {'weight': 0}),\n",
        " (68, 69, {'weight': 0}),\n",
        " (69,\n",
        "  70,\n",
        "  {'rule': ParserRule(production=u'\\u0686\\u0651', prev_classes=None, prev_tokens=None, tokens=['ch', 'ch'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (71, 72, {'weight': 0}),\n",
        " (71,\n",
        "  260,\n",
        "  {'rule': ParserRule(production=u'\\u062d', prev_classes=None, prev_tokens=None, tokens=[';h'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (72,\n",
        "  73,\n",
        "  {'rule': ParserRule(production=u'\\u062d\\u0651', prev_classes=None, prev_tokens=None, tokens=[';h', ';h'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (74,\n",
        "  75,\n",
        "  {'rule': ParserRule(production=u'\\u0686\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['ch', 'chh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (76, 77, {'weight': 0}),\n",
        " (76,\n",
        "  302,\n",
        "  {'rule': ParserRule(production=u'\\u0641', prev_classes=None, prev_tokens=None, tokens=['f'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (77,\n",
        "  78,\n",
        "  {'rule': ParserRule(production=u'\\u0641\\u0651', prev_classes=None, prev_tokens=None, tokens=['f', 'f'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (79,\n",
        "  80,\n",
        "  {'rule': ParserRule(production=u'\\u0628\\u0651', prev_classes=None, prev_tokens=None, tokens=['b', 'b'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (81,\n",
        "  82,\n",
        "  {'rule': ParserRule(production=u'\\u0688\\u0651', prev_classes=None, prev_tokens=None, tokens=[';d', ';d'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (83,\n",
        "  280,\n",
        "  {'rule': ParserRule(production=u'\\u0635', prev_classes=None, prev_tokens=None, tokens=['.s'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (83, 84, {'weight': 0}),\n",
        " (84,\n",
        "  85,\n",
        "  {'rule': ParserRule(production=u'\\u0635\\u0651', prev_classes=None, prev_tokens=None, tokens=['.s', '.s'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (86,\n",
        "  284,\n",
        "  {'rule': ParserRule(production=u'\\u0633', prev_classes=None, prev_tokens=None, tokens=['s'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (86, 87, {'weight': 0}),\n",
        " (87,\n",
        "  88,\n",
        "  {'rule': ParserRule(production=u'\\u0633\\u0651', prev_classes=None, prev_tokens=None, tokens=['s', 's'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (89, 90, {'weight': 0}),\n",
        " (89,\n",
        "  237,\n",
        "  {'rule': ParserRule(production=u'\\u0639', prev_classes=None, prev_tokens=None, tokens=['(('], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (90,\n",
        "  91,\n",
        "  {'rule': ParserRule(production=u'\\u0639\\u0651', prev_classes=None, prev_tokens=None, tokens=['((', '(('], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (92,\n",
        "  337,\n",
        "  {'rule': ParserRule(production=u'\\u0642', prev_classes=None, prev_tokens=None, tokens=['q'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (92, 93, {'weight': 0}),\n",
        " (93,\n",
        "  94,\n",
        "  {'rule': ParserRule(production=u'\\u0642\\u0651', prev_classes=None, prev_tokens=None, tokens=['q', 'q'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (95, 96, {'weight': 0}),\n",
        " (95,\n",
        "  304,\n",
        "  {'rule': ParserRule(production=u'\\u062c', prev_classes=None, prev_tokens=None, tokens=['j'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (95, 157, {'weight': 0}),\n",
        " (96,\n",
        "  97,\n",
        "  {'rule': ParserRule(production=u'\\u062c\\u0651', prev_classes=None, prev_tokens=None, tokens=['j', 'j'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (98,\n",
        "  240,\n",
        "  {'rule': ParserRule(production=u'\\u062f', prev_classes=None, prev_tokens=None, tokens=['d'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (98, 99, {'weight': 0}),\n",
        " (98, 133, {'weight': 0}),\n",
        " (99,\n",
        "  100,\n",
        "  {'rule': ParserRule(production=u'\\u062f\\u0651', prev_classes=None, prev_tokens=None, tokens=['d', 'd'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (101,\n",
        "  160,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (101,\n",
        "  217,\n",
        "  {'rule': ParserRule(production=u'\\u06d2', prev_classes=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (101,\n",
        "  314,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (101,\n",
        "  187,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (101, 102, {'weight': 0}),\n",
        " (102,\n",
        "  103,\n",
        "  {'rule': ParserRule(production=u'\\u06cc\\u0651', prev_classes=None, prev_tokens=None, tokens=['ai', 'y'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (104,\n",
        "  105,\n",
        "  {'rule': ParserRule(production=u'\\u06a9\\u0651', prev_classes=None, prev_tokens=None, tokens=['k', 'k'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (106, 107, {'weight': 0}),\n",
        " (106,\n",
        "  335,\n",
        "  {'rule': ParserRule(production=u'\\u0645', prev_classes=None, prev_tokens=None, tokens=['m'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (107,\n",
        "  108,\n",
        "  {'rule': ParserRule(production=u'\\u0645\\u0651', prev_classes=None, prev_tokens=None, tokens=['m', 'm'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (109,\n",
        "  110,\n",
        "  {'rule': ParserRule(production=u'\\u06af\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['g', 'gh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (111, 112, {'weight': 0}),\n",
        " (111,\n",
        "  253,\n",
        "  {'rule': ParserRule(production=u'\\u0679', prev_classes=None, prev_tokens=None, tokens=[';t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (111, 143, {'weight': 0}),\n",
        " (112,\n",
        "  113,\n",
        "  {'rule': ParserRule(production=u'\\u0679\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=[';t', ';th'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (114,\n",
        "  115,\n",
        "  {'rule': ParserRule(production=u'\\u062a\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['t', 'th'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (116, 117, {'weight': 0}),\n",
        " (116,\n",
        "  326,\n",
        "  {'rule': ParserRule(production=u'\\u063a', prev_classes=None, prev_tokens=None, tokens=[';g'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (117,\n",
        "  118,\n",
        "  {'rule': ParserRule(production=u'\\u063a\\u0651', prev_classes=None, prev_tokens=None, tokens=[';g', ';g'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (119, 120, {'weight': 0}),\n",
        " (119,\n",
        "  306,\n",
        "  {'rule': ParserRule(production=u'\\u0636', prev_classes=None, prev_tokens=None, tokens=['.z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (120,\n",
        "  121,\n",
        "  {'rule': ParserRule(production=u'\\u0636\\u0651', prev_classes=None, prev_tokens=None, tokens=['.z', '.z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (122,\n",
        "  249,\n",
        "  {'rule': ParserRule(production=u'\\u062b', prev_classes=None, prev_tokens=None, tokens=[';s'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (122, 123, {'weight': 0}),\n",
        " (123,\n",
        "  124,\n",
        "  {'rule': ParserRule(production=u'\\u062b\\u0651', prev_classes=None, prev_tokens=None, tokens=[';s', ';s'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (125,\n",
        "  339,\n",
        "  {'rule': ParserRule(production=u'\\u0634', prev_classes=None, prev_tokens=None, tokens=['sh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (125, 126, {'weight': 0}),\n",
        " (126,\n",
        "  127,\n",
        "  {'rule': ParserRule(production=u'\\u0634\\u0651', prev_classes=None, prev_tokens=None, tokens=['sh', 'sh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (128, 129, {'weight': 0}),\n",
        " (128,\n",
        "  341,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['y'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (129,\n",
        "  130,\n",
        "  {'rule': ParserRule(production=u'\\u06cc\\u0651', prev_classes=None, prev_tokens=None, tokens=['y', 'y'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (131,\n",
        "  132,\n",
        "  {'rule': ParserRule(production=u'\\u06c1\\u0651', prev_classes=None, prev_tokens=None, tokens=['h', 'h'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (133,\n",
        "  134,\n",
        "  {'rule': ParserRule(production=u'\\u062f\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['d', 'dh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (135, 136, {'weight': 0}),\n",
        " (135,\n",
        "  308,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['v'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (136,\n",
        "  137,\n",
        "  {'rule': ParserRule(production=u'\\u0648\\u0651', prev_classes=None, prev_tokens=None, tokens=['v', 'v'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (138,\n",
        "  139,\n",
        "  {'rule': ParserRule(production=u'\\u06a9\\u06be', prev_classes=None, prev_tokens=None, tokens=['k', 'kh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (140,\n",
        "  250,\n",
        "  {'rule': ParserRule(production=u'\\u0691', prev_classes=None, prev_tokens=None, tokens=[';r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (140, 141, {'weight': 0}),\n",
        " (141,\n",
        "  142,\n",
        "  {'rule': ParserRule(production=u'\\u0691\\u0651', prev_classes=None, prev_tokens=None, tokens=[';r', ';r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (143,\n",
        "  144,\n",
        "  {'rule': ParserRule(production=u'\\u0679\\u0651', prev_classes=None, prev_tokens=None, tokens=[';t', ';t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (145, 146, {'weight': 0}),\n",
        " (145,\n",
        "  254,\n",
        "  {'rule': ParserRule(production=u'\\u0630', prev_classes=None, prev_tokens=None, tokens=[';z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (146,\n",
        "  147,\n",
        "  {'rule': ParserRule(production=u'\\u062e\\u0651', prev_classes=None, prev_tokens=None, tokens=[';z', ';z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (148,\n",
        "  305,\n",
        "  {'rule': ParserRule(production=u'\\u0646', prev_classes=None, prev_tokens=None, tokens=['n'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (148, 149, {'weight': 0}),\n",
        " (149,\n",
        "  150,\n",
        "  {'rule': ParserRule(production=u'\\u0646\\u0651', prev_classes=None, prev_tokens=None, tokens=['n', 'n'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (151, 152, {'weight': 0}),\n",
        " (151,\n",
        "  238,\n",
        "  {'rule': ParserRule(production=u'\\u0638', prev_classes=None, prev_tokens=None, tokens=[':z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (152,\n",
        "  153,\n",
        "  {'rule': ParserRule(production=u'\\u0638\\u0651', prev_classes=None, prev_tokens=None, tokens=[':z', ':z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (154, 155, {'weight': 0}),\n",
        " (154,\n",
        "  311,\n",
        "  {'rule': ParserRule(production=u'\\u0632', prev_classes=None, prev_tokens=None, tokens=['z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (155,\n",
        "  156,\n",
        "  {'rule': ParserRule(production=u'\\u0632\\u0651', prev_classes=None, prev_tokens=None, tokens=['z', 'z'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (157,\n",
        "  158,\n",
        "  {'rule': ParserRule(production=u'\\u062c\\u0651\\u06be', prev_classes=None, prev_tokens=None, tokens=['j', 'jh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (162,\n",
        "  163,\n",
        "  {'rule': ParserRule(production=u'\\u06d2', prev_classes=['nc_long_vowel'], prev_tokens=None, tokens=['-e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (162,\n",
        "  199,\n",
        "  {'rule': ParserRule(production=u'\\u0650', prev_classes=None, prev_tokens=None, tokens=['-e'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (164,\n",
        "  214,\n",
        "  {'rule': ParserRule(production=u'\\u0650\\u06d2', prev_classes=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (164,\n",
        "  292,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (164,\n",
        "  165,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0650\\u06d2', prev_classes=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 3}),\n",
        " (164,\n",
        "  206,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (166,\n",
        "  261,\n",
        "  {'rule': ParserRule(production=u'\\u0646', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (166,\n",
        "  167,\n",
        "  {'rule': ParserRule(production=u'\\u06ba', prev_classes=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (168,\n",
        "  169,\n",
        "  {'rule': ParserRule(production=u'\\u0627', prev_classes=['wb'], prev_tokens=None, tokens=['u'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (168,\n",
        "  340,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['u'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (170,\n",
        "  258,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=[';e'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (170,\n",
        "  171,\n",
        "  {'rule': ParserRule(production=u'\\u0627', prev_classes=['wb'], prev_tokens=None, tokens=[';e'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (172,\n",
        "  329,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['a'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (172,\n",
        "  173,\n",
        "  {'rule': ParserRule(production=u'\\u0627', prev_classes=['wb'], prev_tokens=None, tokens=['a'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (175,\n",
        "  176,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0650', prev_classes=['wb'], prev_tokens=None, tokens=['^i'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (175,\n",
        "  338,\n",
        "  {'rule': ParserRule(production=u'\\u0650', prev_classes=None, prev_tokens=None, tokens=['^i'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (177,\n",
        "  178,\n",
        "  {'rule': ParserRule(production=u'\\u0627', prev_classes=['wb'], prev_tokens=None, tokens=['i'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (177,\n",
        "  332,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (177,\n",
        "  198,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (179,\n",
        "  180,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0644', prev_classes=['wb'], prev_tokens=None, tokens=['ul-'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (181,\n",
        "  342,\n",
        "  {'rule': ParserRule(production=u'\\u064e', prev_classes=None, prev_tokens=None, tokens=['^a'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (181,\n",
        "  182,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u064e', prev_classes=['wb'], prev_tokens=None, tokens=['^a'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (183,\n",
        "  184,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0648', prev_classes=['wb'], prev_tokens=None, tokens=['o'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (183,\n",
        "  319,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['o'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (185,\n",
        "  313,\n",
        "  {'rule': ParserRule(production=u'\\u0627', prev_classes=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (185,\n",
        "  186,\n",
        "  {'rule': ParserRule(production=u'\\u0622', prev_classes=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (188,\n",
        "  189,\n",
        "  {'rule': ParserRule(production=':', prev_classes=None, prev_tokens=None, tokens=[':'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (190,\n",
        "  318,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['au'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (190,\n",
        "  191,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0648', prev_classes=['wb'], prev_tokens=None, tokens=['au'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (192,\n",
        "  193,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['ii'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (192,\n",
        "  317,\n",
        "  {'rule': ParserRule(production=u'\\u06cc', prev_classes=None, prev_tokens=None, tokens=['ii'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (194,\n",
        "  330,\n",
        "  {'rule': ParserRule(production=u'\\u064f\\u0648', prev_classes=None, prev_tokens=None, tokens=['^uu'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (194,\n",
        "  195,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u064f\\u0648', prev_classes=['wb'], prev_tokens=None, tokens=['^uu'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (196,\n",
        "  197,\n",
        "  {'rule': ParserRule(production=u'\\u0644\\u0644', prev_classes=['wb'], prev_tokens=None, tokens=['il-'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (200,\n",
        "  201,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_classes=['wb'], prev_tokens=None, tokens=['^ii'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (200,\n",
        "  285,\n",
        "  {'rule': ParserRule(production=u'\\u0650\\u06cc', prev_classes=None, prev_tokens=None, tokens=['^ii'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (202,\n",
        "  336,\n",
        "  {'rule': ParserRule(production=u'\\u064f', prev_classes=None, prev_tokens=None, tokens=['^u'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (202,\n",
        "  203,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u064f', prev_classes=['wb'], prev_tokens=None, tokens=['^u'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (204,\n",
        "  205,\n",
        "  {'rule': ParserRule(production=u'\\u06ba', prev_classes=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (204,\n",
        "  262,\n",
        "  {'rule': ParserRule(production=u'\\u0646', prev_classes=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (207,\n",
        "  208,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u064e\\u0648', prev_classes=['wb'], prev_tokens=None, tokens=['^au'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (207,\n",
        "  271,\n",
        "  {'rule': ParserRule(production=u'\\u064e\\u0648', prev_classes=None, prev_tokens=None, tokens=['^au'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (209,\n",
        "  210,\n",
        "  {'rule': ParserRule(production=u'\\u0627\\u0648', prev_classes=['wb'], prev_tokens=None, tokens=['uu'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (209,\n",
        "  303,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['uu'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (211,\n",
        "  212,\n",
        "  {'rule': ParserRule(production='.', prev_classes=None, prev_tokens=None, tokens=['.'], next_tokens=None, next_classes=['wb']),\n",
        "   'weight': 4}),\n",
        " (215,\n",
        "  216,\n",
        "  {'rule': ParserRule(production=u'\\u0644\\u0644', prev_classes=['wb'], prev_tokens=None, tokens=['al-'], next_tokens=None, next_classes=None),\n",
        "   'weight': 4}),\n",
        " (219,\n",
        "  220,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['/'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (221,\n",
        "  222,\n",
        "  {'rule': ParserRule(production=u'\\u06af\\u06be', prev_classes=None, prev_tokens=None, tokens=['gh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (223,\n",
        "  224,\n",
        "  {'rule': ParserRule(production=' ', prev_classes=None, prev_tokens=None, tokens=' ', next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (225,\n",
        "  226,\n",
        "  {'rule': ParserRule(production=u'\\u0691\\u06be', prev_classes=None, prev_tokens=None, tokens=[';rh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (227,\n",
        "  228,\n",
        "  {'rule': ParserRule(production='(', prev_classes=None, prev_tokens=None, tokens=['('], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (229,\n",
        "  230,\n",
        "  {'rule': ParserRule(production=u'\\u060c', prev_classes=None, prev_tokens=None, tokens=[','], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (231,\n",
        "  232,\n",
        "  {'rule': ParserRule(production=u'\\u062a\\u06be', prev_classes=None, prev_tokens=None, tokens=['th'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (233,\n",
        "  234,\n",
        "  {'rule': ParserRule(production=u'\\u064b', prev_classes=None, prev_tokens=None, tokens=[':n'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (235,\n",
        "  236,\n",
        "  {'rule': ParserRule(production=u'\\u062f\\u06be', prev_classes=None, prev_tokens=None, tokens=['dh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (241,\n",
        "  242,\n",
        "  {'rule': ParserRule(production=u'\\u06d4', prev_classes=None, prev_tokens=None, tokens=['--'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (247,\n",
        "  248,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['|'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (251,\n",
        "  252,\n",
        "  {'rule': ParserRule(production=u'\\u0698', prev_classes=None, prev_tokens=None, tokens=['zh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (256,\n",
        "  257,\n",
        "  {'rule': ParserRule(production=u'\\u0688\\u06be', prev_classes=None, prev_tokens=None, tokens=[';dh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (263,\n",
        "  264,\n",
        "  {'rule': ParserRule(production=u'\\u0679\\u06be', prev_classes=None, prev_tokens=None, tokens=[';th'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (265,\n",
        "  266,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=[\"'\"], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (267,\n",
        "  268,\n",
        "  {'rule': ParserRule(production=u'\\u061b', prev_classes=None, prev_tokens=None, tokens=[';'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (269,\n",
        "  270,\n",
        "  {'rule': ParserRule(production=u'\\u061f', prev_classes=None, prev_tokens=None, tokens=['?'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (272,\n",
        "  273,\n",
        "  {'rule': ParserRule(production=u'\\u0628\\u06be', prev_classes=None, prev_tokens=None, tokens=['bh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (274,\n",
        "  275,\n",
        "  {'rule': ParserRule(production=u'\\u062c\\u06be', prev_classes=None, prev_tokens=None, tokens=['jh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (276,\n",
        "  277,\n",
        "  {'rule': ParserRule(production='[', prev_classes=None, prev_tokens=None, tokens=['['], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (278,\n",
        "  279,\n",
        "  {'rule': ParserRule(production='', prev_classes=None, prev_tokens=None, tokens=['_'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (282,\n",
        "  283,\n",
        "  {'rule': ParserRule(production=u'\\u06cc\\u0670', prev_classes=None, prev_tokens=None, tokens=[';aa'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (286,\n",
        "  287,\n",
        "  {'rule': ParserRule(production=u'\\u0648', prev_classes=None, prev_tokens=None, tokens=['w'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (289,\n",
        "  290,\n",
        "  {'rule': ParserRule(production='\\n', prev_classes=None, prev_tokens=None, tokens=['\\n'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (293,\n",
        "  294,\n",
        "  {'rule': ParserRule(production=u'\\u0686\\u06be', prev_classes=None, prev_tokens=None, tokens=['chh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (295,\n",
        "  296,\n",
        "  {'rule': ParserRule(production='\\t', prev_classes=None, prev_tokens=None, tokens=['\\t'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (297,\n",
        "  298,\n",
        "  {'rule': ParserRule(production=u'\\u067e\\u06be', prev_classes=None, prev_tokens=None, tokens=['ph'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (299,\n",
        "  300,\n",
        "  {'rule': ParserRule(production=u'\\u06d4\\u06d4', prev_classes=None, prev_tokens=None, tokens=['----'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (309,\n",
        "  310,\n",
        "  {'rule': ParserRule(production=u'\\u0631', prev_classes=None, prev_tokens=None, tokens=['.r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (315,\n",
        "  316,\n",
        "  {'rule': ParserRule(production='\\r', prev_classes=None, prev_tokens=None, tokens=['\\r'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (320,\n",
        "  321,\n",
        "  {'rule': ParserRule(production='!', prev_classes=None, prev_tokens=None, tokens=['!'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (322,\n",
        "  323,\n",
        "  {'rule': ParserRule(production=')', prev_classes=None, prev_tokens=None, tokens=[')'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (324,\n",
        "  325,\n",
        "  {'rule': ParserRule(production='\"', prev_classes=None, prev_tokens=None, tokens=['\"'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (327,\n",
        "  328,\n",
        "  {'rule': ParserRule(production=']', prev_classes=None, prev_tokens=None, tokens=[']'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5}),\n",
        " (333,\n",
        "  334,\n",
        "  {'rule': ParserRule(production=u'\\u06a9\\u06be', prev_classes=None, prev_tokens=None, tokens=['kh'], next_tokens=None, next_classes=None),\n",
        "   'weight': 5})]"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.DG.node[162]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "{'token': '-e'}"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}