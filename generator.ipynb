{
 "metadata": {
  "name": "",
  "signature": "sha256:308dc653908cb290da0bf28bc58f78f27cfd9d96a6472e790b75b283fd873970"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ghalib Concordance Generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Description\n",
      "\n",
      "This notebook contains code to generator a concordance for the muravvaj divaan of ghalib.\n",
      "\n",
      "Verses are taken from \"input/verses.csv\"\n",
      "\n",
      "The current task is to identify the proper lemma of the tokens, e.g. singular instead of plural,\n",
      "verb infinitive instead of verb root, etc. This can partially be done computationally.\n",
      "\n",
      "Lemma that remain to be checked are in \"output/tocheck.csv\" The first column, if marked as 'x',\n",
      "means that entry is okay. Checked lemma can then be entered into \"input/okay.csv\" using the\n",
      "functions \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import re\n",
      "from collections import *\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = {}                      # dictionary of verses, e.g. 001.01.0='naqsh faryaadii..'\n",
      "tokens = {}                      # dictionary of tokens where key is verses+.xx, e.g. 001.01.0.01 = 'naqsh'\n",
      "unique_tokens = Counter()        # Counter of tokens where value is their count\n",
      "lemmas = defaultdict(list)       # dictionary of tokens where value is a list of their lemmas\n",
      "unique_lemmas = []               # list of unique lemmas\n",
      "okay_lemmas = defaultdict(list)  # dictionary of unique tokens with lists of lemma, e."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# moved load_verses, moved to util.py\n",
      "\n",
      "def load_verses(inputfile='input/verses.csv'):\n",
      "    '''\n",
      "    Loads verses from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: verses where verses['ggg.vv.l']=token; where ggg=ghazal #; vv=verse number;l=line number\n",
      "    '''\n",
      "\n",
      "\n",
      "    verses = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            (verse_id, input_string, real_scan) = row # \n",
      "            if not 'x' in verse_id: # only muravvaj divan for now\n",
      "                verses[verse_id] = input_string.strip() \n",
      "    return verses\n",
      "\n",
      "def get_okay_lemmas(inputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Loads checked lemmas from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: checked_lemmas where checked_lemmas['token'] = [lemmas]\n",
      "    '''\n",
      "\n",
      "    import csv\n",
      "    okay_lemmas = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            (status, unique_token, lemmas) = row\n",
      "            assert status in ['','x']\n",
      "            if status=='x':\n",
      "                okay_lemmas[unique_token]=lemmas.split('|')\n",
      "    return okay_lemmas\n",
      "\n",
      "\n",
      "def get_tokens(verses):\n",
      "    '''\n",
      "    Identifies tokens in verses\n",
      "    verses: verses\n",
      "    returns: tokens, where tokens['ggg.vv.l.tt']=token {tt = token # on line starting  at zero}\n",
      "    '''\n",
      "    tokens = {}\n",
      "    token_instances=defaultdict(list)\n",
      "    token_instance_count = Counter()\n",
      "    for k in verses.keys():\n",
      "        v_tokens = verses[k].split(' ')\n",
      "        for id,t in enumerate(v_tokens):\n",
      "\n",
      "            token_id = k+'.'+str(id).zfill(2)\n",
      "            tokens[token_id] = t\n",
      "            token_instances[t].append(token_id)\n",
      "            token_instance_count[t]+=1\n",
      "    return tokens,token_instances,token_instance_count\n",
      "\n",
      "def locate_token(token):\n",
      "    '''\n",
      "    Finds locations of token\n",
      "    token: string \n",
      "    Input: token (string)\n",
      "    returns: a list of locations, e.g. [001.01.0.01]\n",
      "    '''\n",
      "    assert tokens\n",
      "    return [k  for k,v in tokens.iteritems() if v==token]\n",
      "\n",
      "def match_tokens(match_string):\n",
      "    '''\n",
      "    Finds tokens matching a pattern (from start)\n",
      "    match_string: regular expression string (assumes ^,e.g. 'naq')\n",
      "    returns: a list of tokens,e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.match(match_string,k)]\n",
      "\n",
      "def search_tokens(match_string):\n",
      "    '''\n",
      "    Searches for tokens matching a pattern (anywhere in it)\n",
      "    match_string: regular expression of string\n",
      "    Input: regular expression string (e.g. 'aqsh'\n",
      "    returns: a list of tokens, e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.search(match_string,k)]\n",
      "\n",
      "def get_unique_tokens(tokens):\n",
      "    '''\n",
      "    Finds unique tokens\n",
      "    tokens: a dictionary of tokens at locations, e.g. tokens['001.01.0.00']='naqsh'\n",
      "    returns: a dictionary of unique tokens and their count, unique_tokens['token']=1\n",
      "    '''\n",
      "    unique = Counter()\n",
      "#    print type(tokens)\n",
      "    for k,t in tokens.iteritems():\n",
      "        unique[t]+=1\n",
      "    return unique\n",
      "\n",
      "\n",
      "def get_lemmas(unique_tokens):\n",
      "    '''\n",
      "    Generate lemmas of tokens\n",
      "    unique_tokens: dictionary of unique tokens\n",
      "    returns: lemmas[original_token]=['lemma1','lemma2']\n",
      "    '''\n",
      "    lemmas = {}\n",
      "\n",
      "    \n",
      "    for t in unique_tokens.keys():\n",
      "        lemma = t\n",
      "        if re.search(\"-e$\",t):\n",
      "            lemma = t[:-2] # remove izaafat ending '-e'\n",
      "        if re.search(\"[-']haa$\",t): \n",
      "            lemma = t[:-4] # remove Persian plural ['-]haa ending\n",
      "#            print lemma\n",
      "        t_lemmas = [lemma]\n",
      "        if re.search('-o-',lemma):\n",
      "            nouns = lemma.split('-o-')\n",
      "            t_lemmas = t_lemmas + nouns\n",
      "            \n",
      "        lemmas[t]=t_lemmas\n",
      "    return lemmas\n",
      "\n",
      "def get_unique_lemmas(lemmas):\n",
      "    '''\n",
      "    Generates unique lemma forms\n",
      "    lemmas: dictionary keyed by tokens containing lists of lemma, e.g. lemmas['rang-o-buu']=['rang','buu','rang-o-buu']\n",
      "    returns: unique_lemmas as unique_lemmas['lemma']=count\n",
      "    '''\n",
      "    unique_lemmas = set()\n",
      "    for t,t_lemmas in lemmas.iteritems():\n",
      "        for lemma in t_lemmas:\n",
      "            unique_lemmas.add(lemma)\n",
      "#                unique_lemmas.add(lemma)\n",
      "#            else:\n",
      "#                unique_lemmas[lemma].append(t)\n",
      "    return unique_lemmas\n",
      "\n",
      "\n",
      "def to_check():\n",
      "    '''\n",
      "    Generates list of unique tokens that still need to be checked.\n",
      "    '''\n",
      "    out = []\n",
      "    return [t for t in sorted(unique_tokens.keys()) if not t in okay_lemmas]\n",
      "\n",
      "def print_stats():\n",
      "    print \"Currently there are \",len(okay_lemmas),\" out of \",len(lemmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = load_verses()\n",
      "tokens,token_instances,token_instance_count = get_tokens(verses)\n",
      "unique_tokens = get_unique_tokens(tokens)\n",
      "\n",
      "lemmas = get_lemmas(unique_tokens)\n",
      "unique_lemmas = get_unique_lemmas(lemmas)\n",
      "okay_lemmas = get_okay_lemmas()\n",
      "\n",
      "print_stats()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Currently there are  4512  out of  4510\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Update Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_to_check():\n",
      "    '''\n",
      "    Writes unique tokens not contained in okay_lemmas to output/tocheck.csv\n",
      "    '''\n",
      "    with open('output/tocheck.csv','w') as f:\n",
      "        for t in sorted(unique_tokens.keys()):\n",
      "            if not t in okay_lemmas: # only add unchecked ones\n",
      "                line  = \",\" # good or bad\n",
      "                line += t+\",\" #token\n",
      "                line += '|'.join(lemmas[t]) # possible lemma of token\n",
      "                line += \"\\n\" \n",
      "                f.write(line)\n",
      "\n",
      "def update_okay(inputfile='output/tocheck.csv'):\n",
      "    '''\n",
      "    Loads lemmas noted as correct from inputfile into okay_lemmas\n",
      "    '''\n",
      "    lemmas_to_add = get_okay_lemmas(inputfile=inputfile)\n",
      "    for k,v in lemmas_to_add.iteritems():\n",
      "        if k in okay_lemmas:\n",
      "            print \"WARNING: \",k,\" found in okay_lemmas. Will override.\"\n",
      "        okay_lemmas[k] = v\n",
      "    \n",
      "def write_okay(outputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Writes okay_lemmas to outputfile, as status,token,lemma1|lemma2|lemma3\n",
      "    '''\n",
      "    with open(outputfile,'w') as f:\n",
      "        for t in sorted(okay_lemmas.keys()):\n",
      "            line  = \"x,\" # good or bad\n",
      "            line += t+\",\" #token\n",
      "            line += '|'.join(okay_lemmas[t])\n",
      "            line += \"\\n\" \n",
      "            f.write(line)\n",
      "\n",
      "def update_files():\n",
      "    '''\n",
      "    Loads lemmas noted as correct from tocheck.csv, \n",
      "    Writes okay_lemmas as input/okay.csv\n",
      "    Regenerates output/tocheck.csv\n",
      "    '''\n",
      "    update_okay() \n",
      "    write_okay()\n",
      "    update_to_check()\n",
      "    print_stats()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "update_files()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Currently there are  4512  out of  4510\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Concordance Details\n",
      "\n",
      "Generates \"output/conc_details.csv\" which lists lemmas and their sources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lemmas_out = defaultdict(set)\n",
      "\n",
      "\n",
      "for k,v in okay_lemmas.iteritems(): # k = word; v = lemmas\n",
      "    for l in v:\n",
      "        lemmas_out[l].add(k)\n",
      "\n",
      "with open('output/conc_details.csv','w') as f:\n",
      "    for k,v in sorted(lemmas_out.iteritems()):\n",
      "        f.write(k+','+'|'.join(v)+'\\n')\n",
      "        \n",
      "#okay_lemmas.keys()[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Lemma Instances\n",
      "Sorted list of lemma instances."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def instances_of_lemma(lemma):\n",
      "    i=0\n",
      "    for x in lemmas_out[lemma]:\n",
      "        i+= token_instance_count[x]\n",
      "    return i\n",
      "\n",
      "lemma_instance_count = {lemma: instances_of_lemma(lemma) for lemma in lemmas_out.keys()}\n",
      "#instances_of_lemma for\n",
      "#zz=sorted(lemmas_out.keys(),key=instances_of_lemma)#sort_by_instances)#size_of_lemma_by_instances)\n",
      "#for z in zz: print z, instances_of_lemma[zz])\n",
      "with open(\"output/statistics/lemma-counts.csv\",\"w\") as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        f.write(x+','+str(lemma_instance_count[x])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Izafats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am not sure yet how we will wind up using these. Probably based on a token location range, similarly to compound verbs, etc. There may be some combos I am not grabbing properly. These will need to lemma-ed later (e.g. nasalization)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "izafat_verse_ids = [v_id for v_id in sorted(verses.keys()) if re.search('-e ',verses[v_id])]\n",
      "izafat_verses = [verses[v_id] for v_id in izafat_verse_ids]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "izafat_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+')\n",
      "izafats=Counter()\n",
      "for s in izafat_verses:\n",
      "    x = izafat_re.findall(s)#re.findall(m,s)\n",
      "    for y in x:\n",
      "        izafats[y]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "with open('output/izafats.csv','w') as f:\n",
      "    f.write('\\n'.join(sorted(izafats.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here also is a version of the tokens where izafat phrases are treated as individual tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "iast=Counter() # izafats as tokens, along with tokens\n",
      "iast_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+|[^ ]+')\n",
      "for i,s in verses.iteritems():\n",
      "    words = iast_re.findall(s)\n",
      "    for t in words:\n",
      "        iast[t]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Statistics\n",
      "Word frequencies, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def make_csv_of_token_freq(d, filename):\n",
      "    '''\n",
      "    Generates a CSV file of a dictionary based on numeric value of key, reverse sorted\n",
      "    d: dictionary of tokens and values(token: #)\n",
      "    filename = output file name\n",
      "    '''\n",
      "    with open(filename,'w') as f:\n",
      "        for k,v in d.most_common():\n",
      "            f.write(k+','+str(v)+'\\n')\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "make_csv_of_token_freq(izafats, 'output/statistics/izafat-freq.csv')\n",
      "make_csv_of_token_freq(unique_tokens, 'output/statistics/uniquetokens-freq.csv')\n",
      "make_csv_of_token_freq(iast, 'output/statistics/izafatastokens-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "type(izafats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "collections.Counter"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lemma_counts_beta=Counter()\n",
      "\n",
      "for token, count in unique_tokens.iteritems():\n",
      "    if token in okay_lemmas:\n",
      "        lemma = okay_lemmas[token][0]\n",
      "    else:\n",
      "        lemma = token\n",
      "    lemma_counts_beta[lemma]+=count\n",
      "lemma_counts_beta\n",
      "make_csv_of_token_freq(lemma_counts_beta,'output/statistics/lemmas-beta-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# the following will generate the urdu versions of the statistics (a little slow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import generate_urdu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<z_consonant> + <z_consonant>\n",
        "<m> + <h_char>\n",
        "<n> + consonant\n",
        "<k> + <kh>\n",
        "<r> + <r>\n",
        "<r> + <z>\n",
        "<l> + <h>\n",
        "<v> + <v>\n",
        "<;t> + <;th>\n",
        "<d> + <d>\n",
        "<consonant> <vowel> <s> + <t> <long_vowel>\n",
        "<consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <vowel> <vowel_nasal> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <short_vowel> <h_char> + <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel> <vowel_nasal>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <short_vowel>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant>\n",
        "<consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <vowel> <consonant> + <consonant> <vowel>\n",
        "<wb> <consonant> <short_vowel> <z> + <z> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<n> + <consonant>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "error in string shaan-e shaan\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-6fd5f643b2ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_urdu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/generate_urdu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# <codecell>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shaan-e shaan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# <codecell>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/graphparser/graphparser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error in string\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# for now, croak on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monmatch_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#redo here\n",
      "reload(generate_urdu)#generate_urdu.write_all_urdu_statistics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quick and Dirty Output\n",
      "\n",
      "This generates some quick output for proofing as .md; this a bit sloppy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "with open('output/lemmas-by-size.txt','w') as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        words=lemmas_out[x]\n",
      "        words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "        f.write(x+' '+str(lemma_instance_count[x])+'\\n')\n",
      "        for w in words:\n",
      "            f.write(\"  - \"+w+' '+str(token_instance_count[w])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import codecs\n",
      "import sys\n",
      "sys.path.append('./graphparser/')\n",
      "import graphparser\n",
      "urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "\n",
      "def gen_hiur_lemmas_by_size():\n",
      "    import codecs\n",
      "    import sys\n",
      "    sys.path.append('./graphparser/')\n",
      "    import graphparser\n",
      "    urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "    nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "    def out_hiur(w):\n",
      "        return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "    with codecs.open('output/lemmas-by-size-hiur.md','w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "\n",
      "            f.write(out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "            for w in words:\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                \n",
      "def out_hiur(w):\n",
      "    print w\n",
      "    print 'urdu: '+urdup.parse(w).output\n",
      "    print 'devanagari '+nagarip.parse(w).output\n",
      "    return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "\n",
      "def out_hiur_csv(w):\n",
      "    return urdup.parse(w).output+','+nagarip.parse(w).output+','+w\n",
      "def html_out(w):\n",
      "    return td(urdup.parse(w).output)+td(nagarip.parse(w).output)+td(w)\n",
      "\n",
      "def td(x):\n",
      "    return '<td>'+x+'</td>'\n",
      "\n",
      "def li(x):\n",
      "    return ('<li>'+x+'</li>')\n",
      "def md_link(s,urdu=True):\n",
      "    out =  \" [\"+s+\"]\"\n",
      "    out += \"(\"+'http://www.columbia.edu/itc/mealac/pritchett/00ghalib/'\n",
      "    out += s[0:3]+'/'+s[0:3]+\"_\"+s[4:6]+\".html\"\n",
      "    if urdu==True:\n",
      "        out+=\"?urdu\"\n",
      "    out += \") \"#\n",
      "    return out\n",
      "\n",
      "def gen_hiur_lemmas_by_size_hiur(file_name, with_verses=False, truncate=True,truncate_limit=50):\n",
      "\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "            \n",
      "            f.write(' '+out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "\n",
      "            for w in words:\n",
      "                f.write(\"  - \")\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                vi = set(x[:-5] for x in token_instances[w]) # eg001.01 from 001.01.01.0\n",
      "\n",
      "                if with_verses==True:\n",
      "                    if (truncate==False) or (truncate==True and len (vi)< truncate_limit):\n",
      "    #                print list(vi)[0]\n",
      "                        f.write(\"    - \")# nested indent\n",
      "                        f.write(', '.join([md_link(v) for v in vi]))\n",
      "                        f.write('\\n')\n",
      "\n",
      "def gen_hiur_lemmas_by_size_ul(file_name='output/hiur-lemmas-by-size-ul.html'):\n",
      "    with codecs.open(file_name,'w','utf-8') as f:\n",
      "        f.write('<!DOCTYPE html>\\n')\n",
      "        f.write('<html lang=\"en-GB\">\\n')\n",
      "        f.write('<head><meta charset=\"utf-8\"></head>\\n')\n",
      "        f.write('<body>')\n",
      "\n",
      "\n",
      "        f.write('<table>')\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "            f.write('<p><b>'+out_hiur(x)+' '+str(lemma_instance_count[x])+'</b></p>\\n')\n",
      "            f.write('<ul>')\n",
      "            for w in words:\n",
      "                f.write('<li>'+out_hiur(w)+' '+str(token_instance_count[w])+'</li>\\n')\n",
      "            f.write(\"</ul>\")\n",
      "\n",
      "        f.write(\"</body></html>\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<z_consonant> + <z_consonant>\n",
        "<m> + <h_char>\n",
        "<n> + consonant\n",
        "<k> + <kh>\n",
        "<r> + <r>\n",
        "<r> + <z>\n",
        "<l> + <h>\n",
        "<v> + <v>\n",
        "<;t> + <;th>\n",
        "<d> + <d>\n",
        "<consonant> <vowel> <s> + <t> <long_vowel>\n",
        "<consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <vowel> <vowel_nasal> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <short_vowel> <h_char> + <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel> <vowel_nasal>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <short_vowel>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant>\n",
        "<consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <vowel> <consonant> + <consonant> <vowel>\n",
        "<wb> <consonant> <short_vowel> <z> + <z> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<n> + <consonant>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#md_link('001.03')\n",
      "print urdup.parse('hu))e').output#ab-o-gil').output\n",
      "print nagarip.parse('kyaa se').output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u06c1\u0626\u06cc\n",
        "\u0915\u094d\u092f\u093e \u0938\u0947\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#gen_hiur_lemmas_by_size()\n",
      "#gen_hiur_lemmas_by_size_with_verses()\n",
      "#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-w-verses-all-hiur.md', with_verses=True, truncate=False)#True,truncate_limit=50):\n",
      "#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-countsonly.md', with_verses=False)#True,truncate_limit=50):\n",
      "gen_hiur_lemmas_by_size_ul()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "honaa\n",
        "urdu: \u06c1\u0648\u0646\u0627\n",
        "devanagari \u0939\u094b\u0928\u093e\n",
        "hai\n",
        "urdu: \u06c1\u06cc\n",
        "devanagari \u0939\u0948\n",
        "hai;n\n",
        "urdu: \u06c1\u06cc\u0646\n",
        "devanagari \u0939\u0948\u0902\n",
        "thaa\n",
        "urdu: \u062a\u06be\u0627\n",
        "devanagari \u0925\u093e\n",
        "hu))aa\n",
        "urdu: \u06c1\u0626\u0627\n",
        "devanagari \u0939\u0941\u093e\n",
        "hu))e\n",
        "urdu: \u06c1\u0626\u06cc\n",
        "devanagari \u0939\u0941\u0947\n",
        "hu))ii\n",
        "urdu: \u06c1\u0626\u06cc\n",
        "devanagari \u0939\u0941\u0940\n",
        "the\n",
        "urdu: \u062a\u06be\u06cc\n",
        "devanagari \u0925\u0947\n",
        "thii\n",
        "urdu: \u062a\u06be\u06cc\n",
        "devanagari \u0925\u0940\n",
        "honaa\n",
        "urdu: \u06c1\u0648\u0646\u0627\n",
        "devanagari \u0939\u094b\u0928\u093e\n",
        "ho;n\n",
        "urdu: \u06c1\u0648\u0646\n",
        "devanagari \u0939\u094b\u0902\n",
        "thii;n\n",
        "urdu: \u062a\u06be\u06cc\u0646\n",
        "devanagari \u0925\u0940\u0902\n",
        "huujiye\n",
        "urdu: \u06c1\u0648\u062c\u06cc\u06cc\n",
        "devanagari \u0939\u0942\u091c\u093f\u092f\u0947\n",
        "hu))o;n\n",
        "urdu: \u06c1\u0626\u0648\u0646\n",
        "devanagari \u0939\u0941\u094b\u0902\n",
        "hu;n\n",
        "urdu: \u06c1\u0646\n",
        "devanagari \u0939\u0941\u0902\n",
        "kaa\n",
        "urdu: \u06a9\u0627\n",
        "devanagari \u0915\u093e\n",
        "kaa\n",
        "urdu: \u06a9\u0627\n",
        "devanagari \u0915\u093e\n",
        "ke\n",
        "urdu: \u06a9\u06cc\n",
        "devanagari \u0915\u0947\n",
        "se\n",
        "urdu: \u0633\u06cc\n",
        "devanagari \u0938\u0947\n",
        "se\n",
        "urdu: \u0633\u06cc\n",
        "devanagari \u0938\u0947\n",
        "me;n\n",
        "urdu: \u0645\u06cc\u0646\n",
        "devanagari \u092e\u0947\u0902\n",
        "me;n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "urdu: \u0645\u06cc\u0646\n",
        "devanagari \u092e\u0947\u0902\n",
        "nah\n",
        "urdu: \u0646\u06c1\n",
        "devanagari \u0928\u0939\n",
        "nah\n",
        "urdu: \u0646\u06c1\n",
        "devanagari \u0928\u0939\n",
        "kih\n",
        "urdu: \u06a9\u06c1\n",
        "devanagari \u0915\u093f\u0939\n",
        "kih\n",
        "urdu: \u06a9\u06c1\n",
        "devanagari \u0915\u093f\u0939\n",
        "ko\n",
        "urdu: \u06a9\u0648\n",
        "devanagari \u0915\u094b\n",
        "ko\n",
        "urdu: \u06a9\u0648\n",
        "devanagari \u0915\u094b\n",
        "kii\n",
        "urdu: \u06a9\u06cc\n",
        "devanagari \u0915\u0940\n",
        "kii\n",
        "urdu: \u06a9\u06cc\n",
        "devanagari \u0915\u0940\n",
        "karnaa\n",
        "urdu: \u06a9\u0631\u0646\u0627\n",
        "devanagari \u0915\u0930\u0928\u093e\n",
        "kar\n",
        "urdu: \u06a9\u0631\n",
        "devanagari \u0915\u0930\n",
        "kare\n",
        "urdu: \u06a9\u0631\u06cc\n",
        "devanagari \u0915\u0930\u0947\n",
        "kiye\n",
        "urdu: \u06a9\u06cc\u06cc\n",
        "devanagari \u0915\u093f\u092f\u0947\n",
        "kiyaa\n",
        "urdu: \u06a9\u06cc\u0627\n",
        "devanagari \u0915\u093f\u092f\u093e\n",
        "kartaa\n",
        "urdu: \u06a9\u0631\u062a\u0627\n",
        "devanagari \u0915\u0930\u0924\u093e\n",
        "karte\n",
        "urdu: \u06a9\u0631\u062a\u06cc\n",
        "devanagari \u0915\u0930\u0924\u0947\n",
        "kiije\n",
        "urdu: \u06a9\u06cc\u062c\u06cc\n",
        "devanagari \u0915\u0940\u091c\u0947\n",
        "karuu;n\n",
        "urdu: \u06a9\u0631\u0648\u0646\n",
        "devanagari \u0915\u0930\u0942\u0902\n",
        "kiijiye\n",
        "urdu: \u06a9\u06cc\u062c\u06cc\u06cc\n",
        "devanagari \u0915\u0940\u091c\u093f\u092f\u0947\n",
        "kare;n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "urdu: \u06a9\u0631\u06cc\u0646\n",
        "devanagari \u0915\u0930\u0947\u0902\n",
        "karne\n",
        "urdu: \u06a9\u0631\u0646\u06cc\n",
        "devanagari \u0915\u0930\u0928\u0947\n",
        "kartii\n",
        "urdu: \u06a9\u0631\u062a\u06cc\n",
        "devanagari \u0915\u0930\u0924\u0940\n",
        "karnaa\n",
        "urdu: \u06a9\u0631\u0646\u0627\n",
        "devanagari \u0915\u0930\u0928\u093e\n",
        "kare;nge\n",
        "urdu: \u06a9\u0631\u06cc\u0646\u06af\u06cc\n",
        "devanagari \u0915\u0930\u0947\u0902\u0917\u0947\n",
        "karuu;ngaa\n",
        "urdu: \u06a9\u0631\u0648\u0646\u06af\u0627\n",
        "devanagari \u0915\u0930\u0942\u0902\u0917\u093e\n",
        "ho\n",
        "urdu: \u06c1\u0648\n",
        "devanagari \u0939\u094b\n",
        "ho\n",
        "urdu: \u06c1\u0648\n",
        "devanagari \u0939\u094b\n",
        "nahii;n\n",
        "urdu: \u0646\u06c1\u06cc\u0646\n",
        "devanagari \u0928\u0939\u0940\u0902\n",
        "nahii;n\n",
        "urdu: \u0646\u06c1\u06cc\u0646\n",
        "devanagari \u0928\u0939\u0940\u0902\n",
        "mai;n\n",
        "urdu: \u0645\u06cc\u0646\n",
        "devanagari \u092e\u0948\u0902\n",
        "mai;n\n",
        "urdu: \u0645\u06cc\u0646\n",
        "devanagari \u092e\u0948\u0902\n",
        "mujhe\n",
        "urdu: \u0645\u062c\u06be\u06cc\n",
        "devanagari \u092e\u0941\u091d\u0947\n",
        "mujh\n",
        "urdu: \u0645\u062c\u06be\n",
        "devanagari \u092e\u0941\u091d\n",
        "mujhii\n",
        "urdu: \u0645\u062c\u06be\u06cc\n",
        "devanagari \u092e\u0941\u091d\u0940\n",
        "jaanaa"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "urdu: \u062c\u0627\u0646\u0627\n",
        "devanagari \u091c\u093e\u0928\u093e\n",
        "gayaa\n",
        "urdu: \u06af\u06cc\u0627\n",
        "devanagari \u0917\u092f\u093e\n",
        "jaa))e\n",
        "urdu: \u062c\u0627\u0626\u06cc\n",
        "devanagari \u091c\u093e\u0947\n",
        "ga))ii\n",
        "urdu: \u06af\u0626\u06cc\n",
        "devanagari \u0917\u0940\n",
        "ga))e\n",
        "urdu: \u06af\u0626\u06cc\n",
        "devanagari \u0917\u0947\n",
        "jaanaa\n",
        "urdu: \u062c\u0627\u0646\u0627\n",
        "devanagari \u091c\u093e\u0928\u093e\n",
        "ga))ii;n\n",
        "urdu: \u06af\u0626\u06cc\u0646\n",
        "devanagari \u0917\u0940\u0902\n",
        "jaa))egaa\n",
        "urdu: \u062c\u0627\u0626\u06cc\u06af\u0627\n",
        "devanagari \u091c\u093e\u0947\u0917\u093e\n",
        "jaataa\n",
        "urdu: \u062c\u0627\u062a\u0627\n",
        "devanagari \u091c\u093e\u0924\u093e\n",
        "jaate\n",
        "urdu: \u062c\u0627\u062a\u06cc\n",
        "devanagari \u091c\u093e\u0924\u0947\n",
        "gaye\n",
        "urdu: \u06af\u06cc\u06cc\n",
        "devanagari \u0917\u092f\u0947\n",
        "jaave\n",
        "urdu: \u062c\u0627\u0648\u06cc\n",
        "devanagari \u091c\u093e\u0935\u0947\n",
        "jaatii\n",
        "urdu: \u062c\u0627\u062a\u06cc\n",
        "devanagari \u091c\u093e\u0924\u0940\n",
        "jaa))uu;n\n",
        "urdu: \u062c\u0627\u0626\u0648\u0646\n",
        "devanagari \u091c\u093e\u0942\u0902\n",
        "jaa))e;n\n",
        "urdu: \u062c\u0627\u0626\u06cc\u0646\n",
        "devanagari \u091c\u093e\u0947\u0902\n",
        "jaa))iye\n",
        "urdu: \u062c\u0627\u0626\u06cc\u06cc\n",
        "devanagari \u091c\u093e\u093f\u092f\u0947"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "jaane\n",
        "urdu: \u062c\u0627\u0646\u06cc\n",
        "devanagari \u091c\u093e\u0928\u0947\n",
        "jaave;nge\n",
        "urdu: \u062c\u0627\u0648\u06cc\u0646\u06af\u06cc\n",
        "devanagari \u091c\u093e\u0935\u0947\u0902\u0917\u0947\n",
        "jaa))egii\n",
        "urdu: \u062c\u0627\u0626\u06cc\u06af\u06cc\n",
        "devanagari \u091c\u093e\u0947\u0917\u0940\n",
        "jaa))e;nge\n",
        "urdu: \u062c\u0627\u0626\u06cc\u0646\u06af\u06cc\n",
        "devanagari \u091c\u093e\u0947\u0902\u0917\u0947\n",
        "jaave;n\n",
        "urdu: \u062c\u0627\u0648\u06cc\u0646\n",
        "devanagari \u091c\u093e\u0935\u0947\u0902\n",
        "kyaa\n",
        "urdu: \u06a9\u06cc\u0627\n",
        "devanagari \u0915\u094d\u092f\u093e\n",
        "kyaa\n",
        "urdu: \u06a9\u06cc\u0627\n",
        "devanagari \u0915\u094d\u092f\u093e\n",
        "meraa\n",
        "urdu: \u0645\u06cc\u0631\u0627\n",
        "devanagari \u092e\u0947\u0930\u093e\n",
        "mire\n",
        "urdu: \u0645\u0631\u06cc\n",
        "devanagari \u092e\u093f\u0930\u0947\n",
        "mere\n",
        "urdu: \u0645\u06cc\u0631\u06cc\n",
        "devanagari \u092e\u0947\u0930\u0947\n",
        "merii\n",
        "urdu: \u0645\u06cc\u0631\u06cc\n",
        "devanagari \u092e\u0947\u0930\u0940\n",
        "mirii\n",
        "urdu: \u0645\u0631\u06cc\n",
        "devanagari \u092e\u093f\u0930\u0940\n",
        "meraa\n",
        "urdu: \u0645\u06cc\u0631\u0627\n",
        "devanagari \u092e\u0947\u0930\u093e\n",
        "miraa\n",
        "urdu: \u0645\u0631\u0627\n",
        "devanagari \u092e\u093f\u0930\u093e\n",
        "dil\n",
        "urdu: \u062f\u0644\n",
        "devanagari \u0926\u093f\u0932\n",
        "dil\n",
        "urdu: \u062f\u0644\n",
        "devanagari \u0926\u093f\u0932\n",
        "dil-e\n",
        "error in string dil-e\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-32-0f61aee6104c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-w-verses-all-hiur.md', with_verses=True, truncate=False)#True,truncate_limit=50):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#gen_hiur_lemmas_by_size_hiur('output/lemmas-by-size-countsonly.md', with_verses=False)#True,truncate_limit=50):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgen_hiur_lemmas_by_size_ul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-30-bc1da17cf023>\u001b[0m in \u001b[0;36mgen_hiur_lemmas_by_size_ul\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<ul>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<li>'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mout_hiur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_instance_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'</li>\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</ul>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-30-bc1da17cf023>\u001b[0m in \u001b[0;36mout_hiur\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mout_hiur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'urdu: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'devanagari '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnagarip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnagarip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/graphparser/graphparser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error in string\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# for now, croak on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monmatch_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import markdown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "nagarip.parse('kyaa').output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "adding\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "u'\\u0915\\u094d\\u092f\\u093e'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print nagarip.parse('se').output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u0938\u090f\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.tokenize('shaan-eshaan')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "['sh', 'aa', 'n', '-e', ' ', 'sh', 'aa', 'n']"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.parse('shaan-eshaan')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "error in string shaan-eshaan\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-39-31ebfde5fc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murdup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shaan-eshaan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/seanpue/ghalib-concordance/graphparser/graphparser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error in string\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# for now, croak on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monmatch_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "(ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'ii'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06c1\\u0648\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'aa'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2\\u0654\\u06af\\u0627', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'aa'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2\\u0654\\u06af\\u06cc', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'ii'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06cc\\u06ba\\u200c\\u06af\\u06d2', prev_class=None, prev_tokens=None, tokens=['e', ';n', 'g', 'e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06a9\\u06c1\\u06c1', prev_class=['wb'], prev_tokens=None, tokens=['k', 'a', 'h'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2\\u06af\\u06cc', prev_class=None, prev_tokens=None, tokens=['e', 'g', 'ii'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2\\u06af\\u0627', prev_class=None, prev_tokens=None, tokens=['e', 'g', 'aa'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['-', 'o', '-'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648\\u0654', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'uu'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648\\u0654', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'o'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062a\\u0651', prev_class=None, prev_tokens=None, tokens=['t', 't'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u067e\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['p', 'ph'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0631\\u0651', prev_class=None, prev_tokens=None, tokens=['r', 'r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u067e\\u0651', prev_class=None, prev_tokens=None, tokens=['p', 'p'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0628\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['b', 'bh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0644', prev_class=None, prev_tokens=None, tokens=['l', 'l'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06af\\u0651', prev_class=None, prev_tokens=None, tokens=['g', 'g'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0637\\u0651', prev_class=None, prev_tokens=None, tokens=[':t', ':t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062e\\u0651', prev_class=None, prev_tokens=None, tokens=[';x', ';x'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0688\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=[';d', ';dh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0686\\u0651', prev_class=None, prev_tokens=None, tokens=['ch', 'ch'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062d\\u0651', prev_class=None, prev_tokens=None, tokens=[';h', ';h'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0686\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['ch', 'chh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0641\\u0651', prev_class=None, prev_tokens=None, tokens=['f', 'f'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0628\\u0651', prev_class=None, prev_tokens=None, tokens=['b', 'b'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0688\\u0651', prev_class=None, prev_tokens=None, tokens=[';d', ';d'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0635\\u0651', prev_class=None, prev_tokens=None, tokens=['.s', '.s'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0633\\u0651', prev_class=None, prev_tokens=None, tokens=['s', 's'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0639\\u0651', prev_class=None, prev_tokens=None, tokens=['((', '(('], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0642\\u0651', prev_class=None, prev_tokens=None, tokens=['q', 'q'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062c\\u0651', prev_class=None, prev_tokens=None, tokens=['j', 'j'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062f\\u0651', prev_class=None, prev_tokens=None, tokens=['d', 'd'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc\\u0651', prev_class=None, prev_tokens=None, tokens=['ai', 'y'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06a9\\u0651', prev_class=None, prev_tokens=None, tokens=['k', 'k'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0645\\u0651', prev_class=None, prev_tokens=None, tokens=['m', 'm'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06af\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['g', 'gh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0679\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=[';t', ';th'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062a\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['t', 'th'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u063a\\u0651', prev_class=None, prev_tokens=None, tokens=[';g', ';g'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0636\\u0651', prev_class=None, prev_tokens=None, tokens=['.z', '.z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062b\\u0651', prev_class=None, prev_tokens=None, tokens=[';s', ';s'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0634\\u0651', prev_class=None, prev_tokens=None, tokens=['sh', 'sh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc\\u0651', prev_class=None, prev_tokens=None, tokens=['y', 'y'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06c1\\u0651', prev_class=None, prev_tokens=None, tokens=['h', 'h'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062f\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['d', 'dh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648\\u0651', prev_class=None, prev_tokens=None, tokens=['v', 'v'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06a9\\u06be', prev_class=None, prev_tokens=None, tokens=['k', 'kh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0691\\u0651', prev_class=None, prev_tokens=None, tokens=[';r', ';r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0679\\u0651', prev_class=None, prev_tokens=None, tokens=[';t', ';t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062e\\u0651', prev_class=None, prev_tokens=None, tokens=[';z', ';z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0646\\u0651', prev_class=None, prev_tokens=None, tokens=['n', 'n'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0638\\u0651', prev_class=None, prev_tokens=None, tokens=[':z', ':z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0632\\u0651', prev_class=None, prev_tokens=None, tokens=['z', 'z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062c\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['j', 'jh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06c1', prev_class=['short_vowel_a'], prev_tokens=None, tokens=['h'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2', prev_class=['nc_long_vowel'], prev_tokens=None, tokens=['-e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627\\u0650\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06ba', prev_class=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['u'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=[';e'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['a'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u0650', prev_class=['wb'], prev_tokens=None, tokens=['^i'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['i'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['ul-'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u064e', prev_class=['wb'], prev_tokens=None, tokens=['^a'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['o'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0622', prev_class=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=':', prev_class=None, prev_tokens=None, tokens=[':'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['au'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['ii'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u064f\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['^uu'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0644\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['il-'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0650', prev_class=None, prev_tokens=None, tokens=['-e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['^ii'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u064f', prev_class=['wb'], prev_tokens=None, tokens=['^u'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06ba', prev_class=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u064e\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['^au'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['uu'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='.', prev_class=None, prev_tokens=None, tokens=['.'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u06d2', prev_class=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0650\\u06d2', prev_class=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=u'\\u0644\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['al-'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06d2', prev_class=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=['wb']),\n",
        " ParserRule(production=' ', prev_class=None, prev_tokens=None, tokens=['-'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['/'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06af\\u06be', prev_class=None, prev_tokens=None, tokens=['gh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=' ', prev_class=None, prev_tokens=None, tokens=' ', next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0691\\u06be', prev_class=None, prev_tokens=None, tokens=[';rh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='(', prev_class=None, prev_tokens=None, tokens=['('], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u060c', prev_class=None, prev_tokens=None, tokens=[','], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062a\\u06be', prev_class=None, prev_tokens=None, tokens=['th'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u064b', prev_class=None, prev_tokens=None, tokens=[':n'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062f\\u06be', prev_class=None, prev_tokens=None, tokens=['dh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0639', prev_class=None, prev_tokens=None, tokens=['(('], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0638', prev_class=None, prev_tokens=None, tokens=[':z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0637', prev_class=None, prev_tokens=None, tokens=[':t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062f', prev_class=None, prev_tokens=None, tokens=['d'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06d4', prev_class=None, prev_tokens=None, tokens=['--'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06c1', prev_class=None, prev_tokens=None, tokens=['h'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0644', prev_class=None, prev_tokens=None, tokens=['l'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u067e', prev_class=None, prev_tokens=None, tokens=['p'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062a', prev_class=None, prev_tokens=None, tokens=['t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['|'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062b', prev_class=None, prev_tokens=None, tokens=[';s'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0691', prev_class=None, prev_tokens=None, tokens=[';r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0698', prev_class=None, prev_tokens=None, tokens=['zh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0679', prev_class=None, prev_tokens=None, tokens=[';t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0630', prev_class=None, prev_tokens=None, tokens=[';z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062e', prev_class=None, prev_tokens=None, tokens=[';x'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0688\\u06be', prev_class=None, prev_tokens=None, tokens=[';dh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=[';e'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0688', prev_class=None, prev_tokens=None, tokens=[';d'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062d', prev_class=None, prev_tokens=None, tokens=[';h'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0679\\u06be', prev_class=None, prev_tokens=None, tokens=[';th'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=[\"'\"], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u061b', prev_class=None, prev_tokens=None, tokens=[';'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u061f', prev_class=None, prev_tokens=None, tokens=['?'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u064e\\u0648', prev_class=None, prev_tokens=None, tokens=['^au'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0628\\u06be', prev_class=None, prev_tokens=None, tokens=['bh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062c\\u06be', prev_class=None, prev_tokens=None, tokens=['jh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='[', prev_class=None, prev_tokens=None, tokens=['['], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['_'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0635', prev_class=None, prev_tokens=None, tokens=['.s'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06af', prev_class=None, prev_tokens=None, tokens=['g'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc\\u0670', prev_class=None, prev_tokens=None, tokens=[';aa'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0633', prev_class=None, prev_tokens=None, tokens=['s'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0650\\u06cc', prev_class=None, prev_tokens=None, tokens=['^ii'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['w'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0686', prev_class=None, prev_tokens=None, tokens=['ch'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='\\n', prev_class=None, prev_tokens=None, tokens=['\\n'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0626', prev_class=None, prev_tokens=None, tokens=['))'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0686\\u06be', prev_class=None, prev_tokens=None, tokens=['chh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='\\t', prev_class=None, prev_tokens=None, tokens=['\\t'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u067e\\u06be', prev_class=None, prev_tokens=None, tokens=['ph'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06d4\\u06d4', prev_class=None, prev_tokens=None, tokens=['----'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0628', prev_class=None, prev_tokens=None, tokens=['b'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0641', prev_class=None, prev_tokens=None, tokens=['f'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['uu'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u062c', prev_class=None, prev_tokens=None, tokens=['j'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=['n'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0636', prev_class=None, prev_tokens=None, tokens=['.z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0631', prev_class=None, prev_tokens=None, tokens=['r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['v'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0631', prev_class=None, prev_tokens=None, tokens=['.r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0632', prev_class=None, prev_tokens=None, tokens=['z'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06a9', prev_class=None, prev_tokens=None, tokens=['k'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0627', prev_class=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='\\r', prev_class=None, prev_tokens=None, tokens=['\\r'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['ii'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['au'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['o'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='!', prev_class=None, prev_tokens=None, tokens=['!'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=')', prev_class=None, prev_tokens=None, tokens=[')'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='\"', prev_class=None, prev_tokens=None, tokens=['\"'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u063a', prev_class=None, prev_tokens=None, tokens=[';g'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=']', prev_class=None, prev_tokens=None, tokens=[']'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['a'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u064f\\u0648', prev_class=None, prev_tokens=None, tokens=['^uu'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06a9\\u06be', prev_class=None, prev_tokens=None, tokens=['kh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0645', prev_class=None, prev_tokens=None, tokens=['m'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u064f', prev_class=None, prev_tokens=None, tokens=['^u'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0642', prev_class=None, prev_tokens=None, tokens=['q'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0650', prev_class=None, prev_tokens=None, tokens=['^i'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u0634', prev_class=None, prev_tokens=None, tokens=['sh'], next_tokens=None, next_class=None),\n",
        " ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['u'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['y'], next_tokens=None, next_class=None),\n",
        " ParserRule(production=u'\\u064e', prev_class=None, prev_tokens=None, tokens=['^a'], next_tokens=None, next_class=None))"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urdup.DG.nodes(data=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "[(0, {'token': None}),\n",
        " (1, {'token': 'h'}),\n",
        " (2, {'token': 'u'}),\n",
        " (3, {'token': '))'}),\n",
        " (4, {'token': 'ii'}),\n",
        " (5,\n",
        "  {'found': u'\\u06c1\\u0648\\u0626\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'ii'], next_tokens=None, next_class=['wb'])}),\n",
        " (6, {'token': 'aa'}),\n",
        " (7,\n",
        "  {'found': u'\\u06c1\\u0648\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u06c1\\u0648\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'aa'], next_tokens=None, next_class=['wb'])}),\n",
        " (8, {'token': '))'}),\n",
        " (9, {'token': 'e'}),\n",
        " (10, {'token': 'g'}),\n",
        " (11, {'token': 'aa'}),\n",
        " (12,\n",
        "  {'found': u'\\u06d2\\u0654\\u06af\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u06d2\\u0654\\u06af\\u0627', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'aa'], next_tokens=None, next_class=['wb'])}),\n",
        " (13, {'token': 'ii'}),\n",
        " (14,\n",
        "  {'found': u'\\u06d2\\u0654\\u06af\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06d2\\u0654\\u06af\\u06cc', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'e', 'g', 'ii'], next_tokens=None, next_class=['wb'])}),\n",
        " (15, {'token': 'e'}),\n",
        " (16,\n",
        "  {'found': u'\\u06c1\\u0648\\u0626\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u06c1\\u0648\\u0626\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['h', 'u', '))', 'e'], next_tokens=None, next_class=['wb'])}),\n",
        " (17, {'token': 'e'}),\n",
        " (18, {'token': ';n'}),\n",
        " (19, {'token': 'g'}),\n",
        " (20, {'token': 'e'}),\n",
        " (21,\n",
        "  {'found': u'\\u06cc\\u06ba\\u200c\\u06af\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u06cc\\u06ba\\u200c\\u06af\\u06d2', prev_class=None, prev_tokens=None, tokens=['e', ';n', 'g', 'e'], next_tokens=None, next_class=['wb'])}),\n",
        " (22, {'token': 'k'}),\n",
        " (23, {'token': 'a'}),\n",
        " (24, {'token': 'h'}),\n",
        " (25,\n",
        "  {'found': u'\\u06a9\\u06c1\\u06c1',\n",
        "   'rule': ParserRule(production=u'\\u06a9\\u06c1\\u06c1', prev_class=['wb'], prev_tokens=None, tokens=['k', 'a', 'h'], next_tokens=None, next_class=['wb'])}),\n",
        " (26, {'token': 'g'}),\n",
        " (27, {'token': 'ii'}),\n",
        " (28,\n",
        "  {'found': u'\\u06d2\\u06af\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06d2\\u06af\\u06cc', prev_class=None, prev_tokens=None, tokens=['e', 'g', 'ii'], next_tokens=None, next_class=['wb'])}),\n",
        " (29, {'token': 'aa'}),\n",
        " (30,\n",
        "  {'found': u'\\u06d2\\u06af\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u06d2\\u06af\\u0627', prev_class=None, prev_tokens=None, tokens=['e', 'g', 'aa'], next_tokens=None, next_class=['wb'])}),\n",
        " (31, {'token': '-'}),\n",
        " (32, {'token': 'o'}),\n",
        " (33, {'token': '-'}),\n",
        " (34,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['-', 'o', '-'], next_tokens=None, next_class=None)}),\n",
        " (35, {'token': 'uu'}),\n",
        " (36,\n",
        "  {'found': u'\\u0648\\u0654',\n",
        "   'rule': ParserRule(production=u'\\u0648\\u0654', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'uu'], next_tokens=None, next_class=None)}),\n",
        " (37, {'token': 'o'}),\n",
        " (38,\n",
        "  {'found': u'\\u0648\\u0654',\n",
        "   'rule': ParserRule(production=u'\\u0648\\u0654', prev_class=['vowel'], prev_tokens=None, tokens=['))', 'o'], next_tokens=None, next_class=None)}),\n",
        " (39, {'token': 't'}),\n",
        " (40, {'token': 't'}),\n",
        " (41,\n",
        "  {'found': u'\\u062a\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062a\\u0651', prev_class=None, prev_tokens=None, tokens=['t', 't'], next_tokens=None, next_class=None)}),\n",
        " (42, {'token': 'p'}),\n",
        " (43, {'token': 'ph'}),\n",
        " (44,\n",
        "  {'found': u'\\u067e\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u067e\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['p', 'ph'], next_tokens=None, next_class=None)}),\n",
        " (45, {'token': 'r'}),\n",
        " (46, {'token': 'r'}),\n",
        " (47,\n",
        "  {'found': u'\\u0631\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0631\\u0651', prev_class=None, prev_tokens=None, tokens=['r', 'r'], next_tokens=None, next_class=None)}),\n",
        " (48, {'token': 'p'}),\n",
        " (49,\n",
        "  {'found': u'\\u067e\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u067e\\u0651', prev_class=None, prev_tokens=None, tokens=['p', 'p'], next_tokens=None, next_class=None)}),\n",
        " (50, {'token': 'b'}),\n",
        " (51, {'token': 'bh'}),\n",
        " (52,\n",
        "  {'found': u'\\u0628\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0628\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['b', 'bh'], next_tokens=None, next_class=None)}),\n",
        " (53, {'token': 'l'}),\n",
        " (54, {'token': 'l'}),\n",
        " (55,\n",
        "  {'found': u'\\u0644',\n",
        "   'rule': ParserRule(production=u'\\u0644', prev_class=None, prev_tokens=None, tokens=['l', 'l'], next_tokens=None, next_class=None)}),\n",
        " (56, {'token': 'g'}),\n",
        " (57, {'token': 'g'}),\n",
        " (58,\n",
        "  {'found': u'\\u06af\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u06af\\u0651', prev_class=None, prev_tokens=None, tokens=['g', 'g'], next_tokens=None, next_class=None)}),\n",
        " (59, {'token': ':t'}),\n",
        " (60, {'token': ':t'}),\n",
        " (61,\n",
        "  {'found': u'\\u0637\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0637\\u0651', prev_class=None, prev_tokens=None, tokens=[':t', ':t'], next_tokens=None, next_class=None)}),\n",
        " (62, {'token': ';x'}),\n",
        " (63, {'token': ';x'}),\n",
        " (64,\n",
        "  {'found': u'\\u062e\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062e\\u0651', prev_class=None, prev_tokens=None, tokens=[';x', ';x'], next_tokens=None, next_class=None)}),\n",
        " (65, {'token': ';d'}),\n",
        " (66, {'token': ';dh'}),\n",
        " (67,\n",
        "  {'found': u'\\u0688\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0688\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=[';d', ';dh'], next_tokens=None, next_class=None)}),\n",
        " (68, {'token': 'ch'}),\n",
        " (69, {'token': 'ch'}),\n",
        " (70,\n",
        "  {'found': u'\\u0686\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0686\\u0651', prev_class=None, prev_tokens=None, tokens=['ch', 'ch'], next_tokens=None, next_class=None)}),\n",
        " (71, {'token': ';h'}),\n",
        " (72, {'token': ';h'}),\n",
        " (73,\n",
        "  {'found': u'\\u062d\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062d\\u0651', prev_class=None, prev_tokens=None, tokens=[';h', ';h'], next_tokens=None, next_class=None)}),\n",
        " (74, {'token': 'chh'}),\n",
        " (75,\n",
        "  {'found': u'\\u0686\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0686\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['ch', 'chh'], next_tokens=None, next_class=None)}),\n",
        " (76, {'token': 'f'}),\n",
        " (77, {'token': 'f'}),\n",
        " (78,\n",
        "  {'found': u'\\u0641\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0641\\u0651', prev_class=None, prev_tokens=None, tokens=['f', 'f'], next_tokens=None, next_class=None)}),\n",
        " (79, {'token': 'b'}),\n",
        " (80,\n",
        "  {'found': u'\\u0628\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0628\\u0651', prev_class=None, prev_tokens=None, tokens=['b', 'b'], next_tokens=None, next_class=None)}),\n",
        " (81, {'token': ';d'}),\n",
        " (82,\n",
        "  {'found': u'\\u0688\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0688\\u0651', prev_class=None, prev_tokens=None, tokens=[';d', ';d'], next_tokens=None, next_class=None)}),\n",
        " (83, {'token': '.s'}),\n",
        " (84, {'token': '.s'}),\n",
        " (85,\n",
        "  {'found': u'\\u0635\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0635\\u0651', prev_class=None, prev_tokens=None, tokens=['.s', '.s'], next_tokens=None, next_class=None)}),\n",
        " (86, {'token': 's'}),\n",
        " (87, {'token': 's'}),\n",
        " (88,\n",
        "  {'found': u'\\u0633\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0633\\u0651', prev_class=None, prev_tokens=None, tokens=['s', 's'], next_tokens=None, next_class=None)}),\n",
        " (89, {'token': '(('}),\n",
        " (90, {'token': '(('}),\n",
        " (91,\n",
        "  {'found': u'\\u0639\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0639\\u0651', prev_class=None, prev_tokens=None, tokens=['((', '(('], next_tokens=None, next_class=None)}),\n",
        " (92, {'token': 'q'}),\n",
        " (93, {'token': 'q'}),\n",
        " (94,\n",
        "  {'found': u'\\u0642\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0642\\u0651', prev_class=None, prev_tokens=None, tokens=['q', 'q'], next_tokens=None, next_class=None)}),\n",
        " (95, {'token': 'j'}),\n",
        " (96, {'token': 'j'}),\n",
        " (97,\n",
        "  {'found': u'\\u062c\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062c\\u0651', prev_class=None, prev_tokens=None, tokens=['j', 'j'], next_tokens=None, next_class=None)}),\n",
        " (98, {'token': 'd'}),\n",
        " (99, {'token': 'd'}),\n",
        " (100,\n",
        "  {'found': u'\\u062f\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062f\\u0651', prev_class=None, prev_tokens=None, tokens=['d', 'd'], next_tokens=None, next_class=None)}),\n",
        " (101, {'token': 'ai'}),\n",
        " (102, {'token': 'y'}),\n",
        " (103,\n",
        "  {'found': u'\\u06cc\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u06cc\\u0651', prev_class=None, prev_tokens=None, tokens=['ai', 'y'], next_tokens=None, next_class=None)}),\n",
        " (104, {'token': 'k'}),\n",
        " (105,\n",
        "  {'found': u'\\u06a9\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u06a9\\u0651', prev_class=None, prev_tokens=None, tokens=['k', 'k'], next_tokens=None, next_class=None)}),\n",
        " (106, {'token': 'm'}),\n",
        " (107, {'token': 'm'}),\n",
        " (108,\n",
        "  {'found': u'\\u0645\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0645\\u0651', prev_class=None, prev_tokens=None, tokens=['m', 'm'], next_tokens=None, next_class=None)}),\n",
        " (109, {'token': 'gh'}),\n",
        " (110,\n",
        "  {'found': u'\\u06af\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u06af\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['g', 'gh'], next_tokens=None, next_class=None)}),\n",
        " (111, {'token': ';t'}),\n",
        " (112, {'token': ';th'}),\n",
        " (113,\n",
        "  {'found': u'\\u0679\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0679\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=[';t', ';th'], next_tokens=None, next_class=None)}),\n",
        " (114, {'token': 'th'}),\n",
        " (115,\n",
        "  {'found': u'\\u062a\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062a\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['t', 'th'], next_tokens=None, next_class=None)}),\n",
        " (116, {'token': ';g'}),\n",
        " (117, {'token': ';g'}),\n",
        " (118,\n",
        "  {'found': u'\\u063a\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u063a\\u0651', prev_class=None, prev_tokens=None, tokens=[';g', ';g'], next_tokens=None, next_class=None)}),\n",
        " (119, {'token': '.z'}),\n",
        " (120, {'token': '.z'}),\n",
        " (121,\n",
        "  {'found': u'\\u0636\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0636\\u0651', prev_class=None, prev_tokens=None, tokens=['.z', '.z'], next_tokens=None, next_class=None)}),\n",
        " (122, {'token': ';s'}),\n",
        " (123, {'token': ';s'}),\n",
        " (124,\n",
        "  {'found': u'\\u062b\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062b\\u0651', prev_class=None, prev_tokens=None, tokens=[';s', ';s'], next_tokens=None, next_class=None)}),\n",
        " (125, {'token': 'sh'}),\n",
        " (126, {'token': 'sh'}),\n",
        " (127,\n",
        "  {'found': u'\\u0634\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0634\\u0651', prev_class=None, prev_tokens=None, tokens=['sh', 'sh'], next_tokens=None, next_class=None)}),\n",
        " (128, {'token': 'y'}),\n",
        " (129, {'token': 'y'}),\n",
        " (130,\n",
        "  {'found': u'\\u06cc\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u06cc\\u0651', prev_class=None, prev_tokens=None, tokens=['y', 'y'], next_tokens=None, next_class=None)}),\n",
        " (131, {'token': 'h'}),\n",
        " (132,\n",
        "  {'found': u'\\u06c1\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u06c1\\u0651', prev_class=None, prev_tokens=None, tokens=['h', 'h'], next_tokens=None, next_class=None)}),\n",
        " (133, {'token': 'dh'}),\n",
        " (134,\n",
        "  {'found': u'\\u062f\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062f\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['d', 'dh'], next_tokens=None, next_class=None)}),\n",
        " (135, {'token': 'v'}),\n",
        " (136, {'token': 'v'}),\n",
        " (137,\n",
        "  {'found': u'\\u0648\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0648\\u0651', prev_class=None, prev_tokens=None, tokens=['v', 'v'], next_tokens=None, next_class=None)}),\n",
        " (138, {'token': 'kh'}),\n",
        " (139,\n",
        "  {'found': u'\\u06a9\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u06a9\\u06be', prev_class=None, prev_tokens=None, tokens=['k', 'kh'], next_tokens=None, next_class=None)}),\n",
        " (140, {'token': ';r'}),\n",
        " (141, {'token': ';r'}),\n",
        " (142,\n",
        "  {'found': u'\\u0691\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0691\\u0651', prev_class=None, prev_tokens=None, tokens=[';r', ';r'], next_tokens=None, next_class=None)}),\n",
        " (143, {'token': ';t'}),\n",
        " (144,\n",
        "  {'found': u'\\u0679\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0679\\u0651', prev_class=None, prev_tokens=None, tokens=[';t', ';t'], next_tokens=None, next_class=None)}),\n",
        " (145, {'token': ';z'}),\n",
        " (146, {'token': ';z'}),\n",
        " (147,\n",
        "  {'found': u'\\u062e\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u062e\\u0651', prev_class=None, prev_tokens=None, tokens=[';z', ';z'], next_tokens=None, next_class=None)}),\n",
        " (148, {'token': 'n'}),\n",
        " (149, {'token': 'n'}),\n",
        " (150,\n",
        "  {'found': u'\\u0646\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0646\\u0651', prev_class=None, prev_tokens=None, tokens=['n', 'n'], next_tokens=None, next_class=None)}),\n",
        " (151, {'token': ':z'}),\n",
        " (152, {'token': ':z'}),\n",
        " (153,\n",
        "  {'found': u'\\u0638\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0638\\u0651', prev_class=None, prev_tokens=None, tokens=[':z', ':z'], next_tokens=None, next_class=None)}),\n",
        " (154, {'token': 'z'}),\n",
        " (155, {'token': 'z'}),\n",
        " (156,\n",
        "  {'found': u'\\u0632\\u0651',\n",
        "   'rule': ParserRule(production=u'\\u0632\\u0651', prev_class=None, prev_tokens=None, tokens=['z', 'z'], next_tokens=None, next_class=None)}),\n",
        " (157, {'token': 'jh'}),\n",
        " (158,\n",
        "  {'found': u'\\u062c\\u0651\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062c\\u0651\\u06be', prev_class=None, prev_tokens=None, tokens=['j', 'jh'], next_tokens=None, next_class=None)}),\n",
        " (159,\n",
        "  {'found': u'\\u0627\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_class=['wb'])}),\n",
        " (160,\n",
        "  {'found': u'\\u0627\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=['wb'])}),\n",
        " (161,\n",
        "  {'found': u'\\u06c1',\n",
        "   'rule': ParserRule(production=u'\\u06c1', prev_class=['short_vowel_a'], prev_tokens=None, tokens=['h'], next_tokens=None, next_class=['wb'])}),\n",
        " (162, {'token': '-e'}),\n",
        " (163,\n",
        "  {'found': u'\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u06d2', prev_class=['nc_long_vowel'], prev_tokens=None, tokens=['-e'], next_tokens=None, next_class=['wb'])}),\n",
        " (164, {'token': '^ai'}),\n",
        " (165,\n",
        "  {'found': u'\\u0627\\u0650\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0650\\u06d2', prev_class=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=['wb'])}),\n",
        " (166, {'token': ';n'}),\n",
        " (167,\n",
        "  {'found': u'\\u06ba',\n",
        "   'rule': ParserRule(production=u'\\u06ba', prev_class=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_class=['wb'])}),\n",
        " (168, {'token': 'u'}),\n",
        " (169,\n",
        "  {'found': u'\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['u'], next_tokens=None, next_class=None)}),\n",
        " (170, {'token': ';e'}),\n",
        " (171,\n",
        "  {'found': u'\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=[';e'], next_tokens=None, next_class=None)}),\n",
        " (172, {'token': 'a'}),\n",
        " (173,\n",
        "  {'found': u'\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['a'], next_tokens=None, next_class=None)}),\n",
        " (174,\n",
        "  {'found': u'\\u0627\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['e'], next_tokens=None, next_class=None)}),\n",
        " (175, {'token': '^i'}),\n",
        " (176,\n",
        "  {'found': u'\\u0627\\u0650',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0650', prev_class=['wb'], prev_tokens=None, tokens=['^i'], next_tokens=None, next_class=None)}),\n",
        " (177, {'token': 'i'}),\n",
        " (178,\n",
        "  {'found': u'\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u0627', prev_class=['wb'], prev_tokens=None, tokens=['i'], next_tokens=None, next_class=None)}),\n",
        " (179, {'token': 'ul-'}),\n",
        " (180,\n",
        "  {'found': u'\\u0627\\u0644',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['ul-'], next_tokens=None, next_class=None)}),\n",
        " (181, {'token': '^a'}),\n",
        " (182,\n",
        "  {'found': u'\\u0627\\u064e',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u064e', prev_class=['wb'], prev_tokens=None, tokens=['^a'], next_tokens=None, next_class=None)}),\n",
        " (183, {'token': 'o'}),\n",
        " (184,\n",
        "  {'found': u'\\u0627\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['o'], next_tokens=None, next_class=None)}),\n",
        " (185, {'token': 'aa'}),\n",
        " (186,\n",
        "  {'found': u'\\u0622',\n",
        "   'rule': ParserRule(production=u'\\u0622', prev_class=['wb'], prev_tokens=None, tokens=['aa'], next_tokens=None, next_class=None)}),\n",
        " (187,\n",
        "  {'found': u'\\u0627\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=None)}),\n",
        " (188, {'token': ':'}),\n",
        " (189,\n",
        "  {'found': ':',\n",
        "   'rule': ParserRule(production=':', prev_class=None, prev_tokens=None, tokens=[':'], next_tokens=None, next_class=['wb'])}),\n",
        " (190, {'token': 'au'}),\n",
        " (191,\n",
        "  {'found': u'\\u0627\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['au'], next_tokens=None, next_class=None)}),\n",
        " (192, {'token': 'ii'}),\n",
        " (193,\n",
        "  {'found': u'\\u0627\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['ii'], next_tokens=None, next_class=None)}),\n",
        " (194, {'token': '^uu'}),\n",
        " (195,\n",
        "  {'found': u'\\u0627\\u064f\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u064f\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['^uu'], next_tokens=None, next_class=None)}),\n",
        " (196, {'token': 'il-'}),\n",
        " (197,\n",
        "  {'found': u'\\u0644\\u0644',\n",
        "   'rule': ParserRule(production=u'\\u0644\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['il-'], next_tokens=None, next_class=None)}),\n",
        " (198,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_class=['wb'])}),\n",
        " (199,\n",
        "  {'found': u'\\u0650',\n",
        "   'rule': ParserRule(production=u'\\u0650', prev_class=None, prev_tokens=None, tokens=['-e'], next_tokens=None, next_class=['wb'])}),\n",
        " (200, {'token': '^ii'}),\n",
        " (201,\n",
        "  {'found': u'\\u0627\\u0650\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['^ii'], next_tokens=None, next_class=None)}),\n",
        " (202, {'token': '^u'}),\n",
        " (203,\n",
        "  {'found': u'\\u0627\\u064f',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u064f', prev_class=['wb'], prev_tokens=None, tokens=['^u'], next_tokens=None, next_class=None)}),\n",
        " (204, {'token': ';m'}),\n",
        " (205,\n",
        "  {'found': u'\\u06ba',\n",
        "   'rule': ParserRule(production=u'\\u06ba', prev_class=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_class=['wb'])}),\n",
        " (206,\n",
        "  {'found': u'\\u0627\\u0650\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0650\\u06cc', prev_class=['wb'], prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=None)}),\n",
        " (207, {'token': '^au'}),\n",
        " (208,\n",
        "  {'found': u'\\u0627\\u064e\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u064e\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['^au'], next_tokens=None, next_class=None)}),\n",
        " (209, {'token': 'uu'}),\n",
        " (210,\n",
        "  {'found': u'\\u0627\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0627\\u0648', prev_class=['wb'], prev_tokens=None, tokens=['uu'], next_tokens=None, next_class=None)}),\n",
        " (211, {'token': '.'}),\n",
        " (212,\n",
        "  {'found': '.',\n",
        "   'rule': ParserRule(production='.', prev_class=None, prev_tokens=None, tokens=['.'], next_tokens=None, next_class=['wb'])}),\n",
        " (213,\n",
        "  {'found': u'\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u06d2', prev_class=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_class=['wb'])}),\n",
        " (214,\n",
        "  {'found': u'\\u0650\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u0650\\u06d2', prev_class=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=['wb'])}),\n",
        " (215, {'token': 'al-'}),\n",
        " (216,\n",
        "  {'found': u'\\u0644\\u0644',\n",
        "   'rule': ParserRule(production=u'\\u0644\\u0644', prev_class=['wb'], prev_tokens=None, tokens=['al-'], next_tokens=None, next_class=None)}),\n",
        " (217,\n",
        "  {'found': u'\\u06d2',\n",
        "   'rule': ParserRule(production=u'\\u06d2', prev_class=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=['wb'])}),\n",
        " (218,\n",
        "  {'found': ' ',\n",
        "   'rule': ParserRule(production=' ', prev_class=None, prev_tokens=None, tokens=['-'], next_tokens=None, next_class=None)}),\n",
        " (219, {'token': '/'}),\n",
        " (220,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['/'], next_tokens=None, next_class=None)}),\n",
        " (221, {'token': 'gh'}),\n",
        " (222,\n",
        "  {'found': u'\\u06af\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u06af\\u06be', prev_class=None, prev_tokens=None, tokens=['gh'], next_tokens=None, next_class=None)}),\n",
        " (223, {'token': ' '}),\n",
        " (224,\n",
        "  {'found': ' ',\n",
        "   'rule': ParserRule(production=' ', prev_class=None, prev_tokens=None, tokens=' ', next_tokens=None, next_class=None)}),\n",
        " (225, {'token': ';rh'}),\n",
        " (226,\n",
        "  {'found': u'\\u0691\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0691\\u06be', prev_class=None, prev_tokens=None, tokens=[';rh'], next_tokens=None, next_class=None)}),\n",
        " (227, {'token': '('}),\n",
        " (228,\n",
        "  {'found': '(',\n",
        "   'rule': ParserRule(production='(', prev_class=None, prev_tokens=None, tokens=['('], next_tokens=None, next_class=None)}),\n",
        " (229, {'token': ','}),\n",
        " (230,\n",
        "  {'found': u'\\u060c',\n",
        "   'rule': ParserRule(production=u'\\u060c', prev_class=None, prev_tokens=None, tokens=[','], next_tokens=None, next_class=None)}),\n",
        " (231, {'token': 'th'}),\n",
        " (232,\n",
        "  {'found': u'\\u062a\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062a\\u06be', prev_class=None, prev_tokens=None, tokens=['th'], next_tokens=None, next_class=None)}),\n",
        " (233, {'token': ':n'}),\n",
        " (234,\n",
        "  {'found': u'\\u064b',\n",
        "   'rule': ParserRule(production=u'\\u064b', prev_class=None, prev_tokens=None, tokens=[':n'], next_tokens=None, next_class=None)}),\n",
        " (235, {'token': 'dh'}),\n",
        " (236,\n",
        "  {'found': u'\\u062f\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062f\\u06be', prev_class=None, prev_tokens=None, tokens=['dh'], next_tokens=None, next_class=None)}),\n",
        " (237,\n",
        "  {'found': u'\\u0639',\n",
        "   'rule': ParserRule(production=u'\\u0639', prev_class=None, prev_tokens=None, tokens=['(('], next_tokens=None, next_class=None)}),\n",
        " (238,\n",
        "  {'found': u'\\u0638',\n",
        "   'rule': ParserRule(production=u'\\u0638', prev_class=None, prev_tokens=None, tokens=[':z'], next_tokens=None, next_class=None)}),\n",
        " (239,\n",
        "  {'found': u'\\u0637',\n",
        "   'rule': ParserRule(production=u'\\u0637', prev_class=None, prev_tokens=None, tokens=[':t'], next_tokens=None, next_class=None)}),\n",
        " (240,\n",
        "  {'found': u'\\u062f',\n",
        "   'rule': ParserRule(production=u'\\u062f', prev_class=None, prev_tokens=None, tokens=['d'], next_tokens=None, next_class=None)}),\n",
        " (241, {'token': '--'}),\n",
        " (242,\n",
        "  {'found': u'\\u06d4',\n",
        "   'rule': ParserRule(production=u'\\u06d4', prev_class=None, prev_tokens=None, tokens=['--'], next_tokens=None, next_class=None)}),\n",
        " (243,\n",
        "  {'found': u'\\u06c1',\n",
        "   'rule': ParserRule(production=u'\\u06c1', prev_class=None, prev_tokens=None, tokens=['h'], next_tokens=None, next_class=None)}),\n",
        " (244,\n",
        "  {'found': u'\\u0644',\n",
        "   'rule': ParserRule(production=u'\\u0644', prev_class=None, prev_tokens=None, tokens=['l'], next_tokens=None, next_class=None)}),\n",
        " (245,\n",
        "  {'found': u'\\u067e',\n",
        "   'rule': ParserRule(production=u'\\u067e', prev_class=None, prev_tokens=None, tokens=['p'], next_tokens=None, next_class=None)}),\n",
        " (246,\n",
        "  {'found': u'\\u062a',\n",
        "   'rule': ParserRule(production=u'\\u062a', prev_class=None, prev_tokens=None, tokens=['t'], next_tokens=None, next_class=None)}),\n",
        " (247, {'token': '|'}),\n",
        " (248,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['|'], next_tokens=None, next_class=None)}),\n",
        " (249,\n",
        "  {'found': u'\\u062b',\n",
        "   'rule': ParserRule(production=u'\\u062b', prev_class=None, prev_tokens=None, tokens=[';s'], next_tokens=None, next_class=None)}),\n",
        " (250,\n",
        "  {'found': u'\\u0691',\n",
        "   'rule': ParserRule(production=u'\\u0691', prev_class=None, prev_tokens=None, tokens=[';r'], next_tokens=None, next_class=None)}),\n",
        " (251, {'token': 'zh'}),\n",
        " (252,\n",
        "  {'found': u'\\u0698',\n",
        "   'rule': ParserRule(production=u'\\u0698', prev_class=None, prev_tokens=None, tokens=['zh'], next_tokens=None, next_class=None)}),\n",
        " (253,\n",
        "  {'found': u'\\u0679',\n",
        "   'rule': ParserRule(production=u'\\u0679', prev_class=None, prev_tokens=None, tokens=[';t'], next_tokens=None, next_class=None)}),\n",
        " (254,\n",
        "  {'found': u'\\u0630',\n",
        "   'rule': ParserRule(production=u'\\u0630', prev_class=None, prev_tokens=None, tokens=[';z'], next_tokens=None, next_class=None)}),\n",
        " (255,\n",
        "  {'found': u'\\u062e',\n",
        "   'rule': ParserRule(production=u'\\u062e', prev_class=None, prev_tokens=None, tokens=[';x'], next_tokens=None, next_class=None)}),\n",
        " (256, {'token': ';dh'}),\n",
        " (257,\n",
        "  {'found': u'\\u0688\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0688\\u06be', prev_class=None, prev_tokens=None, tokens=[';dh'], next_tokens=None, next_class=None)}),\n",
        " (258,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=[';e'], next_tokens=None, next_class=None)}),\n",
        " (259,\n",
        "  {'found': u'\\u0688',\n",
        "   'rule': ParserRule(production=u'\\u0688', prev_class=None, prev_tokens=None, tokens=[';d'], next_tokens=None, next_class=None)}),\n",
        " (260,\n",
        "  {'found': u'\\u062d',\n",
        "   'rule': ParserRule(production=u'\\u062d', prev_class=None, prev_tokens=None, tokens=[';h'], next_tokens=None, next_class=None)}),\n",
        " (261,\n",
        "  {'found': u'\\u0646',\n",
        "   'rule': ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=[';n'], next_tokens=None, next_class=None)}),\n",
        " (262,\n",
        "  {'found': u'\\u0646',\n",
        "   'rule': ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=[';m'], next_tokens=None, next_class=None)}),\n",
        " (263, {'token': ';th'}),\n",
        " (264,\n",
        "  {'found': u'\\u0679\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0679\\u06be', prev_class=None, prev_tokens=None, tokens=[';th'], next_tokens=None, next_class=None)}),\n",
        " (265, {'token': \"'\"}),\n",
        " (266,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=[\"'\"], next_tokens=None, next_class=None)}),\n",
        " (267, {'token': ';'}),\n",
        " (268,\n",
        "  {'found': u'\\u061b',\n",
        "   'rule': ParserRule(production=u'\\u061b', prev_class=None, prev_tokens=None, tokens=[';'], next_tokens=None, next_class=None)}),\n",
        " (269, {'token': '?'}),\n",
        " (270,\n",
        "  {'found': u'\\u061f',\n",
        "   'rule': ParserRule(production=u'\\u061f', prev_class=None, prev_tokens=None, tokens=['?'], next_tokens=None, next_class=None)}),\n",
        " (271,\n",
        "  {'found': u'\\u064e\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u064e\\u0648', prev_class=None, prev_tokens=None, tokens=['^au'], next_tokens=None, next_class=None)}),\n",
        " (272, {'token': 'bh'}),\n",
        " (273,\n",
        "  {'found': u'\\u0628\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0628\\u06be', prev_class=None, prev_tokens=None, tokens=['bh'], next_tokens=None, next_class=None)}),\n",
        " (274, {'token': 'jh'}),\n",
        " (275,\n",
        "  {'found': u'\\u062c\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u062c\\u06be', prev_class=None, prev_tokens=None, tokens=['jh'], next_tokens=None, next_class=None)}),\n",
        " (276, {'token': '['}),\n",
        " (277,\n",
        "  {'found': '[',\n",
        "   'rule': ParserRule(production='[', prev_class=None, prev_tokens=None, tokens=['['], next_tokens=None, next_class=None)}),\n",
        " (278, {'token': '_'}),\n",
        " (279,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['_'], next_tokens=None, next_class=None)}),\n",
        " (280,\n",
        "  {'found': u'\\u0635',\n",
        "   'rule': ParserRule(production=u'\\u0635', prev_class=None, prev_tokens=None, tokens=['.s'], next_tokens=None, next_class=None)}),\n",
        " (281,\n",
        "  {'found': u'\\u06af',\n",
        "   'rule': ParserRule(production=u'\\u06af', prev_class=None, prev_tokens=None, tokens=['g'], next_tokens=None, next_class=None)}),\n",
        " (282, {'token': ';aa'}),\n",
        " (283,\n",
        "  {'found': u'\\u06cc\\u0670',\n",
        "   'rule': ParserRule(production=u'\\u06cc\\u0670', prev_class=None, prev_tokens=None, tokens=[';aa'], next_tokens=None, next_class=None)}),\n",
        " (284,\n",
        "  {'found': u'\\u0633',\n",
        "   'rule': ParserRule(production=u'\\u0633', prev_class=None, prev_tokens=None, tokens=['s'], next_tokens=None, next_class=None)}),\n",
        " (285,\n",
        "  {'found': u'\\u0650\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u0650\\u06cc', prev_class=None, prev_tokens=None, tokens=['^ii'], next_tokens=None, next_class=None)}),\n",
        " (286, {'token': 'w'}),\n",
        " (287,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['w'], next_tokens=None, next_class=None)}),\n",
        " (288,\n",
        "  {'found': u'\\u0686',\n",
        "   'rule': ParserRule(production=u'\\u0686', prev_class=None, prev_tokens=None, tokens=['ch'], next_tokens=None, next_class=None)}),\n",
        " (289, {'token': '\\n'}),\n",
        " (290,\n",
        "  {'found': '\\n',\n",
        "   'rule': ParserRule(production='\\n', prev_class=None, prev_tokens=None, tokens=['\\n'], next_tokens=None, next_class=None)}),\n",
        " (291,\n",
        "  {'found': u'\\u0626',\n",
        "   'rule': ParserRule(production=u'\\u0626', prev_class=None, prev_tokens=None, tokens=['))'], next_tokens=None, next_class=None)}),\n",
        " (292,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['^ai'], next_tokens=None, next_class=None)}),\n",
        " (293, {'token': 'chh'}),\n",
        " (294,\n",
        "  {'found': u'\\u0686\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u0686\\u06be', prev_class=None, prev_tokens=None, tokens=['chh'], next_tokens=None, next_class=None)}),\n",
        " (295, {'token': '\\t'}),\n",
        " (296,\n",
        "  {'found': '\\t',\n",
        "   'rule': ParserRule(production='\\t', prev_class=None, prev_tokens=None, tokens=['\\t'], next_tokens=None, next_class=None)}),\n",
        " (297, {'token': 'ph'}),\n",
        " (298,\n",
        "  {'found': u'\\u067e\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u067e\\u06be', prev_class=None, prev_tokens=None, tokens=['ph'], next_tokens=None, next_class=None)}),\n",
        " (299, {'token': '----'}),\n",
        " (300,\n",
        "  {'found': u'\\u06d4\\u06d4',\n",
        "   'rule': ParserRule(production=u'\\u06d4\\u06d4', prev_class=None, prev_tokens=None, tokens=['----'], next_tokens=None, next_class=None)}),\n",
        " (301,\n",
        "  {'found': u'\\u0628',\n",
        "   'rule': ParserRule(production=u'\\u0628', prev_class=None, prev_tokens=None, tokens=['b'], next_tokens=None, next_class=None)}),\n",
        " (302,\n",
        "  {'found': u'\\u0641',\n",
        "   'rule': ParserRule(production=u'\\u0641', prev_class=None, prev_tokens=None, tokens=['f'], next_tokens=None, next_class=None)}),\n",
        " (303,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['uu'], next_tokens=None, next_class=None)}),\n",
        " (304,\n",
        "  {'found': u'\\u062c',\n",
        "   'rule': ParserRule(production=u'\\u062c', prev_class=None, prev_tokens=None, tokens=['j'], next_tokens=None, next_class=None)}),\n",
        " (305,\n",
        "  {'found': u'\\u0646',\n",
        "   'rule': ParserRule(production=u'\\u0646', prev_class=None, prev_tokens=None, tokens=['n'], next_tokens=None, next_class=None)}),\n",
        " (306,\n",
        "  {'found': u'\\u0636',\n",
        "   'rule': ParserRule(production=u'\\u0636', prev_class=None, prev_tokens=None, tokens=['.z'], next_tokens=None, next_class=None)}),\n",
        " (307,\n",
        "  {'found': u'\\u0631',\n",
        "   'rule': ParserRule(production=u'\\u0631', prev_class=None, prev_tokens=None, tokens=['r'], next_tokens=None, next_class=None)}),\n",
        " (308,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['v'], next_tokens=None, next_class=None)}),\n",
        " (309, {'token': '.r'}),\n",
        " (310,\n",
        "  {'found': u'\\u0631',\n",
        "   'rule': ParserRule(production=u'\\u0631', prev_class=None, prev_tokens=None, tokens=['.r'], next_tokens=None, next_class=None)}),\n",
        " (311,\n",
        "  {'found': u'\\u0632',\n",
        "   'rule': ParserRule(production=u'\\u0632', prev_class=None, prev_tokens=None, tokens=['z'], next_tokens=None, next_class=None)}),\n",
        " (312,\n",
        "  {'found': u'\\u06a9',\n",
        "   'rule': ParserRule(production=u'\\u06a9', prev_class=None, prev_tokens=None, tokens=['k'], next_tokens=None, next_class=None)}),\n",
        " (313,\n",
        "  {'found': u'\\u0627',\n",
        "   'rule': ParserRule(production=u'\\u0627', prev_class=None, prev_tokens=None, tokens=['aa'], next_tokens=None, next_class=None)}),\n",
        " (314,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['ai'], next_tokens=None, next_class=None)}),\n",
        " (315, {'token': '\\r'}),\n",
        " (316,\n",
        "  {'found': '\\r',\n",
        "   'rule': ParserRule(production='\\r', prev_class=None, prev_tokens=None, tokens=['\\r'], next_tokens=None, next_class=None)}),\n",
        " (317,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['ii'], next_tokens=None, next_class=None)}),\n",
        " (318,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['au'], next_tokens=None, next_class=None)}),\n",
        " (319,\n",
        "  {'found': u'\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u0648', prev_class=None, prev_tokens=None, tokens=['o'], next_tokens=None, next_class=None)}),\n",
        " (320, {'token': '!'}),\n",
        " (321,\n",
        "  {'found': '!',\n",
        "   'rule': ParserRule(production='!', prev_class=None, prev_tokens=None, tokens=['!'], next_tokens=None, next_class=None)}),\n",
        " (322, {'token': ')'}),\n",
        " (323,\n",
        "  {'found': ')',\n",
        "   'rule': ParserRule(production=')', prev_class=None, prev_tokens=None, tokens=[')'], next_tokens=None, next_class=None)}),\n",
        " (324, {'token': '\"'}),\n",
        " (325,\n",
        "  {'found': '\"',\n",
        "   'rule': ParserRule(production='\"', prev_class=None, prev_tokens=None, tokens=['\"'], next_tokens=None, next_class=None)}),\n",
        " (326,\n",
        "  {'found': u'\\u063a',\n",
        "   'rule': ParserRule(production=u'\\u063a', prev_class=None, prev_tokens=None, tokens=[';g'], next_tokens=None, next_class=None)}),\n",
        " (327, {'token': ']'}),\n",
        " (328,\n",
        "  {'found': ']',\n",
        "   'rule': ParserRule(production=']', prev_class=None, prev_tokens=None, tokens=[']'], next_tokens=None, next_class=None)}),\n",
        " (329,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['a'], next_tokens=None, next_class=None)}),\n",
        " (330,\n",
        "  {'found': u'\\u064f\\u0648',\n",
        "   'rule': ParserRule(production=u'\\u064f\\u0648', prev_class=None, prev_tokens=None, tokens=['^uu'], next_tokens=None, next_class=None)}),\n",
        " (331,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['e'], next_tokens=None, next_class=None)}),\n",
        " (332,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['i'], next_tokens=None, next_class=None)}),\n",
        " (333, {'token': 'kh'}),\n",
        " (334,\n",
        "  {'found': u'\\u06a9\\u06be',\n",
        "   'rule': ParserRule(production=u'\\u06a9\\u06be', prev_class=None, prev_tokens=None, tokens=['kh'], next_tokens=None, next_class=None)}),\n",
        " (335,\n",
        "  {'found': u'\\u0645',\n",
        "   'rule': ParserRule(production=u'\\u0645', prev_class=None, prev_tokens=None, tokens=['m'], next_tokens=None, next_class=None)}),\n",
        " (336,\n",
        "  {'found': u'\\u064f',\n",
        "   'rule': ParserRule(production=u'\\u064f', prev_class=None, prev_tokens=None, tokens=['^u'], next_tokens=None, next_class=None)}),\n",
        " (337,\n",
        "  {'found': u'\\u0642',\n",
        "   'rule': ParserRule(production=u'\\u0642', prev_class=None, prev_tokens=None, tokens=['q'], next_tokens=None, next_class=None)}),\n",
        " (338,\n",
        "  {'found': u'\\u0650',\n",
        "   'rule': ParserRule(production=u'\\u0650', prev_class=None, prev_tokens=None, tokens=['^i'], next_tokens=None, next_class=None)}),\n",
        " (339,\n",
        "  {'found': u'\\u0634',\n",
        "   'rule': ParserRule(production=u'\\u0634', prev_class=None, prev_tokens=None, tokens=['sh'], next_tokens=None, next_class=None)}),\n",
        " (340,\n",
        "  {'found': '',\n",
        "   'rule': ParserRule(production='', prev_class=None, prev_tokens=None, tokens=['u'], next_tokens=None, next_class=None)}),\n",
        " (341,\n",
        "  {'found': u'\\u06cc',\n",
        "   'rule': ParserRule(production=u'\\u06cc', prev_class=None, prev_tokens=None, tokens=['y'], next_tokens=None, next_class=None)}),\n",
        " (342,\n",
        "  {'found': u'\\u064e',\n",
        "   'rule': ParserRule(production=u'\\u064e', prev_class=None, prev_tokens=None, tokens=['^a'], next_tokens=None, next_class=None)})]"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}