{
 "metadata": {
  "name": "",
  "signature": "sha256:9ab39020eaf9b097b6d7bc96ce9c4b29ea757dacee9a8667b0cc8d8356fb7dd3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ghalib Concordance Generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Description\n",
      "\n",
      "This notebook contains code to generator a concordance for the muravvaj divaan of ghalib.\n",
      "\n",
      "Verses are taken from \"input/verses.csv\"\n",
      "\n",
      "The current task is to identify the proper lemma of the tokens, e.g. singular instead of plural,\n",
      "verb infinitive instead of verb root, etc. This can partially be done computationally.\n",
      "\n",
      "Lemma that remain to be checked are in \"output/tocheck.csv\" The first column, if marked as 'x',\n",
      "means that entry is okay. Checked lemma can then be entered into \"input/okay.csv\" using the\n",
      "functions \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import *\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "verses = {}                      # dictionary of verses, e.g. 001.01.0='naqsh faryaadii..'\n",
      "tokens = {}                      # dictionary of tokens where key is verses+.xx, e.g. 001.01.0.01 = 'naqsh'\n",
      "unique_tokens = Counter()        # Counter of tokens where value is their count\n",
      "lemmas = defaultdict(list)       # dictionary of tokens where value is a list of their lemmas\n",
      "unique_lemmas = []               # list of unique lemmas\n",
      "okay_lemmas = defaultdict(list)  # dictionary of unique tokens with lists of lemma, e."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# moved load_verses, moved to util.py\n",
      "\n",
      "def load_verses(inputfile='input/verses.csv'):\n",
      "    '''\n",
      "    Loads verses from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: verses where verses['ggg.vv.l']=token; where ggg=ghazal #; vv=verse number;l=line number\n",
      "    '''\n",
      "\n",
      "\n",
      "    verses = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            (verse_id, input_string, real_scan) = row # \n",
      "            if not 'x' in verse_id: # only muravvaj divan for now\n",
      "                verses[verse_id] = input_string.strip() \n",
      "    return verses\n",
      "\n",
      "def get_okay_lemmas(inputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Loads checked lemmas from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: checked_lemmas where checked_lemmas['token'] = [lemmas]\n",
      "    '''\n",
      "\n",
      "    import csv\n",
      "    okay_lemmas = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            (status, unique_token, lemmas) = row\n",
      "            assert status in ['','x']\n",
      "            if status=='x':\n",
      "                okay_lemmas[unique_token]=lemmas.split('|')\n",
      "    return okay_lemmas\n",
      "\n",
      "\n",
      "def get_tokens(verses):\n",
      "    '''\n",
      "    Identifies tokens in verses\n",
      "    verses: verses\n",
      "    returns: tokens, where tokens['ggg.vv.l.tt']=token {tt = token # on line starting  at zero}\n",
      "    '''\n",
      "    tokens = {}\n",
      "    token_instances=defaultdict(list)\n",
      "    token_instance_count = Counter()\n",
      "    for k in verses.keys():\n",
      "        v_tokens = verses[k].split(' ')\n",
      "        for id,t in enumerate(v_tokens):\n",
      "\n",
      "            token_id = k+'.'+str(id).zfill(2)\n",
      "            tokens[token_id] = t\n",
      "            token_instances[t].append(token_id)\n",
      "            token_instance_count[t]+=1\n",
      "    return tokens,token_instances,token_instance_count\n",
      "\n",
      "def locate_token(token):\n",
      "    '''\n",
      "    Finds locations of token\n",
      "    token: string \n",
      "    Input: token (string)\n",
      "    returns: a list of locations, e.g. [001.01.0.01]\n",
      "    '''\n",
      "    assert tokens\n",
      "    return [k  for k,v in tokens.iteritems() if v==token]\n",
      "\n",
      "def match_tokens(match_string):\n",
      "    '''\n",
      "    Finds tokens matching a pattern (from start)\n",
      "    match_string: regular expression string (assumes ^,e.g. 'naq')\n",
      "    returns: a list of tokens,e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.match(match_string,k)]\n",
      "\n",
      "def search_tokens(match_string):\n",
      "    '''\n",
      "    Searches for tokens matching a pattern (anywhere in it)\n",
      "    match_string: regular expression of string\n",
      "    Input: regular expression string (e.g. 'aqsh'\n",
      "    returns: a list of tokens, e.g. ['naqsh']\n",
      "    '''\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.search(match_string,k)]\n",
      "\n",
      "def get_unique_tokens(tokens):\n",
      "    '''\n",
      "    Finds unique tokens\n",
      "    tokens: a dictionary of tokens at locations, e.g. tokens['001.01.0.00']='naqsh'\n",
      "    returns: a dictionary of unique tokens and their count, unique_tokens['token']=1\n",
      "    '''\n",
      "    unique = Counter()\n",
      "#    print type(tokens)\n",
      "    for k,t in tokens.iteritems():\n",
      "        unique[t]+=1\n",
      "    return unique\n",
      "\n",
      "\n",
      "def get_lemmas(unique_tokens):\n",
      "    '''\n",
      "    Generate lemmas of tokens\n",
      "    unique_tokens: dictionary of unique tokens\n",
      "    returns: lemmas[original_token]=['lemma1','lemma2']\n",
      "    '''\n",
      "    lemmas = {}\n",
      "\n",
      "    \n",
      "    for t in unique_tokens.keys():\n",
      "        lemma = t\n",
      "        if re.search(\"-e$\",t):\n",
      "            lemma = t[:-2] # remove izaafat ending '-e'\n",
      "        if re.search(\"[-']haa$\",t): \n",
      "            lemma = t[:-4] # remove Persian plural ['-]haa ending\n",
      "#            print lemma\n",
      "        t_lemmas = [lemma]\n",
      "        if re.search('-o-',lemma):\n",
      "            nouns = lemma.split('-o-')\n",
      "            t_lemmas = t_lemmas + nouns\n",
      "            \n",
      "        lemmas[t]=t_lemmas\n",
      "    return lemmas\n",
      "\n",
      "def get_unique_lemmas(lemmas):\n",
      "    '''\n",
      "    Generates unique lemma forms\n",
      "    lemmas: dictionary keyed by tokens containing lists of lemma, e.g. lemmas['rang-o-buu']=['rang','buu','rang-o-buu']\n",
      "    returns: unique_lemmas as unique_lemmas['lemma']=count\n",
      "    '''\n",
      "    unique_lemmas = set()\n",
      "    for t,t_lemmas in lemmas.iteritems():\n",
      "        for lemma in t_lemmas:\n",
      "            unique_lemmas.add(lemma)\n",
      "#                unique_lemmas.add(lemma)\n",
      "#            else:\n",
      "#                unique_lemmas[lemma].append(t)\n",
      "    return unique_lemmas\n",
      "\n",
      "\n",
      "def to_check():\n",
      "    '''\n",
      "    Generates list of unique tokens that still need to be checked.\n",
      "    '''\n",
      "    out = []\n",
      "    return [t for t in sorted(unique_tokens.keys()) if not t in okay_lemmas]\n",
      "\n",
      "def print_stats():\n",
      "    print \"Currently there are \",len(okay_lemmas),\" out of \",len(lemmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "verses = load_verses()\n",
      "tokens,token_instances,token_instance_count = get_tokens(verses)\n",
      "unique_tokens = get_unique_tokens(tokens)\n",
      "\n",
      "lemmas = get_lemmas(unique_tokens)\n",
      "unique_lemmas = get_unique_lemmas(lemmas)\n",
      "okay_lemmas = get_okay_lemmas()\n",
      "\n",
      "print_stats()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Update Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_to_check():\n",
      "    '''\n",
      "    Writes unique tokens not contained in okay_lemmas to output/tocheck.csv\n",
      "    '''\n",
      "    with open('output/tocheck.csv','w') as f:\n",
      "        for t in sorted(unique_tokens.keys()):\n",
      "            if not t in okay_lemmas: # only add unchecked ones\n",
      "                line  = \",\" # good or bad\n",
      "                line += t+\",\" #token\n",
      "                line += '|'.join(lemmas[t]) # possible lemma of token\n",
      "                line += \"\\n\" \n",
      "                f.write(line)\n",
      "\n",
      "def update_okay(inputfile='output/tocheck.csv'):\n",
      "    '''\n",
      "    Loads lemmas noted as correct from inputfile into okay_lemmas\n",
      "    '''\n",
      "    lemmas_to_add = get_okay_lemmas(inputfile=inputfile)\n",
      "    for k,v in lemmas_to_add.iteritems():\n",
      "        if k in okay_lemmas:\n",
      "            print \"WARNING: \",k,\" found in okay_lemmas. Will override.\"\n",
      "        okay_lemmas[k] = v\n",
      "    \n",
      "def write_okay(outputfile='input/okay.csv'):\n",
      "    '''\n",
      "    Writes okay_lemmas to outputfile, as status,token,lemma1|lemma2|lemma3\n",
      "    '''\n",
      "    with open(outputfile,'w') as f:\n",
      "        for t in sorted(okay_lemmas.keys()):\n",
      "            line  = \"x,\" # good or bad\n",
      "            line += t+\",\" #token\n",
      "            line += '|'.join(okay_lemmas[t])\n",
      "            line += \"\\n\" \n",
      "            f.write(line)\n",
      "\n",
      "def update_files():\n",
      "    '''\n",
      "    Loads lemmas noted as correct from tocheck.csv, \n",
      "    Writes okay_lemmas as input/okay.csv\n",
      "    Regenerates output/tocheck.csv\n",
      "    '''\n",
      "    update_okay() \n",
      "    write_okay()\n",
      "    update_to_check()\n",
      "    print_stats()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "update_files()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Concordance Details\n",
      "\n",
      "Generates \"output/conc_details.csv\" which lists lemmas and their sources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmas_out = defaultdict(set)\n",
      "\n",
      "\n",
      "for k,v in okay_lemmas.iteritems(): # k = word; v = lemmas\n",
      "    for l in v:\n",
      "        lemmas_out[l].add(k)\n",
      "\n",
      "with open('output/conc_details.csv','w') as f:\n",
      "    for k,v in sorted(lemmas_out.iteritems()):\n",
      "        f.write(k+','+'|'.join(v)+'\\n')\n",
      "        \n",
      "#okay_lemmas.keys()[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Lemma Instances\n",
      "Sorted list of lemma instances."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def instances_of_lemma(lemma):\n",
      "    i=0\n",
      "    for x in lemmas_out[lemma]:\n",
      "        i+= token_instance_count[x]\n",
      "    return i\n",
      "\n",
      "lemma_instance_count = {lemma: instances_of_lemma(lemma) for lemma in lemmas_out.keys()}\n",
      "#instances_of_lemma for\n",
      "#zz=sorted(lemmas_out.keys(),key=instances_of_lemma)#sort_by_instances)#size_of_lemma_by_instances)\n",
      "#for z in zz: print z, instances_of_lemma[zz])\n",
      "with open(\"output/statistics/lemma-counts.csv\",\"w\") as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        f.write(x+','+str(lemma_instance_count[x])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Izafats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am not sure yet how we will wind up using these. Probably based on a token location range, similarly to compound verbs, etc. There may be some combos I am not grabbing properly. These will need to lemma-ed later (e.g. nasalization)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "izafat_verse_ids = [v_id for v_id in sorted(verses.keys()) if re.search('-e ',verses[v_id])]\n",
      "izafat_verses = [verses[v_id] for v_id in izafat_verse_ids]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "izafat_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+')\n",
      "izafats=Counter()\n",
      "for s in izafat_verses:\n",
      "    x = izafat_re.findall(s)#re.findall(m,s)\n",
      "    for y in x:\n",
      "        izafats[y]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('output/izafats.csv','w') as f:\n",
      "    f.write('\\n'.join(sorted(izafats.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here also is a version of the tokens where izafat phrases are treated as individual tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iast=Counter() # izafats as tokens, along with tokens\n",
      "iast_re = re.compile('(?:[^ ]+-e )+(?:z )?[^ ]+|[^ ]+')\n",
      "for i,s in verses.iteritems():\n",
      "    words = iast_re.findall(s)\n",
      "    for t in words:\n",
      "        iast[t]+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Statistics\n",
      "Word frequencies, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_csv_of_token_freq(d, filename):\n",
      "    '''\n",
      "    Generates a CSV file of a dictionary based on numeric value of key, reverse sorted\n",
      "    d: dictionary of tokens and values(token: #)\n",
      "    filename = output file name\n",
      "    '''\n",
      "    with open(filename,'w') as f:\n",
      "        for k,v in d.most_common():\n",
      "            f.write(k+','+str(v)+'\\n')\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_csv_of_token_freq(izafats, 'output/statistics/izafat-freq.csv')\n",
      "make_csv_of_token_freq(unique_tokens, 'output/statistics/uniquetokens-freq.csv')\n",
      "make_csv_of_token_freq(iast, 'output/statistics/izafatastokens-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(izafats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemma_counts_beta=Counter()\n",
      "\n",
      "for token, count in unique_tokens.iteritems():\n",
      "    if token in okay_lemmas:\n",
      "        lemma = okay_lemmas[token][0]\n",
      "    else:\n",
      "        lemma = token\n",
      "    lemma_counts_beta[lemma]+=count\n",
      "lemma_counts_beta\n",
      "make_csv_of_token_freq(lemma_counts_beta,'output/statistics/lemmas-beta-freq.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the following will generate the urdu versions of the statistics (a little slow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import generate_urdu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#redo here\n",
      "reload(generate_urdu)#generate_urdu.write_all_urdu_statistics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quick and Dirty Output\n",
      "\n",
      "This generates some quick output for proofing as .md; this a bit sloppy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('output/lemmas-by-size.txt','w') as f:\n",
      "    for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "        words=lemmas_out[x]\n",
      "        words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "        f.write(x+' '+str(lemma_instance_count[x])+'\\n')\n",
      "        for w in words:\n",
      "            f.write(\"  - \"+w+' '+str(token_instance_count[w])+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import sys\n",
      "sys.path.append('./graphparser/')\n",
      "import graphparser\n",
      "urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "\n",
      "def gen_hiur_lemmas_by_size():\n",
      "    import codecs\n",
      "    import sys\n",
      "    sys.path.append('./graphparser/')\n",
      "    import graphparser\n",
      "    urdup = graphparser.GraphParser('./graphparser/settings/urdu.yaml')\n",
      "    nagarip = graphparser.GraphParser('./graphparser/settings/devanagari.yaml')\n",
      "    def out_hiur(w):\n",
      "        return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "    with codecs.open('output/lemmas-by-size-hiur.md','w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "\n",
      "            f.write(out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "            for w in words:\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                \n",
      "def out_hiur(w):\n",
      "    return urdup.parse(w).output+' '+nagarip.parse(w).output+' '+w\n",
      "\n",
      "def md_link(s,urdu=True):\n",
      "    out =  \" [\"+s+\"]\"\n",
      "    out += \"(\"+'http://www.columbia.edu/itc/mealac/00pritchett/'\n",
      "    out += s[0:4]+'/'+s[6:8]+\".html?urdu\"\n",
      "    out += \") \"#\n",
      "    return out\n",
      "\n",
      "def gen_hiur_lemmas_by_size_with_verses():\n",
      "    with codecs.open('output/lemmas-by-size-w-verses-hiur.md','w','utf-8') as f:\n",
      "        for x in sorted(lemma_instance_count, key=lemma_instance_count.get,reverse=True):\n",
      "            words=lemmas_out[x]\n",
      "            words = sorted(words,key=token_instance_count.get, reverse=True)\n",
      "\n",
      "            f.write(out_hiur(x)+' '+str(lemma_instance_count[x])+'\\n')\n",
      "            for w in words:\n",
      "                f.write(\"  - \"+out_hiur(w)+' '+str(token_instance_count[w])+'\\n')\n",
      "                vi = set(x[:-5] for x in token_instances['kaa']) # eg001.01 from 001.01.01.0\n",
      "                f.write(\"    - \")# nested indent\n",
      "                f.write(', '.join([md_link(v) for v in vi]))\n",
      "                f.write('\\n')\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#gen_hiur_lemmas_by_size()\n",
      "gen_hiur_lemmas_by_size_with_verses()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set(x[:-5] for x in token_instances['kaa'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 195,
       "text": [
        "{'001.01',\n",
        " '001.02',\n",
        " '001.03',\n",
        " '001.04',\n",
        " '001.05',\n",
        " '003.02',\n",
        " '004.02',\n",
        " '005.04',\n",
        " '005.05',\n",
        " '006.04',\n",
        " '007.02',\n",
        " '007.05',\n",
        " '010.01',\n",
        " '010.02',\n",
        " '010.03',\n",
        " '010.04',\n",
        " '010.05',\n",
        " '010.06',\n",
        " '010.07',\n",
        " '010.08',\n",
        " '010.09',\n",
        " '010.10',\n",
        " '010.11',\n",
        " '010.12',\n",
        " '012.01',\n",
        " '012.02',\n",
        " '013.01',\n",
        " '013.02',\n",
        " '013.03',\n",
        " '013.04',\n",
        " '013.05',\n",
        " '013.06',\n",
        " '013.07',\n",
        " '014.01',\n",
        " '014.02',\n",
        " '014.03',\n",
        " '014.04',\n",
        " '014.05',\n",
        " '014.08',\n",
        " '014.09',\n",
        " '015.03',\n",
        " '015.07',\n",
        " '015.13',\n",
        " '015.14',\n",
        " '016.01',\n",
        " '016.04',\n",
        " '017.01',\n",
        " '017.05',\n",
        " '017.08',\n",
        " '017.09',\n",
        " '018.02',\n",
        " '021.01',\n",
        " '021.03',\n",
        " '021.06',\n",
        " '021.09',\n",
        " '021.11',\n",
        " '022.03',\n",
        " '022.05',\n",
        " '022.06',\n",
        " '022.07',\n",
        " '022.08',\n",
        " '024.01',\n",
        " '024.02',\n",
        " '024.03',\n",
        " '024.04',\n",
        " '024.05',\n",
        " '024.06',\n",
        " '024.07',\n",
        " '024.08',\n",
        " '025.03',\n",
        " '025.09',\n",
        " '027.01',\n",
        " '027.02',\n",
        " '027.03',\n",
        " '027.04',\n",
        " '027.05',\n",
        " '027.06',\n",
        " '027.07',\n",
        " '027.08',\n",
        " '031.02',\n",
        " '031.03',\n",
        " '032.02',\n",
        " '033.01',\n",
        " '033.02',\n",
        " '033.03',\n",
        " '033.04',\n",
        " '033.05',\n",
        " '033.06',\n",
        " '033.07',\n",
        " '036.02',\n",
        " '037.01',\n",
        " '037.02',\n",
        " '038.01',\n",
        " '038.04',\n",
        " '039.04',\n",
        " '042.01',\n",
        " '042.04',\n",
        " '043.01',\n",
        " '043.04',\n",
        " '045.01',\n",
        " '045.02',\n",
        " '045.03',\n",
        " '045.04',\n",
        " '045.05',\n",
        " '047.01',\n",
        " '047.02',\n",
        " '048.01',\n",
        " '048.03',\n",
        " '048.05',\n",
        " '048.06',\n",
        " '048.07',\n",
        " '048.10',\n",
        " '050.01',\n",
        " '050.02',\n",
        " '053.05',\n",
        " '054.01',\n",
        " '055.01',\n",
        " '058.02',\n",
        " '058.06',\n",
        " '059.03',\n",
        " '059.05',\n",
        " '060.10',\n",
        " '060.12',\n",
        " '062.08',\n",
        " '062.11',\n",
        " '063.01',\n",
        " '064.01',\n",
        " '064.03',\n",
        " '064.05',\n",
        " '064.06',\n",
        " '066.04',\n",
        " '066.06',\n",
        " '066.08',\n",
        " '070.02',\n",
        " '072.07',\n",
        " '075.04',\n",
        " '077.03',\n",
        " '077.04',\n",
        " '078.03',\n",
        " '078.07',\n",
        " '079.02',\n",
        " '080.04',\n",
        " '080.08',\n",
        " '080.09',\n",
        " '087.06',\n",
        " '087.08',\n",
        " '089.02',\n",
        " '090.05',\n",
        " '091.01',\n",
        " '091.02',\n",
        " '091.10',\n",
        " '096.05',\n",
        " '096.06',\n",
        " '097.03',\n",
        " '097.10',\n",
        " '099.02',\n",
        " '099.09',\n",
        " '103.01',\n",
        " '104.01',\n",
        " '107.01',\n",
        " '107.07',\n",
        " '108.02',\n",
        " '111.08',\n",
        " '111.12',\n",
        " '116.09',\n",
        " '119.02',\n",
        " '119.03',\n",
        " '119.09',\n",
        " '120.05',\n",
        " '120.06',\n",
        " '120.10',\n",
        " '123.09',\n",
        " '125.02',\n",
        " '125.04',\n",
        " '125.08',\n",
        " '126.04',\n",
        " '126.07',\n",
        " '126.08',\n",
        " '130.01',\n",
        " '131.07',\n",
        " '136.01',\n",
        " '136.02',\n",
        " '136.03',\n",
        " '136.04',\n",
        " '136.05',\n",
        " '136.06',\n",
        " '138.01',\n",
        " '138.03',\n",
        " '138.07',\n",
        " '139.02',\n",
        " '139.03',\n",
        " '139.04',\n",
        " '139.09',\n",
        " '139.12',\n",
        " '141.02',\n",
        " '143.01',\n",
        " '143.04',\n",
        " '145.01',\n",
        " '145.02',\n",
        " '145.03',\n",
        " '149.05',\n",
        " '152.03',\n",
        " '152.05',\n",
        " '158.07',\n",
        " '158.08',\n",
        " '161.02',\n",
        " '163.03',\n",
        " '164.12',\n",
        " '164.13',\n",
        " '169.01',\n",
        " '170.06',\n",
        " '170.07',\n",
        " '171.03',\n",
        " '173.05',\n",
        " '173.09',\n",
        " '173.11',\n",
        " '174.05',\n",
        " '174.07',\n",
        " '174.08',\n",
        " '177.09',\n",
        " '177.10',\n",
        " '178.10',\n",
        " '180.02',\n",
        " '183.08',\n",
        " '185.01',\n",
        " '186.02',\n",
        " '189.04',\n",
        " '189.05',\n",
        " '191.05',\n",
        " '192.02',\n",
        " '197.01',\n",
        " '198.02',\n",
        " '200.02',\n",
        " '201.03',\n",
        " '201.06',\n",
        " '202.02',\n",
        " '202.07',\n",
        " '204.03',\n",
        " '205.08',\n",
        " '208.08',\n",
        " '210.05',\n",
        " '210.06',\n",
        " '214.11',\n",
        " '215.02',\n",
        " '215.03',\n",
        " '215.10',\n",
        " '219.03',\n",
        " '219.04',\n",
        " '219.08',\n",
        " '219.09',\n",
        " '222.01',\n",
        " '223.02',\n",
        " '226.05',\n",
        " '228.02',\n",
        " '228.04',\n",
        " '231.09',\n",
        " '233.07',\n",
        " '233.08',\n",
        " '233.10',\n",
        " '234.10'}"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}